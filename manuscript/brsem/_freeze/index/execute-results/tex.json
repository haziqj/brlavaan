{
  "hash": "f6d137242e2e093b8a4418ce709e6483",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bias-Reduced Estimation of Structural Equation Models\nauthors:\n  - name: Haziq Jamil\n    orcid: 0000-0003-3298-1010\n    email: haziq.jamil@ubd.edu.bn\n    # corresponding: true\n    url: \"https://haziqj.ml\"\n    affiliations:\n      - name: Universiti Brunei Darussalam\n        id: ubd\n        department: Mathematical Sciences, Faculty of Science\n        address: Universiti Brunei Darussalam, Jalan Tungku Link\n        city: Bandar Seri Begawan\n        country: Brunei\n        postal-code: BE 1410\n      - name: London School of Economics and Political Science\n        id: lse\n        department: Department of Statistics\n        address: Columbia House, Houghton Street\n        city: London\n        country: United Kingdom\n        postal-code: WC2A 2AE\n  - name: Yves Rosseel\n    orcid: 0000-0002-4129-4477\n    email: Yves.Rosseel@UGent.be\n    affiliations:\n      - name: Ghent University\n        id: ugent\n        department: Department of data analysis\n        address: Henri Dunantlaan 2\n        city: Gent\n        country: Belgium\n        postal-code: 9000\n  - name: Oliver Kemp\n    # orcid: \n    email: Oliver.Kemp@warwick.ac.uk\n    affiliations:\n      - name: University of Warwick\n        id: warwick\n        department: Department of Statistics\n        address: Mathematical Sciences Building, University of Warwick\n        city: Coventry\n        country: United Kingdom\n        postal-code: CV4 7AL\n  - name: Ioannis Kosmidis\n    email: ioannis.kosmidis@warwick.ac.uk\n    orcid: 0000-0003-1556-0302\n    affiliations:\n      - ref: warwick\ndate-modified: last-modified\nabstract: |\n  An exploration of explicit and implicit bias-reduced maximum likelihood estimation for structural equation models (SEMs) and latent growth curve models (LGCMs) is presented. Comparisons to prior work on resampling-based methods like Bootstrap and Jackknife are made. Performance of our methods are favourable.\nkeywords:\n  - Structural Equation Models\n  - Growth Curve Models\n  - Bias Reduction\nfilters:\n  - highlight-text\n---\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.0.4     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(brlavaan)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: lavaan\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(gt)\nlibrary(glue)\nlibrary(semPlot)\nlibrary(semptools)\nload(\"results.RData\")\n```\n:::\n\n\n## Introduction\n\nStructural Equation Modeling (SEM) is a widely used statistical framework that enables researchers to specify, estimate, and test complex relationships among observed and latent variables. \nDespite its versatility and theoretical appeal, the practical application of SEM in empirical research is often challenged by limitations in sample size. \nSmall samples are particularly common in fields such as clinical psychology, developmental research, and neuroscience, where data collection is expensive, time-consuming, or logistically constrained [@vandeschoot2020small].\n\nHowever, small samples do not always arise from poor study design or logistical constraints; \nin many cases, they are an inevitable consequence of studying rare populations. \nFor example, research involving children with burned facial injuries is inherently limited by the small number of individuals who meet these specific criteria. \nSimilarly, studies focusing on rare genetic disorders, elite athlete populations, or unique cultural groups often face strict limitations in sample size due to the rarity of the target population. \nIn such contexts, the use of SEM remains desirable for testing complex theoretical models, but the challenges posed by small samples become unavoidable and must be addressed through methodological refinement.\n\nWhen the sample size is small, SEM faces a range of well-documented difficulties [@deng2018structural;@bentler1999structural;@nevitt2004evaluating]. \nThese include non-convergence of estimation algorithms, improper solutions such as negative variance estimates (Heywood cases), inflated standard errors, and low statistical power. \nNon-convergence, in particular, can be a very frustrating experience for applied researchers, often leading them to consider more simple and often suboptimal (non-SEM) procedures [@dejonckere2022using].\n\nIn response to these challenges, researchers have developed a variety of techniques to improve estimation performance in small samples. \nThese include alternative estimators such as Bayesian methods [@muthen2012bayesian;@lee2012basic], penalized likelihood approaches [@huang2017penalized;@jacobucci2016regularized], as well as model simplification strategies [@rosseel2024structural]. \nSuch developments have significantly improved the feasibility and stability of SEM in small-sample contexts. \nHowever, even when convergence is achieved and estimates appear admissible, a subtler problem remains: \nthe presence of finite-sample bias in parameter estimates. \n\nFinite-sample bias is a well-recognized issue in statistics, particularly in\nthe context of maximum likelihood estimation, where estimators often exhibit\nsystematic deviations from the true parameter values in small samples [@cox1979theoretical].\nTo address this, a variety of strategies have been proposed.\nAnalytical bias corrections based on higher-order asymptotic theory---such as those introduced by @cox1968general and extended by @firth1993bias---modify the score function or likelihood equations to produce estimators with reduced bias. \nBuilding on this foundation, Kosmidis and colleagues [e.g., @kosmidis2009bias;@kosmidis2020mean] have developed generalised bias-reduction techniques using adjusted score functions, which are applicable across a wide\nrange of models, including generalized linear and mixed models. \nThese methods are particularly appealing due to their strong theoretical properties and broad applicability. \nIn parallel, resampling-based techniques such as bootstrap and jackknife procedures [@efron1994introduction] offer flexible, model-agnostic tools for bias correction and have gained popularity in applied research for their ease of implementation. \nIn Bayesian estimation, prior distributions can also help shrink estimates toward more plausible values in small samples, though this introduces sensitivity to prior specification [@vanerp2018prior].\n\nWithin the SEM literature, initial attempts to mitigate the finite-sample bias involved the use of restricted maximum likelihood (REML), an estimation technique originally developed for variance component models [@corbeil1976restricted;@patterson1971recovery].\nAnd although effective, this approach is only applicable for a very restricted subclass of SEMs, namely those models that are mathematically equivalent to mixed effect models\n[@bauer2003estimating;@cheung2013implementing;@mcneish2016using;@mcneish2018brief].\nIn a paper focusing on small sample corrections for Wald tests in SEMs, @ozenne2020small describe an analytic method to correct for the finite-sample bias in SEMs based on technology originating from the generalized estimating equation (GEE) literature [@kauermann2001note]. \nImportantly, their method only targets the (observed or latent) (co)variance parameters in the model. \nAll other parameters (intercepts, regressions, factor loadings) are unaffected by their method.\nFinally, a more general strategy is the resampling-based approach proposed by @dhaene2022resampling, which adapts jacknife and bootstrap techniques to SEM\nfor correcting finite-sample bias in all parameter estimates. \nThis method is broadly applicable and relatively easy to implement, though it can be computationally intensive and may still exhibit some limitations in very small samples. Together, these approaches represent important steps toward addressing finite-sample bias in SEM, though each carries trade-offs between generality, complexity, and computational cost.\n\nRecently, @kosmidis2024empirical introduced a new framework for reducing bias in M-estimators (including maximum likelihood estimators) derived from asymptotically unbiased estimating functions [@vandervaart1998asymptotic]. \nThis framework offers both implicit and explicit methods for bias correction. \nThe implicit approach involves solving adjusted estimating equations, while the explicit method subtracts an estimated bias from the original M-estimates. \nNotably, these methods require only the first and second derivatives of the estimating functions, eliminating the need for complex expectations or resampling techniques. \nIn addition, these methods maintain the same asymptotic distribution as the original M-estimators, ensuring that standard inference and model selection procedures remain applicable. \n\nMotivated by these appealing properties, the goal of this paper is to investigate how the reduced-bias M (RBM) estimation framework developed by @kosmidis2024empirical can be extended to the context of structural equation modeling. \nIn contrast to existing bias-reduction techniques for SEM, the RBM approach is fully general---applicable to any SEM specification---while simultaneously targeting all model parameters. \nIt also offers a computationally efficient alternative to resampling-based methods. \nMoreover, the RBM method is relatively straightforward to implement in practice, and we provide accompanying R code in the supplementary materials to facilitate its application.\n\nThe remainder of the paper is structured as follows. \nWe begin by briefly outlining the SEM framework and introducing the notation used throughout the paper. \nWe then present a concise overview of several bias-reduction techniques, including the method proposed by Ozenne and colleagues, the jackknife and bootstrap approaches, and the recently introduced RBM method. \nFollowing this, we report the results of a comprehensive simulation study designed to evaluate the performance of the RBM approach in comparison to existing alternatives.\nFinally, we conclude with a discussion of the findings and offer practical recommendations for applied researchers.\n\n\n[To place this table elsewhere (or remove)]{bg-colour=\"#FFEA00\"}\n\n\n\n::: {#tbl-timing .cell tbl-cap='Comparison of mean computation times (in seconds, with standard deviations in parentheses) for estimating the two-factor SEM and latent growth curve models for $n=1000$.'}\n\n```{.r .cell-code .hidden}\nres_timing_n1000 |>\n  rowwise() |>\n  mutate(\n    thing = glue::glue(\"{gt::vec_fmt_number(mean)} ({gt::vec_fmt_number(sd)})\"),\n    method = factor(\n      method,\n      levels = c(\"ML\", \"lav\", \"eRBM\", \"iRBM\", \"JB\", \"BB\", \"Ozenne\", \"REML\"),\n      labels = names(mycols)\n    )\n  ) |>\n  select(-mean, -sd) |>\n  pivot_wider(names_from = model, values_from = thing) |>\n  arrange(method) |>\n  gt() |>\n  sub_missing(missing_text = \"\") |>\n  cols_label(\n    method = \"Method\",\n    twofac = \"Two-factor SEM\",\n    growth = \"Latent growth curve model\"\n  ) |>\n  cols_width(\n    twofac ~ px(180),\n    growth ~ px(180)\n  )\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}c>{\\centering\\arraybackslash}p{\\dimexpr 135.00pt -2\\tabcolsep-1.5\\arrayrulewidth}>{\\centering\\arraybackslash}p{\\dimexpr 135.00pt -2\\tabcolsep-1.5\\arrayrulewidth}}\n\\toprule\nMethod & Two-factor SEM & Latent growth curve model \\\\ \n\\midrule\\addlinespace[2.5pt]\nML & 0.07 (0.02) & 0.03 (0.01) \\\\ \neRBM & 2.45 (0.15) & 1.36 (0.07) \\\\ \niRBM & 43.03 (11.97) & 8.70 (1.10) \\\\ \nJackknife & 105.85 (7.54) & 94.79 (3.37) \\\\ \nBootstrap & 52.34 (4.13) & 45.24 (1.97) \\\\ \nOzenne et al. & 0.49 (0.05) & 0.11 (0.02) \\\\ \nREML &  & 0.61 (0.10) \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n## Methodology\n\n### Structural Equation Models\n\nA structural equation model (SEM) has two parts, a measurement part and a structural part. \nFor observation $i$, the measurement part of the model is defined as \n\n$$\ny_i = \\nu + \\Lambda \\eta_i + \\epsilon_i,\n$${#eq-measurement}\n\nwhere $y_i$ is a $p \\times 1$ vector of observed variables, $\\nu$ is a $p \\times 1$ vector of measurement intercepts, $\\Lambda$ is a $p \\times q$ matrix of factor loadings, $\\eta_i$ is a $q \\times 1$ vector of latent variables, and $\\epsilon_i$ is a $p \\times 1$ vector of measurement errors, assumed to be normally distributed with mean zero and covariance matrix $\\Theta$.\n\nThe structural part of the model is defined as\n\n$$\n\\eta_i =\\alpha + B \\eta_i + \\zeta_i,\n$$ {#eq-structural}\n\nwhere $\\alpha$ is a $q \\times 1$ vector of factor means, $B$ is a $q \\times q$ matrix of latent regression coefficients for which $\\operatorname{diag}(B) = 0$ and $I-B$ is invertible, and $\\zeta_i$ is a $q \\times 1$ vector of structural disturbances, assumed to be normally distributed with mean zero and covariance matrix $\\Psi$. \nWe also assume that $\\operatorname{Cov}(\\eta_i, \\epsilon_i) = \\text{cov}(\\epsilon_i, \\zeta_i) = 0$.\n\n<!-- $$ -->\n<!-- \\epsilon \\sim N(0, \\Theta_\\epsilon)\\\\ -->\n<!-- \\zeta \\sim N(0, \\Psi)\\\\ -->\n<!-- $$ {#eq-sem} -->\n\n### Estimation methods\n\n#### Maximum likelihood estimation\n\nParameter estimation in a SEM is typically performed using maximum likelihood estimation, where given some observed data, we estimate parameters as those which make the observations most likely under the assumed model. The specific parameters to estimate depend on the model, since which parameters from @eq-measurement and @eq-structural are present in the model and free depend on the specific SEM. In any case, as described in @dhaene2022resampling, we can combine all free parameters to be estimated in to a single $m \\times 1$ vector $\\theta$, and define the model mean vector and covariance matrix as\n\n\\begin{align}\n\\mu(\\theta) &= \\nu + \\Lambda (I - B)^{-1} \\alpha \\\\\n\\Sigma(\\theta) &= \\Lambda (I - B)^{-1} \\Psi (I-B)^{-\\top} \\Lambda^\\top + \\Theta,\n\\end{align}\n\nwhere for an invertible matrix $A$, $A^{-\\top} = (A^{-1})^\\top$.\n\nThen given independent and identically distributed data $\\mathcal Y = \\{y_1, \\dots, y_n \\}$ sampled from $\\text{N}\\big(\\mu(\\theta), \\Sigma(\\theta)\\big)$, parameter estimation is achieved by maximising the multivariate normal log likelihood defined as\n$$\n\\ell(\\theta \\mid \\mathcal Y) = \\frac{-np}{2}\\log(2\\pi) -\\frac{n}{2}\\log|\\Sigma(\\theta)| - \\frac{1}{2}\\sum_{i=1}^n \\big(y_i - \\mu(\\theta)\\big)^\\top \\Sigma(\\theta)^{-1}\\big(y_i - \\mu(\\theta)\\big).\n$${#eq-sem_lik}\n\n\nMaximising @eq-sem_lik with respect to $\\theta$ gives the maximum likelihood estimator of $\\theta$, which we denote by $\\hat{\\theta}$. Equivalently, we can solve the equations $\\nabla \\ell(\\theta) = 0_m$, where here $\\nabla$ denotes gradient with respect to $\\theta$, $0_m$ is an $m-$vector of zeroes.\n\n#### Reduced-bias estimation\n\nWithin statistical research, it is often the case that an estimator of an unknown parameter $\\theta \\in \\mathbb{R}^m$ is biased, that is the expected value of the estimator is not equal to the parameter. The maximum likelihood estimator, as used for SEMs, is an example of a typically biased estimator which under regularity conditions from @cox1979theoretical, has bias with asymptotic order $O(n^{-1})$. Therefore, $\\hat{\\theta}$ is asymptotically unbiased but for small samples the bias may be substantial. As reviewed in, for example, @kosmidis2014bias, the aim of bias reduction methods is to produce a new estimator $\\theta^*$ of $\\theta$ that approximates the solution of\n$$\n\\hat{\\theta} - \\theta^* = B(\\bar{\\theta})\n$$ {#eq-bias_eq}\nwith respect to $\\theta^*$. In @eq-bias_eq, $B(\\bar{\\theta}) = \\mathbb{E}(\\hat{\\theta} - \\bar{\\theta})$ is the bias function, and $\\bar{\\theta}$ is the value that $\\hat{\\theta}$ converges to, or is assumed to converge to in probability, as sample size increases. However, the unbiased estimator $\\theta^*$ generally can not be computed exactly since $B(\\cdot)$ is typically not available in closed form and $\\bar{\\theta}$ is not known. Therefore, we may not be able to produce a new estimator that is unbiased, but we can approximate the solution of @eq-bias_eq to produce an estimator with asymptotically smaller bias than the original $\\hat{\\theta}$.\n\nBias reduction methods operate explicitly or implicitly, we denote the explicit estimator as $\\theta_{\\text{exp}}$ and the implicit estimator as $\\tilde{\\theta}$. In explicit methods, we aim to approximate the bias of an estimator, then subtract this bias from the original estimator to produce a reduced-bias estimator. In particular, such methods replace $B(\\theta)$ with $B^*$, which is computed from the available data, before computing $\\theta_{\\text{exp}} = \\hat{\\theta} - B^*$. Implicit methods compute a reduced-bias estimator by replacing $B(\\theta)$ with $\\hat{B}(\\tilde{\\theta})$, which is an estimator of the whole bias function at the desired estimator. Then we solve the implicit equation $\\hat{\\theta} - \\tilde{\\theta} = \\hat{B}(\\tilde{\\theta})$ for $\\tilde{\\theta}$. \nExamples of explicit bias reduction methods include the bootstrap [@efron1994introduction; @hall1988bootstrap] and jackknife [@quenouille1956notes; @efron1982jackknife], which are both examined in this paper, while a notable example of an implicit method is the bias-reducing adjusted score equations of @firth1993bias, from which reduced-bias M-estimation was developed.\n\nThe main bias reduction technique to be examined in this paper is Reduced-bias M-estimation [RBM-estimation, @kosmidis2024empirical], which can operate implicitly or explicitly. This method requires only the SEM likelihood and its first three derivatives, so can be readily implemented, either in closed form, or numerically using packages such as `{numDeriv}` [@gilbert2022numderiv]. Since the bias is approximated analytically, RBM-estimation is likely to be noticeably faster than simulation based methods such as the bootstrap and jackknife. When estimation is by maximising a log-likelihood, as is the case for SEMs, implicit RBM-estimation works by adding an appropriate penalty to the log-likelihood, so that rather than maximising $\\ell(\\theta)$, we instead maximise\n\n$$\n\\ell(\\theta) + P(\\theta)\n$$\n\nfor an appropriate penalty term $P(\\theta)$. The penalty term is chosen such that the resulting estimator, $\\tilde{\\theta}$, has asymptotically reduced bias compared to $\\hat{\\theta}$, while still retaining the same asymptotic properties. This concept started in @firth1993bias for regular maximum likelihood estimators, by adding a bias-reducing adjustment term to the score equations, the derivative of the log likelihood. When estimating the canonical parameter of an exponential family model, @jeffreys1946prior invariant prior can be added as a penalty term to the log-likelihood to achieve the same result. @kosmidis2024empirical generalised this adjustment to generic M-estimators, which also has the significant advantage of giving rise to a penalised likelihood for any M-estimator estimated by maximising an objective function, rather than only when estimating canonical parameters in an exponential family model.\n\nThe penalty term $P(\\theta)$ is defined as\n$$\nP(\\theta) = -\\frac{1}{2}\\operatorname{tr}\\big\\{j(\\theta)^{-1} e(\\theta)\\big\\},\n$$\nwhere $j(\\theta)$ is the negative Hessian matrix $-\\nabla \\nabla^T \\ell(\\theta)$ and $e(\\theta)$ is the first order information, a matrix with elements computed as \n$$\n[e(\\theta)]_{kl} = \\sum_{i=1}^n \\left\\{ \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta_l}  \\frac{\\partial \\ell_i(\\theta)}{\\partial \\theta_k} \\right\\},\n$$\n\nwhere $\\ell_i(\\theta)$ is the $i$-th contribution to the log-likelihood, such that $\\ell(\\theta) = \\sum_{i=1}^n \\ell_i(\\theta)$.\n\nWe define the explicit version of RBM-estimation by obtaining an explicit expression for the bias and subtracting this from the maximum likelihood estimator. The explicit estimator is\n$$\n\\theta_{\\text{exp}} = \\hat{\\theta} + j(\\hat{\\theta})^{-1} A(\\hat{\\theta}),\n$$\nwhere $A(\\hat\\theta) = \\nabla P(\\theta)\\Big|_{\\theta=\\hat\\theta}$.\n\nThere are advantages and disadvantages to the implicit and explicit approaches. Computing $\\theta_{\\text{exp}}$ is a single step procedure so is likely to be faster to compute then $\\tilde{\\theta}$. Indeed, computing $\\theta_{\\text{exp}}$ can be seen as performing one step of the optimisation routine used to compute $\\tilde{\\theta}$. However, the explicit approach contains no safeguards on what value the final estimator can take, so may result in an estimate outside the parameter space. The implicit approach allows such constraints to be incorporated into the optimisation procedure, and may be more stable as it does not require $\\hat{\\theta}$, so is useful in cases where obtaining $\\hat{\\theta}$ is unstable. The implicit approach also allows extra plug in penalties to be added to achieve extra desired properties such as finiteness of estimators. Such penalties are discussed in @kosmidis2024empirical, and for SEMs specifically, are the subject of future work with the aim of increasing estimation stability.\n\n\n## Simulation Study\n\n---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n# execute:\n#   freeze: auto\n#   cache: true\n#   echo: false\n# \n# bibliography: \n#   - ../refs-HJ.bib\n#   - ../refs-OK.bib\n---\n\n\n<!-- ```{r} -->\n<!-- #| label: setup -->\n<!-- #| include: false -->\n<!-- library(tidyverse) -->\n<!-- library(brlavaan) -->\n<!-- library(gt) -->\n<!-- library(glue) -->\n<!-- library(semPlot) -->\n<!-- library(semptools) -->\n<!-- load(\"results.RData\") -->\n<!-- ``` -->\n\nTo evaluate the effectiveness of our proposed explicit and implicit bias reduction methods, we conducted an extensive simulation study mirroring the work done by @dhaene2022resampling.\nWe replicated the models and experimental settings exactly, allowing for a direct comparison with existing resampling-based and other methods for bias correction.\nThis section describes the models, simulation scenarios, and the outcome of interest of our simulations.\n\n### Models\n\nTwo distinct structural equation models (SEM) were considered: a) A two-factor SEM with a latent regression; and b) a latent growth curve model (GCM), \nThese models represent commonly used models used by practitioners and researchers alike in the social sciences.\n\nThe two-factor SEM contains two latent variables, $\\eta_{1}$ and $\\eta_{2}$, each indicated by three observed variables: \n$y_{1}$, $y_{2}$, $y_{3}$ for $\\eta_{1}$ and $y_{4}$, $y_{5}$, $y_{6}$ for $\\eta_{2}$. \nEach observed measure has an associated error term $\\epsilon_{j}\\sim\\operatorname{N}(0,\\Theta_{jj})$. \nThe latent regression path from $\\eta_{1}$ to $\\eta_{2}$ is labelled $\\beta$. \nFactor loadings are represented by $\\Lambda_{ij}$, and the latent variances by $\\Psi_{11}$ and $\\Psi_{22}$.\nAltogether, the model has 13 estimable parameters, as specified in @tbl-truth-twofac.\nThe corresponding path diagram is illustrated in @fig-path-twofac.\n\n::: {#fig-path-twofac}\n\n::: {.cell}\n\n```{.tex .cell-code .hidden}\n\\begin{tikzpicture}\n\\node[inner sep=3pt] {%\n\\begin{tikzpicture}[%\n  >=stealth,\n  auto,\n  node distance=1.6cm,\n  every node/.style={font=\\small},\n  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},\n  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},\n  error/.style={circle, draw, minimum size=6mm, inner sep=0mm}\n]\n\n% --------------------------------------------------------\n% 1) LATENT VARIABLES\n% --------------------------------------------------------\n\\node[latent] (eta1) at (-1.5, -2) {$\\eta_{1}$};\n\\node[latent] (eta2) at ( 3, -2) {$\\eta_{2}$};\n\n% --------------------------------------------------------\n% 2) OBSERVED VARIABLES + ERROR TERMS\n%    Place y1..y3 (for eta1) on the left, y4..y6 (for eta2) on the right.\n% --------------------------------------------------------\n\\node[obs] (y1) at (-3, 0) {$y_{1}$};\n\\node[obs] (y2) at (-1.5, 0) {$y_{2}$};\n\\node[obs] (y3) at ( 0, 0) {$y_{3}$};\n\n\\node[obs] (y4) at ( 1.5, 0) {$y_{4}$};\n\\node[obs] (y5) at ( 3, 0) {$y_{5}$};\n\\node[obs] (y6) at ( 4.5, 0) {$y_{6}$};\n\n% Error terms above each observed variable\n\\node[error] (e1) at (-3, 1.5) {$\\epsilon_{1}$};\n\\node[error] (e2) at (-1.5, 1.5) {$\\epsilon_{2}$};\n\\node[error] (e3) at ( 0, 1.5) {$\\epsilon_{3}$};\n\\node[error] (e4) at ( 1.5, 1.5) {$\\epsilon_{4}$};\n\\node[error] (e5) at ( 3, 1.5) {$\\epsilon_{5}$};\n\\node[error] (e6) at ( 4.5, 1.5) {$\\epsilon_{6}$};\n\n% --------------------------------------------------------\n% 3) PATHS FROM ERROR TERMS TO OBSERVED VARIABLES\n% --------------------------------------------------------\n\\draw[->] (e1) -- (y1);\n\\draw[->] (e2) -- (y2);\n\\draw[->] (e3) -- (y3);\n\\draw[->] (e4) -- (y4);\n\\draw[->] (e5) -- (y5);\n\\draw[->] (e6) -- (y6);\n\n% --------------------------------------------------------\n% 4) FACTOR LOADINGS\n%    \\eta_1 -> y1, y2, y3\n%    \\eta_2 -> y4, y5, y6\n% --------------------------------------------------------\n\\draw[->] (eta1) -- node[pos=0.5, right] {$\\Lambda_{11}$} (y1);\n\\draw[->] (eta1) -- node[pos=0.5, right] {$\\Lambda_{21}$} (y2);\n\\draw[->] (eta1) -- node[pos=0.5, right] {$\\Lambda_{31}$} (y3);\n\n\\draw[->] (eta2) -- node[pos=0.5, right] {$\\Lambda_{42}$} (y4);\n\\draw[->] (eta2) -- node[pos=0.5, right] {$\\Lambda_{52}$} (y5);\n\\draw[->] (eta2) -- node[pos=0.5, right] {$\\Lambda_{62}$} (y6);\n\n% --------------------------------------------------------\n% 5) REGRESSION BETWEEN LATENT VARIABLES\n%    \\eta_1 -> \\eta_2, labeled beta\n% --------------------------------------------------------\n\\draw[->] (eta1) -- node[midway, above] {$\\beta$} (eta2);\n\n% --------------------------------------------------------\n% 6) VARIANCES OF LATENT VARIABLES\n%    Double-headed arrows for psi_{11} and psi_{22}\n% --------------------------------------------------------\n\\draw[<->] (eta1) to[out=200, in=230, looseness=4] \n  node[left] {$\\Psi_{11}$} (eta1);\n\n\\draw[<->] (eta2) to[out=-50, in=-20, looseness=4] \n  node[right] {$\\Psi_{22}$} (eta2);\n\n\\end{tikzpicture}\n};\n\\end{tikzpicture}\n```\n\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/pathtwofac-1.pdf){width=90%}\n:::\n:::\n\nThe path diagram for the two-factor structural equation model. \n:::\n\nThe latent GCM on the other hand characterises longitudinally observed continuous variables measured across ten time points $y_{1},\\dots,y_{10}$ using two latent factors labelled $i$ for *intercept*, and $s$ for *slope*.\nThe intercept factor loadings are fixed to unity, while slope factor loadings increment from 0 to 9. \nLatent variances and covariance are represented by parameters $\\Psi_{11}$, $\\Psi_{22}$, and $\\Psi_{12}$. \nAdditionally, latent intercepts for both growth factors are freely estimated, representing average initial status and growth trajectory, respectively. \n\nThis model, illustrated in @fig-path-growth, involves a total of 6 parameters (see @tbl-truth-growth).\nThe reduced number of parameters is due to constraining all error variances $\\operatorname{Var}(\\epsilon_{j})$ to be equal (to $\\Theta$) across all measurement occasions, rather than being subscripted by $j$ as in the regular SEM.\nThe reason for this is that the latent GCM model can equivalently be viewed as a random intercept and random slope (multilevel) model, where repeated observations over time are nested within individuals. \nUnder this hierarchical perspective, it is customary--and theoretically justified--to assume homogeneous measurement error variances across repeated measures, reflecting consistent reliability of observations at each measurement occasion. \nThis equivalence also motivates our examination of Restricted Maximum Likelihood (REML) estimation, which is commonly employed in multilevel modelling contexts to reduce bias in the estimation of variance components, particularly when dealing with limited sample sizes.\nA description of the REML method is provided in the next subsection.\n\n::: {#fig-path-growth}\n\n::: {.cell}\n\n```{.tex .cell-code .hidden}\n\\usetikzlibrary{shapes.geometric,arrows,calc,positioning}\n\\begin{tikzpicture}\n\\node[inner sep=3pt] {%\n\\begin{tikzpicture}[%\n  >=stealth,              % for arrow tips\n  auto,                   % automatic label positioning\n  node distance=1.6cm,\n  every node/.style={font=\\small},\n  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},\n  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},\n  error/.style={circle, draw, minimum size=6mm, inner sep=0mm},\nintercept/.style={regular polygon, regular polygon sides=3, draw, inner sep=0pt, minimum size=6mm}\n  ]\n\n%-------------------------------------------------------\n% 1) LATENT FACTORS (intercept i, slope s)\n%-------------------------------------------------------\n\\node[latent] (i) at (-2,-3) {$i$};\n\\node[latent] (s) at ( 2,-3) {$s$};\n\\node[intercept] (int1) at (-2,-4.2) {\\footnotesize 1};\n\\node[intercept] (int2) at (2,-4.2) {\\footnotesize 1};\n\n%-------------------------------------------------------\n% 2) COVARIANCES AMONG LATENT FACTORS\n%    - psi_{11} on i\n%    - psi_{22} on s\n%    - psi_{21} between i and s\n%-------------------------------------------------------\n% i <-> s\n\\draw[<->] (i) to[out=-45, in=225] node[below] {$\\Psi_{12}$} (s);\n\n% Variance of i\n\\draw[<->] (i) to[out=190, in=220, looseness=4] \n  node[left] {$\\Psi_{11}$} (i);\n\n% Variance of s\n\\draw[<->] (s) to[out=-40, in=-10, looseness=4] \n  node[right] {$\\Psi_{22}$} (s);\n\n\\draw[->] (int1) -- node[right,pos=0.3] {$\\alpha_1$} (i);\n\\draw[->] (int2) -- node[right,pos=0.3] {$\\alpha_2$} (s);\n\n%-------------------------------------------------------\n% 3) OBSERVED VARIABLES (y1..y10) + ERRORS (eps1..eps10)\n%    Arrange them in a horizontal row above i and s.\n%-------------------------------------------------------\n\\foreach \\j in {1,...,10} {\n  % X-position for y_j (shift them so they are roughly centered)\n  \\pgfmathsetmacro{\\x}{(\\j*1.3 - 6.5)}\n\n  % Observed variable y_j\n  \\node[obs] (y\\j) at (\\x,0) {$y_{\\j}$};\n\n  % Error term epsilon_j above y_j\n  \\node[error] (e\\j) at (\\x,1.5) {$\\epsilon_{\\j}$};\n\n  % Arrow from error to observed\n  \\draw[->] (e\\j) -- (y\\j);\n\n  % Intercept factor loadings: all = 1\n  \\draw[->] (i) -- node[pos=0.45,right] {\\footnotesize 1} (y\\j);\n\n  % Slope factor loadings: 0..9\n  \\pgfmathtruncatemacro{\\loading}{\\j - 1}\n  \\draw[->] (s) -- node[pos=0.7,right] {\\footnotesize\\loading} (y\\j);\n}\n\n\\end{tikzpicture}\n};\n\\end{tikzpicture}\n```\n\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/pathgrowth-1.pdf)\n:::\n:::\n\nPath diagram for the growth curve model.\n:::\n\n### Competitor methods\n\n[I wonder if this should go in the section prior.]{bg-colour=\"#FFEA00\"}\n\nIn @dhaene2022resampling, the authors compared four bias reduction methods applied to the two SEM models above, namely the bootstrap, jackknife, Ozenne et al.'s [-@ozenne2020small] method, and Restricted Maximum Likelihood (REML).\nIn this subsection, we briefly describe these methods and their implementation in the context of our simulation study.\n\n#### Bootstrap and jackknife\n\nThe bootstrap and jackknife methods [@quenouille1956notes;@tukey1958bias;@hall1988bootstrap;@efron1982jackknife;@efron1992bootstrap] are resampling techniques that provide an empirical way of estimating the bias inherent in parameter estimates, which is then subtracted from the original estimate to obtain a bias-corrected estimate [@quenouille1949approximate;@efron1994introduction].\nAlthough similar, the two methods differ in their approach to resampling.\nThe bootstrap involves generating numerous replicated data sets $\\{y^{*}_{(b)}\\}_{b=1}^B$ by sampling with replacement from the original data set, and subsequently refitting the model to each bootstrap sample to obtain estimates $\\hat\\theta^{*}_{(b)}$.\nThe bootstrap bias-corrected estimator is computed as:\n$$\n\\hat\\theta_{\\text{boot}} = 2\\hat\\theta - \\frac{1}{B}\\sum_{b=1}^B \\hat\\theta^{*}_{(b)},\n$$\nwhere $B$ is the number of bootstrap samples.\nIn our study, $B=500$ was utilised.\nOn the other hand, the jackknife method involves obtaining estimates $\\hat\\theta_{(i)}$ by repeatedly fitting the model to $n$ subsamples of size $n-1$, systematically leaving out one observation at a time from the data set.\nThe jackknife bias-corrected estimator is then computed as:\n$$\n\\hat\\theta_{\\text{jack}} = n\\hat\\theta - (n-1)\\frac{1}{n}\\sum_{i=1}^n \\hat\\theta_{(i)}.\n$$\n\nConfidence intervals can be constructed in several ways [@davison1997bootstrap], but here we use the normal approximation interval. \nThis method centers the bias-corrected estimate, assumes approximate normality of its sampling distribution, and uses standard errors derived from the variability across resamples.\n\nAlthough intuitive and straightforward to implement, the bootstrap and jackknife each have their limitations. \nIn small samples, convergence issues and estimator instability can lead to unreliable resampled estimates [@dhaene2022resampling], while in large samples, the procedures can be computationally intensive. \nAdditionally, the jackknife may underestimate variability for nonlinear estimators [@efron1994introduction], and the bootstrap can perform poorly when the sampling distribution is skewed or parameters are near the boundary of the parameter space [@davison1997bootstrap].\n\n#### REML\n\nREML, while not an explicit bias reduction technique, is frequently employed in the estimation of variance components [@patterson1971recovery], particularly within the linear mixed effects framework [@corbeil1976restricted].\nAs noted earlier, the latent growth curve models may be expressed as a linear mixed model [@bauer2003estimating;@cheung2013implementing], where the latent intercept and slope are treated as random effects with model equations\n$$\n\\begin{gathered}\ny_i = X_i \\beta + Z_i b_i + \\epsilon_i \\\\\nu_i \\sim N(0, \\Sigma_u) \\\\\n\\epsilon_i \\sim N(0, \\sigma^2I) \\\\\n\\end{gathered}\n$$\nHere, $y_i\\in\\mathbb R^p$ are the observed item responses as before, $X_i$ and $Z_i$ are the fixed and random design matrices, respectively, $\\beta$ is the vector of fixed effects, $b_i$ is the vector of random effects, and $\\epsilon_i$ is the vector of residual errors.\nSpecifically, the latent GCM in @fig-path-growth has $X_i$ and $Z_i$, for each $i$, both being $10 \\times 2$ matrices where the first column contains all 1s and the second column contains the time points 0 to 9 in increments of 1.\nFurther, the 2-vector $\\beta$ in fact contains the latent intercepts $\\alpha_1$ and $\\alpha_2$, while the vector $b_i$ represents the two latent factors.\nTherefore, the covariance matrix $\\Sigma_u$ contains elements $\\Psi_{11}$, $\\Psi_{22}$, and $\\Psi_{12}$.\n\nLinear mixed models elicit a Gaussian likelihood.\nHowever, unlike standard ML, REML estimates are obtained by maximising a likelihood function derived from linearly transformed data, designed to remove fixed-effects parameters. \nSpecifically, REML maximises [@fitzmaurice2011applied]\n$$\n\\ell_{\\text{REML}}(\\theta) \n= \\text{const.} \n- \\frac{1}{2} \\log |V| \n- \\frac{1}{2} \\| y - X\\hat{\\beta} \\|_{V^{-1}}^2 \n- \\frac{1}{2} \\log | X^\\top V^{-1} X |,\n$$\nwhere $y$, $X$, and $Z$ represent the vertically stacked versions of $y_i$, $X_i$, and $Z_i$ across all $n$ observations, respectively, and $V=Z\\Sigma_u Z^\\top + \\sigma^2I$ is the variance-covariance matrix of these stacked observations.\nThe term $\\hat\\beta = (X^\\top V^{-1}X)^{-1}X^\\top V^{-1} y$ represents the generalised least squares estimates of the fixed-effects parameters.\n\nVariance parameters estimated this way yield less biased estimates, and is particularly beneficial for data characterised by small sample sizes and complex covariance structures.\nConfidence intervals are constructed in the usual way using Wald-type intervals based on the inverse of the observed information matrix, though their accuracy may be limited for variance components near the boundary or in small samples.\nFor further details, see, for example, @harville1977maximum, @skrondal2004generalized, and @bates2015fitting.\n\n#### Ozenne et al.'s method\n\n[Add description of @ozenne2020small method.]{bg-colour=\"#FFEA00\"}\n\n### Simulation scenarios\n\nAssessing bias reduction under these various settings is crucial because estimation methods may behave differently when a departure from ideal condition occurs.\nTo thoroughly assess robustness and performance of the proposed methods, we considered multiple scenarios varying by three experimental factors: \n\n- Sample size $n=15,20,50,100,1000$\n- Reliability of the observed variables $\\text{rel}=0.5,0.8$\n- Distribution of $\\eta$ and $\\epsilon$, one of \"Normal\", \"Kurtosis\", or \"Non-normal\".\n\nReliability in this context refers to the extent to which an observed indicator accurately reflects the underlying latent construct, with higher reliability indicating that a greater proportion of the indicatorâ€™s variance is attributable to the true score, and a lower proportion to measurement error. \nLower reliability (indicating high measurement error) can exacerbate finite sample bias, particularly in the estimation of variance components, and thus influence the overall performance of bias-correction techniques. \nThe chosen values of 0.8 and 0.5 are justified as they represent two distinct and informative measurement scenarios: a reliability of 0.8 simulates a close to ideal condition where the indicators capture 80% of the true variability, thereby reflecting a high-quality measurement scenario, while a reliability of 0.5 simulates a more challenging condition.\n\n::: {#tbl-panel layout-ncol=2}\n\n\n\n::: {#tbl-truth-twofac .cell tbl-cap='Two-factor model parameters.'}\n\n```{.r .cell-code .hidden}\ntibble(\n  rel = paste0(\"Reliability = \", c(0.8, 0.5))\n) |>\n  mutate(\n    truth = list(truth_twofac(0.8), truth_twofac(0.5)),\n    param = map(truth, ~ names(.x))\n  ) |>\n  unnest(c(truth, param)) |> \n  mutate(param = case_when(\n    param == \"fx=~x2\" ~ \"$\\\\Lambda_{2,1}$\",\n    param == \"fx=~x3\" ~ \"$\\\\Lambda_{3,1}$\",\n    param == \"fy=~y2\" ~ \"$\\\\Lambda_{5,2}$\",\n    param == \"fy=~y3\" ~ \"$\\\\Lambda_{6,2}$\",\n    param == \"fy~fx\"  ~ \"$\\\\beta$\",\n    param == \"x1~~x1\" ~ \"$\\\\Theta_{1,1}$\",\n    param == \"x2~~x2\" ~ \"$\\\\Theta_{2,2}$\",\n    param == \"x3~~x3\" ~ \"$\\\\Theta_{3,3}$\",\n    param == \"y1~~y1\" ~ \"$\\\\Theta_{4,4}$\",\n    param == \"y2~~y2\" ~ \"$\\\\Theta_{5,5}$\",\n    param == \"y3~~y3\" ~ \"$\\\\Theta_{6,6}$\",\n    param == \"fx~~fx\" ~ \"$\\\\Psi_{1,1}$\",\n    param == \"fy~~fy\" ~ \"$\\\\Psi_{2,2}$\"\n  )) |>\n  mutate(truth = as.character(truth)) |>\n  pivot_wider(names_from = rel, values_from = truth) |>\n  gt(rowname_col = \"param\") |>\n  fmt_markdown(\"param\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rr}\n\\toprule\n & Reliability = 0.8 & Reliability = 0.5 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\(\\Lambda_{2,1}\\) & 0.7 & 0.7 \\\\ \n\\(\\Lambda_{3,1}\\) & 0.6 & 0.6 \\\\ \n\\(\\Lambda_{5,2}\\) & 0.7 & 0.7 \\\\ \n\\(\\Lambda_{6,2}\\) & 0.6 & 0.6 \\\\ \n\\(\\beta\\) & 0.25 & 0.25 \\\\ \n\\(\\Theta_{1,1}\\) & 0.25 & 1 \\\\ \n\\(\\Theta_{2,2}\\) & 0.1225 & 0.49 \\\\ \n\\(\\Theta_{3,3}\\) & 0.09 & 0.36 \\\\ \n\\(\\Theta_{4,4}\\) & 0.25 & 1 \\\\ \n\\(\\Theta_{5,5}\\) & 0.1225 & 0.49 \\\\ \n\\(\\Theta_{6,6}\\) & 0.09 & 0.36 \\\\ \n\\(\\Psi_{1,1}\\) & 1 & 1 \\\\ \n\\(\\Psi_{2,2}\\) & 1 & 1 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n\n::: {#tbl-truth-growth .cell tbl-cap='Growth curve model parameters.'}\n\n```{.r .cell-code .hidden}\ntibble(\n  rel = paste0(\"Reliability = \", c(0.8, 0.5))\n) |>\n  mutate(\n    truth = list(truth_growth(0.8), truth_growth(0.5)),\n    param = map(truth, ~ names(.x))\n  ) |>\n  unnest(c(truth, param)) |>\n  distinct(rel, truth, param, .keep_all = TRUE) |>\n  mutate(param = case_when(\n    param == \"v\" ~ \"$\\\\theta$\",\n    param == \"i~~i\" ~ \"$\\\\Psi_{1,1}$\",\n    param == \"s~~s\" ~ \"$\\\\Psi_{2,2}$\",\n    param == \"i~~s\" ~ \"$\\\\Psi_{1,2}$\",\n    param == \"i~1\" ~ \"$\\\\alpha_1$\",\n    param == \"s~1\" ~ \"$\\\\alpha_2$\",\n  )) |>\n  mutate(ord = case_when(\n    grepl(\"Psi\", param) ~ 3,\n    grepl(\"alpha\", param) ~ 1,\n    grepl(\"theta\", param) ~ 2\n  )) |>\n  arrange(ord) |>\n  select(-ord) |>\n  mutate(truth = as.character(truth)) |>\n  pivot_wider(names_from = rel, values_from = truth) |>\n  gt(rowname_col = \"param\") |>\n  fmt_markdown(\"param\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rr}\n\\toprule\n & Reliability = 0.8 & Reliability = 0.5 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\(\\alpha_1\\) & 0 & 0 \\\\ \n\\(\\alpha_2\\) & 0 & 0 \\\\ \n\\(\\theta\\) & 500 & 1300 \\\\ \n\\(\\Psi_{1,1}\\) & 550 & 275 \\\\ \n\\(\\Psi_{2,2}\\) & 100 & 50 \\\\ \n\\(\\Psi_{1,2}\\) & 40 & 20 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\nTrue parameter values used in the simulations for the each reliability setting.\n:::\n\nDistribution-free estimation methods exist for SEM, which are initially derived from the Gaussian likelihood, but is generally thought to be quite robust to non-normality.\nInvestigating the effect of departing from the baseline normality assumption in SEM is also of interest.\nThis was done by replacing the normal distribution of *both* the latent factors and the measurement errors with one that has higher-order moments, yet still retains the same covariance structure.\nIn other words, every simulated dataset is constructed to have the same underlying covariance matrix, but the distributional shape (e.g., the bell-shaped curve in the normal case or the altered spread and tail behaviour in the non-normal and kurtosis cases) differs. \nThe `{covsim}` [@gronneberg2022covsim] package was utilised for this purpose.\nThe function allows to generate multivariate data with a target covariance matrix while controlling for the excess kurtosis and skewness of the distribution.\nIn the baseline Normal distribution, both the skewness and excess kurtosis were set to $0$.\nIn the \"Kurtosis\" condition, excess kurtosis was set to $6$ (with skewness held at $0$) to induce heavier tails; and in the \"Non-normal\" condition, skewness was set to $-2$ with excess kurtosis of $6$, producing asymmetric heavy-tailed distributions.\n@fig-compare-dist compares the shapes of the distributions for the three conditions.\nOnce the data were generated, the observed variables were computed as per equations ... for each model.\n\n\n::: {#cell-fig-compare-dist .cell}\n\n```{.r .cell-code .hidden}\nn <- 1000\nrho <- 0.3\nSigma <- matrix(c(1, rho, rho, 1), nrow = 2)\ndat <- list()\ndat$Normal <-\n  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(0, 2))[[1]]\ndat$Kurtosis <-  # kurtosis\n  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(6, 2))[[1]]\ndat$`Non-normal` <-  # non-normal\n  covsim::rIG(n, Sigma, skewness = rep(-2, 2), excesskurtosis = rep(6, 2))[[1]]\n\n# get max and min for x and y axes\nxyminmax <- \n  map(dat, as.data.frame) |>\n  bind_rows() |>\n  summarise(\n    x_min = min(V1),\n    x_max = max(V1),\n    y_min = min(V2),\n    y_max = max(V2)\n  ) |> \n  unlist() \n\nallplots <- imap(dat, \\(x, idx) {\n  mycols <- rev(RColorBrewer::brewer.pal(3, \"Set1\"))\n  names(mycols) <- names(dat)\n  \n  p <-\n    as.data.frame(x) |>\n    ggplot(aes(x = V1, y = V2)) +\n    geom_point(alpha = 0.5, size = 1, col = mycols[idx]) +\n    geom_density_2d(col = \"black\") +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    scale_x_continuous(limits = c(xyminmax[\"x_min\"], xyminmax[\"x_max\"])) +\n    scale_y_continuous(limits = c(xyminmax[\"y_min\"], xyminmax[\"y_max\"])) +\n    labs(title = glue::glue(\"{idx}\"), x = NULL, y = NULL)\n  ggExtra:::ggMarginal(p, fill = mycols[idx], alpha = 0.5, size = 10, \n                       trim = idx != \"Non-normal\") \n})\ncowplot::plot_grid(plotlist = allplots, nrow = 1, labels = \"auto\")\n```\n\n::: {.cell-output-display}\n![Scatterplots illustrating the three distributional conditions used in the simulation study. Each plot displays 1000 randomly generated observations drawn from a distribution with zero mean vector and covariance matrix $\\begin{bmatrix} 1 & 0.3 \\\\ 0.3 & 1 \\end{bmatrix}$. Marginal density plots on the axes highlight differences in symmetry and tail behavior across conditions.](index_files/figure-pdf/fig-compare-dist-1.pdf){#fig-compare-dist width=100%}\n:::\n:::\n\n\n### Performance measures\n\nAcross these $5\\times 2 \\times 3 = 30$ distinct scenarios, 1000 replications of the data were generated and fitted using the ML, our two bias reduction methods (eBR and iBR), as well as the competitor methods.\nThe performance of each method was evaluated using the criteria below.\nIn what follows, let $r=1,\\dots,R$ index the replications, $\\hat{\\theta}^{(r)}$ be the parameter estimates from the $r$-th replication, and $\\theta$ be the true parameter value.\n\n1. Mean bias\n  $$\n  \\operatorname{Bias}(\\hat{\\theta}) = \\frac{1}{R}\\sum_{r=1}^{R}(\\hat{\\theta}^{(r)} - \\theta)\n  $$\n\n2. Probability of underestimation (median)\n  $$\n  \\operatorname{P}(\\hat{\\theta}^{(r)} < \\theta) = \\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{1}(\\hat{\\theta}^{(r)} < \\theta)\n  $$\n\n3. Root Mean Square Error (RMSE)\n  $$\n  \\text{RMSE}(\\hat{\\theta}) = \\sqrt{\\frac{1}{R}\\sum_{r=1}^{R}(\\hat{\\theta}^{(r)} - \\theta)^2}\n  $$\n\n4. Coverage of 95% Wald Confidence Intervals\n  $$\n  \\text{Coverage} = \\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{1}\\left(\\hat{\\theta}^{(r)} - 1.96\\sqrt{\\widehat{\\text{Var}}(\\hat{\\theta}^{(r)})} \\leq \\theta \\leq \\hat{\\theta}^{(r)} + 1.96\\sqrt{\\widehat{\\text{Var}}(\\hat{\\theta}^{(r)})}\\right)\n  $$\n\nResults summarising these outcomes are discussed in detail in the next section.\n\n\n## Results\n\n### Success rates\n\n\n::: {#tbl-convsucc .cell tbl-cap='Success rates for estimation of the two-factor model for the ML, eBR, and iBR methods. Highlighted in red are the scenarios which produced success rates below 50%.'}\n\n```{.r .cell-code .hidden}\ngt(res_conv) |>\n  tab_spanner(\n    columns = matches(\"twofac.*0\\\\.8\"),\n    label = \"Reliability = 0.8\",\n    id = \"twofac80\"\n  ) |>\n  tab_spanner(\n    columns = matches(\"twofac.*0\\\\.5\"),\n    label = \"Reliability = 0.5\",\n    id = \"twofac50\"\n  ) |>\n  tab_spanner(\n    columns = starts_with(\"twofac\"),\n    label = \"Two-factor model\"\n  ) |>\n  tab_spanner(\n    columns = matches(\"growth.*0\\\\.8\"),\n    label = \"Reliability = 0.8\",\n    id = \"growth80\"\n  ) |>\n  tab_spanner(\n    columns = matches(\"growth.*0\\\\.5\"),\n    label = \"Reliability = 0.5\",\n    id = \"growth50\"\n  ) |>\n  tab_spanner(\n    columns = starts_with(\"growth\"),\n    label = \"Growth curve model\"\n  ) |>\n  fmt_percent(contains(\"0.\"), decimals = 1) |>\n  cols_label(\n    contains(\"ML\") ~ \"ML\",\n    contains(\"eRBM\") ~ \"eRBM\",\n    contains(\"iRBM\") ~ \"iRBM\"\n  ) |>\n  data_color(\n    columns = -all_of(\"n\"),\n    domain = c(0, 1.1),\n    palette = c(\"red3\", \"white\",  \"white\")\n  ) |>\n  tab_options(table.font.size = \"11px\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{8.2pt}{9.9pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}rrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{6}{c}{Two-factor model} & \\multicolumn{6}{c}{Growth curve model} \\\\ \n\\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n & \\multicolumn{3}{c}{Reliability = 0.8} & \\multicolumn{3}{c}{Reliability = 0.5} & \\multicolumn{3}{c}{Reliability = 0.8} & \\multicolumn{3}{c}{Reliability = 0.5} \\\\ \n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\nn & ML & eRBM & iRBM & ML & eRBM & iRBM & ML & eRBM & iRBM & ML & eRBM & iRBM \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{13}{l}{Normal} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\n15 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{91.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.3\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFD0C4}{\\textcolor[HTML]{000000}{42.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{92.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{95.5\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.7\\%}}} \\\\ \n20 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{96.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFF3F0}{\\textcolor[HTML]{000000}{51.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.6\\%}}} \\\\ \n50 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{91.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.6\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n100 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n1000 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{13}{l}{Kurtosis} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\n15 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{87.3\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFC8B9}{\\textcolor[HTML]{000000}{40.6\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{88.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{87.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{95.7\\%}}} \\\\ \n20 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.3\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFEBE5}{\\textcolor[HTML]{000000}{49.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{89.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{93.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.4\\%}}} \\\\ \n50 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{87.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.6\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.8\\%}}} \\\\ \n100 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} \\\\ \n1000 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{13}{l}{Non-normal} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\n15 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{81.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{89.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFC1B1}{\\textcolor[HTML]{000000}{38.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{81.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{81.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{95.2\\%}}} \\\\ \n20 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{90.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.5\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFEAE4}{\\textcolor[HTML]{000000}{49.5\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{86.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{86.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.4\\%}}} \\\\ \n50 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{85.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.8\\%}}} \\\\ \n100 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n1000 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\nBefore evaluating parameter estimates, we first examined the success rate of the estimation methods across all simulation scenarios. \nA successful estimation was determined if \n1) the optimiser reported successful convergence; and \n2) standard errors of estimates fell within acceptable numerical ranges. \nSpecifically, for the two-factor model, all standard errors were required to be less than 5, whereas for the growth curve model, reflecting differences in the scaling of observed variables, standard errors needed to be less than 500. \nAdditionally, in all cases, we required that the implied covariance matrix $\\hat{\\Sigma}$ to be positive definite.\n\nIn summarising the results we first filtered out the unsuccessful estimation cases.\nOur eRBM and iRBM methods demonstrated high success rates overall across simulation conditions, with a few exceptions. \nFor sample sizes $n \\geq 50$, the success rates were consistently above 85%, reaching 100% convergence for all scenarios with $n=100$ and $n=1000$.\n\nHowever, we observed notably lower success rates for smaller sample sizes ($n=15,20$), especially pronounced in the two-factor model when employing the eRBM. \nSince convergence criteria were met by the maximum likelihood step, these failures stemmed specifically from numerical instabilities arising during the computation of the derivatives required for the correction term.\nMoreover, it is possible that the post-hoc correction brings the parameter estimates outside the feasible parameter space, leading to non-positive definite covariance matrices.\nConsequently, caution should be exercised when interpreting eRBM estimates obtained from very small sample scenarios in the two-factor setting.\n\n### Two-factor model results\n\nWe first focus on the two-factor model under the scenario with normally distributed errors and high reliability (0.8), as this scenario yielded the highest convergence rates across all estimation methods, enabling a fair and direct comparison.\nSimilar to the work by @dhaene2022resampling, we focus our evaluation on a subset of key parameters, namely the error variance for item $y_1$, $\\Theta_{1,1}$, the second factor loading for item $y_1$, $\\Lambda_{2,1}$, the two variance parameters for the latent factors, $\\Psi_{1,1}$ and $\\Psi_{2,2}$, and the regression coefficient $\\beta$.\nThis selection spans a range of parameter types, providing a comprehensive overview of estimation performance across different components of the model.\n\nThe distribution plots of the centred parameter estimates (see @fig-centdist-twofac) revealed substantial variability across all methods at smaller sample sizes.\nHowever, variability decreased notably as the sample size increased. \n@fig-perf-twofac presents the performance metrics for the two-factor model, including relative bias, RMSE, probability of underestimation, and coverage rates.\nAcross all evaluated conditions, our proposed methods (eRBM and iRBM) consistently achieved reductions in mean bias compared to maximum likelihood (ML), particularly at smaller sample sizes ($n = 15, 20, 50$). \nRoot mean square error (RMSE), however, remained approximately equivalent across all methods, indicating similar levels of overall variability, a result visually corroborated by the comparative distribution plots.\nOur bias-reduction methods also provided notable improvements in confidence interval coverage, with increases of up to approximately 5% over ML, again primarily in smaller-sample scenarios.\n\n\n::: {#cell-fig-centdist-twofac .cell}\n\n```{.r .cell-code .hidden}\np1 <-\n  plot_df |>\n  filter(param %in% twofacpars) |> \n  ggplot(aes(bias, param, fill = method)) +\n  geom_boxplot(alpha = 0.8, outlier.size = 0.3, linewidth = 0.3) +\n  # geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(beta),\n    expression(Lambda[\"2,1\"])\n  ))) +\n  facet_grid(n ~ .) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"inside\")) +\n  theme_bw() +\n  theme(\n    legend.position.inside = c(0.91, 0.095), \n    legend.background = element_rect(fill = NA), \n    legend.text = element_text(size = 8), \n    legend.title = element_text(size = 9),\n    strip.background = element_blank(),\n    strip.text = element_blank()    \n  ) +\n  labs(x = NULL, y = NULL, fill = \"Method\", subtitle = glue::glue(\"{plot_df$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df$rel[1])}\"))\n\np2 <-\n  plot_df |>\n  filter(param %in% twofacpars) |> \n  summarise(count = n(), .by = dist:param) |>\n  ggplot(aes(count, param, fill = method)) +\n  geom_col(width = 0.8, position = position_dodge()) +\n  geom_vline(xintercept = 2000, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_x_continuous(expand = c(0, 0, 0, 100)) +\n  facet_grid(n ~ .) +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(), \n    axis.ticks.y = element_blank(), \n    axis.text.x = element_text(size = 7.5), \n    legend.position = \"none\"\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \" \")\n\ncowplot::plot_grid(p1, p2, rel_widths = c(1, 1 / 3))\n```\n\n::: {.cell-output-display}\n![Centered distributions of estimates for the two-factor model (left panel) using normally generated data at 80% reliability. Non-convergence cases and extreme estimates have been excluded. The right panel displays the number of estimates used to compute the summary statistics.](index_files/figure-pdf/fig-centdist-twofac-1.pdf){#fig-centdist-twofac width=100%}\n:::\n:::\n\n\n\n\n::: {#cell-fig-perf-twofac .cell}\n\n```{.r .cell-code .hidden}\nplot_df |> \n  filter(param %in% twofacpars) |> \n  summarise(\n    B = mean(bias, na.rm = TRUE, trim = 0.05),\n    rmse = sqrt(mean(bias ^ 2, na.rm = TRUE, trim = 0.05)),\n    pu = mean(bias < 0),\n    covr = mean(covered, na.rm = TRUE),\n    .by = c(dist:param)\n  ) |>\n  pivot_longer(B:covr, names_to = \"metric\", values_to = \"value\") |>\n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"B\", \"rmse\", \"pu\", \"covr\"),\n      labels = c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")\n    )\n  ) |>\n  ggplot(aes(value, param, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  geom_vline(\n    data = tibble(\n      metric = factor(c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")),\n      value = c(0, 0, 0.5, 0.95)\n    ),\n    aes(xintercept = value),\n    linetype = \"dashed\"\n  ) +\n  scale_fill_manual(values = mycols) +\n  facet_grid(n ~ metric, scales = \"free_x\") +\n  ggh4x::facetted_pos_scales(\n    x = list(\n      scale_x_continuous(),\n      scale_x_continuous(expand = c(0, 0, 0, 0.1)),\n      scale_x_continuous(limits = c(0.3, 0.7), labels = scales::percent),\n      scale_x_continuous(limits = c(0.6, 1), labels = scales::percent)\n    )\n  ) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(beta),\n    expression(Lambda[\"2,1\"])\n  ))) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"bottom\")) +\n  theme_bw() +\n  theme(\n    axis.text.x = element_text(size = 7.5),\n    legend.key.height = unit(1, \"pt\"), \n    legend.key.width = unit(9, \"pt\")\n  ) +\n  labs(x = NULL, y = NULL, fill = NULL, subtitle = glue::glue(\"{plot_df$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df$rel[1])}\"))\n```\n\n::: {.cell-output-display}\n![Performance metrics (relative bias, RMSE, probability of understimation, and coverage) of the ML, eBR, and iBR methods for estimation of the two-factor model. Vertical dashed lines indicate the ideal values for each metric.](index_files/figure-pdf/fig-perf-twofac-1.pdf){#fig-perf-twofac width=100%}\n:::\n:::\n\n\nComparisons with the resampling-based approaches of @dhaene2022resampling--including the Jackknife, Bootstrap--and the analytic Ozenne method, showed that our methods consistently improved relative mean bias across all reliability settings and distributional scenarios. \n@fig-meanbias-twofac and @fig-rmse-twofac demonstrate the relative mean bias and RMSE of our methods compared to aforementioned methods, respectively.\nFor brevity, we excluded the Kurtosis scenario from the plots, as it produced roughly similar results to the normal distribution.\nThe full table of results are available in the Appendix.\n\nWhile the bias performance of our methods was generally comparable to these established approaches, the resampling-based methods tended to achieve slightly lower bias at the cost of greater variability, particularly pronounced in scenarios with non-normal errors. \nOur eRBM and iRBM methods did not share this disadvantage, providing robust and stable estimates across scenarios.\nOne notable exception occurred in non-normal conditions with small sample sizes $(n = 15, 20)$, where the eRBM method encountered estimation difficulties, leading to rejection of approximately 60% of parameter estimates. \nThis reduced the observed bias performance in these specific cases. \nDespite this, median bias remained close to zero, suggesting that the inflated mean bias was driven predominantly by a small number of extreme estimates attributable to numerical instabilities.\n\n\n\n::: {#cell-fig-meanbias-twofac .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"twofac\", !method %in% c(\"lav\")) |>\n  ggplot(aes(n, relbias, col = method)) +\n  geom_line(linewidth = 0.75, alpha = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_color_manual(values = mycols) +\n  scale_x_continuous(labels = c(15, 20, 50, 100, 1000)) +\n  scale_y_continuous(labels = scales::percent) +\n  # coord_cartesian(ylim = c(-0.3, 0.3)) +\n  guides(colour = guide_legend(nrow = 1, reverse = TRUE, position = \"top\")) +\n  theme_bw() +\n  labs(x = \"Sample size (n)\", y = glue(\"Relative mean bias\"), col = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of relative mean bias of the ML, eBR, and iBR methods against the D&R methods for estimation of the two-factor model.](index_files/figure-pdf/fig-meanbias-twofac-1.pdf){#fig-meanbias-twofac width=100%}\n:::\n:::\n\n\n\n::: {#cell-fig-rmse-twofac .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"twofac\", !method %in% c(\"lav\")) |>\n  mutate(method = fct_rev(method)) |>\n  ggplot(aes(n, rmse, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  scale_x_continuous(breaks = 1:5, labels = c(15, 20, 50, 100, 1000)) +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_fill_manual(values = mycols) +\n  guides(fill = guide_legend(nrow = 1, position = \"bottom\")) +\n  theme_bw() +\n  theme(legend.key.height = unit(1, \"pt\"), legend.key.width = unit(9, \"pt\")) +\n  labs(x = \"Sample size (n)\", y = \"RMSE\", fill = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of RMSE of the ML, eBR, and iBR methods against the D&R methods for estimation of the two-factor model.](index_files/figure-pdf/fig-rmse-twofac-1.pdf){#fig-rmse-twofac width=100%}\n:::\n:::\n\n\n### Growth curve model results\n\nThe GCM saw more successful estimation cases than the two-factor SEM, even at smaller sample sizes and low reliability.\nWe focus on the normal distribution and low reliability (0.5) scenario as this is considered a more challenging case for estimation.\nIn total there were six free parameters to estimate, but we excluded the intercepts from analyses as these were not particularly interesting. \nThe four remaining parameters are all the components in the factor covariance matrix $\\Psi$, as well as the equality constrained error variance $\\Theta_{1,1}$.\n\n@fig-centdist-growth displays the distribution of estimates for the growth curve model, highlighting the centered distributions of estimates across methods.\nParameter estimates exhibited markedly lower variability--or at least, produced fewer outlier estimates in the boxplots--compared to the two-factor model\nThis is possibly attributable to the more parsimonious structure of the growth model (6 vs 13 parameters).\n@fig-perf-growth presents the performance metrics for the growth curve model, similar to before in the two-factor SEM experiments.\nAlso consistent with findings from before, both eRBM and iRBM methods yielded substantial improvements in bias and coverage relative to ML, particularly for small to moderate sample sizes. \n\nRMSE values remained comparable across methods, indicating consistent variability in parameter estimates across different approaches.\nNotably, our bias-reduced methods achieved better calibration in terms of median bias. Specifically, our methods consistently resulted in probabilities of underestimating parameters closer to the nominal 50% level, whereas ML tended to systematically overshoot, thus introducing bias.\nIt was interesting to see that the variance $\\Psi_{1,1}$ for the latent factor $\\eta_1$ benefited the most from the bias-reduction methods.\nFor context, the true value of this parameter was set to $\\Psi_{1,1}=275$, and the bias was reduced by 30 units (10.9%) at best in the $n=15$ scenario.\n\n\n::: {#cell-fig-centdist-growth .cell}\n\n```{.r .cell-code .hidden}\np1 <-\n  plot_df50 |>\n  filter(param %in% growthpars) |> \n  ggplot(aes(bias, param, fill = method)) +\n  geom_boxplot(alpha = 0.8, outlier.size = 0.3, linewidth = 0.3) +\n  # geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(Psi[\"1,2\"])\n  ))) +\n  facet_grid(n ~ .) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"inside\")) +\n  theme_bw() +\n  theme(\n    legend.position.inside = c(0.89, 0.095), \n    legend.background = element_rect(fill = NA), \n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 9),\n    strip.background = element_blank(),\n    strip.text = element_blank()     \n  ) +\n  labs(x = NULL, y = NULL, fill = \"Method\", subtitle =  glue::glue(\"{plot_df50$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df50$rel[1])}\"))\n\np2 <-\n  plot_df |>\n  filter(param %in% growthpars) |> \n  summarise(count = n(), .by = dist:param) |>\n  ggplot(aes(count, param, fill = method)) +\n  geom_col(width = 0.8, position = position_dodge()) +\n  geom_vline(xintercept = 2000, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_x_continuous(expand = c(0, 0, 0, 100)) +\n  facet_grid(n ~ .) +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(), \n    axis.ticks.y = element_blank(), \n    axis.text.x = element_text(size = 7.5), \n    legend.position = \"none\"\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \" \")\n\ncowplot::plot_grid(p1, p2, rel_widths = c(1, 1 / 3))\n```\n\n::: {.cell-output-display}\n![Centered distributions of estimates for the growth curve model (left panel) using normally generated data at 50% reliability. Non-convergence cases and extreme estimates have been excluded. The right panel displays the number of estimates used to compute the summary statistics.](index_files/figure-pdf/fig-centdist-growth-1.pdf){#fig-centdist-growth width=100%}\n:::\n:::\n\n\n\n\n::: {#cell-fig-perf-growth .cell}\n\n```{.r .cell-code .hidden}\nplot_df |>\n  filter(param %in% growthpars) |> \n  summarise(\n    B = mean(bias, na.rm = TRUE, trim = 0.05),\n    rmse = sqrt(mean(bias ^ 2, na.rm = TRUE, trim = 0.05)),\n    pu = mean(bias < 0),\n    covr = mean(covered, na.rm = TRUE),\n    .by = c(dist:param)\n  ) |>\n  pivot_longer(B:covr, names_to = \"metric\", values_to = \"value\") |>\n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"B\", \"rmse\", \"pu\", \"covr\"),\n      labels = c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")\n    )\n  ) |>\n  ggplot(aes(value, param, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  geom_vline(\n    data = tibble(\n      metric = factor(c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")),\n      value = c(0, 0, 0.5, 0.95)\n    ),\n    aes(xintercept = value),\n    linetype = \"dashed\"\n  ) +\n  scale_fill_manual(values = mycols) +\n  facet_grid(n ~ metric, scales = \"free_x\") +\n  ggh4x::facetted_pos_scales(\n    x = list(\n      scale_x_continuous(),\n      scale_x_continuous(expand = c(0, 0, 0, 10)),\n      scale_x_continuous(limits = c(0.35, 0.65), labels = scales::percent),\n      scale_x_continuous(limits = c(0.6, 1), labels = scales::percent)\n    )\n  ) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(Psi[\"1,2\"])\n  ))) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"bottom\")) +\n  theme_bw() +\n  theme(\n    axis.text.x = element_text(size = 7.5),\n    legend.key.height = unit(1, \"pt\"), \n    legend.key.width = unit(9, \"pt\")\n  ) +  \n  labs(x = NULL, y = NULL, fill = NULL, subtitle = glue::glue(\"{plot_df$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df$rel[1])}\"))\n```\n\n::: {.cell-output-display}\n![Performance metrics (relative bias, RMSE, probability of understimation, and coverage) of the ML, eBR, and iBR methods for estimation of the growth curve model. Vertical dashed lines indicate the ideal values for each metric.](index_files/figure-pdf/fig-perf-growth-1.pdf){#fig-perf-growth width=100%}\n:::\n:::\n\n\nNext, we compared our methods to the competitor methods, including REML.\nIt is noted that the constraint of equal error variances across all 10 items contributed to the stabilisation of estimates, promoting bias reduction via averages.\nThis is seen clearly in the previous figures, as well as when comparing across methods (see @fig-meanbias-growth).\nIn the scenario involving normally distributed data, REML generally provided the lowest relative mean bias among all methods.\nThis result was expected given REMLâ€™s theoretical optimality in estimating variance components under normality. \nOn the flip side, REML exhibited substantially poorer performance under non-normal data conditions, underscoring its lack of robustness against departures from normality.\n\nThe results are, on the whole, expected.\nThe bias performance of our proposed eRBM and iRBM methods was consistently comparable--and occasionally superior--to existing resampling-based methods (Jackknife, Bootstrap) and the analytic Ozenne approach. \nUnlike REML, our methods maintained robustness in non-normal settings without compromising performance.\n\nIn low reliability scenarios, the estimate of the latent slope-intercept covariance $\\Psi_{1,2}$ tended to be the most unstable in small samples. \nOur methods did a great job in reducing the bias of this parameter, considering both ML and REML gave much greater bias.\nNotably, for $n\\geq 100$, differences between methods converge, with all methods yielding similar performance.\nThis emphasises that bias reduction is most valueable in small to moderate sample regimes, where ML is particularly vulnerable.\n\n\n::: {#cell-fig-meanbias-growth .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"growth\", !method %in% c(\"lav\")) |>\n  ggplot(aes(n, relbias, col = method)) +\n  geom_line(linewidth = 0.75, alpha = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_color_manual(values = mycols) +\n  scale_x_continuous(labels = c(15, 20, 50, 100, 1000)) +\n  scale_y_continuous(labels = scales::percent) +\n  # coord_cartesian(ylim = c(-0.25, 0.25)) +\n  guides(colour = guide_legend(nrow = 1, reverse = TRUE, position = \"top\")) +\n  theme_bw() +\n  labs(x = \"Sample size (n)\", y = \"Relative mean bias\", col = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of relative mean bias of the ML, eBR, and iBR methods against the D&R methods for estimation of the growth curve model.](index_files/figure-pdf/fig-meanbias-growth-1.pdf){#fig-meanbias-growth width=100%}\n:::\n:::\n\n\n\n::: {#cell-fig-rmse-growth .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"growth\", !method %in% c(\"lav\")) |>\n  mutate(method = fct_rev(method)) |>\n  ggplot(aes(n, rmse, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  scale_x_continuous(breaks = 1:5, labels = c(15, 20, 50, 100, 1000)) +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_fill_manual(values = mycols) +\n  # coord_cartesian(ylim = c(0, 280)) +\n  guides(fill = guide_legend(nrow = 1, position = \"bottom\")) +\n  theme_bw() +\n  theme(legend.key.height = unit(1, \"pt\"), legend.key.width = unit(9, \"pt\")) +\n  labs(x = \"Sample size (n)\", y = \"RMSE\", fill = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of RMSE of the ML, eBR, and iBR methods against the D&R methods for estimation of the growth curve model.](index_files/figure-pdf/fig-rmse-growth-1.pdf){#fig-rmse-growth width=100%}\n:::\n:::\n\n\n\n## Discussion\n\n\n- Both the implicit (iRBM) and explicit (eRBM) bias-reduction methods demonstrated high convergence rates across various simulation scenarios, particularly for sample sizes $n\\geq 50$.\n- In both the two-factor model and latent GCM, iRBM and eRBM consistently reduced bias compared to maximum likelihood (ML) estimates, especially in small sample sizes. These methods also enhanced the coverage probabilities of confidence intervals. The eRBM and iRBM also showed robustness under non-normal data distributions for the GCM especially.\n- While resampling-based methods like Jackknife and Bootstrap occasionally achieved slightly lower bias, they often exhibited higher variability. Our methods provided a favourable balance between bias reduction and estimate stability.\n- Restricted Maximum Likelihood (REML) performed well under normal data conditions but showed significant bias when the data deviated from normality, highlighting its sensitivity to distributional assumptions.\n- The eRBM and iRBM method offered a computationally efficient alternative to traditional resampling techniques, achieving bias reduction with significantly lower computational cost. Further improvements could be made to the codebase to increase efficiency--at the moment it is a very naive implementation.\n- Future work:\n  - To address computational problems, consider plugin penalties and/or bounded estimation (??)\n  - More complex models\n\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n\n## Appendix {.unnumbered}\n\n### A Derivatives {.unnumbered}\n\n#### A1 General form {.unnumbered}\n\nWe derive the derivatives of @eq-sem_lik with respect to each of the possible parameters in a SEM. These derivatives are used to construct the bias-reducing adjustments for our two example models. Denoting $\\theta$ to be the collection of all free parameters in the model, we observe that for both the growth model and two factor model, each element of $\\theta$ appears in either $\\mu(\\theta)$ or $\\Sigma(\\theta)$, but not both. Therefore the derivation is divided into each of these two possibilities. \n\nConsider the gradient of $\\ell(\\theta)$ with respect to $\\theta_{\\mu}$, the vector of all elements of $\\theta$ appearing in $\\mu$. This is given by\n\n\n\\begin{align}\n\\nabla_{\\theta_\\mu} \\ell(\\theta)\n&= J(\\mu; \\theta_\\mu)^\\top \\nabla_\\mu \\ell(\\theta) \\\\\n&= \\sum_{i=1}^n J(\\mu; \\theta_\\mu)^\\top \\Sigma^{-1}(y_i - \\mu)\\\\\n&= n J(\\mu; \\theta_\\mu)^\\top \\Sigma^{-1} (\\bar{y} - \\mu),\n\\end{align}\n\nwhere $\\bar{y} = n^{-1}\\sum_{i=1}^n y_i$ and $J(\\mu; \\theta_{\\mu})$ is a Jacobian matrix with $i$-th row $\\left[\\frac{\\partial \\mu_i}{\\partial \\theta_{\\mu, 1}}  \\frac{\\partial \\mu_i}{\\partial \\theta_{\\mu, 2}} \\cdots \\right]$.\n\n\n\nThe derivative of the log-likelihood with respect to a single component of $\\theta$ appearing in $\\Sigma(\\mu)$, say $\\theta_{\\Sigma, t}$, is as follows:\n\n\n\\begin{align}\n\\frac{\\partial\\ell(\\theta)}{\\partial \\theta_{\\Sigma, t}} \n&= -\\frac{n}{2} \\left[ \\frac{\\partial}{\\partial \\theta_{\\Sigma, t}} \\log|\\Sigma |\\right] - \\frac{1}{2}\\sum_{i=1}^n (y_i - \\mu)^\\top \\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}} \\Sigma^{-1}\\right] (y_i - \\mu)  \\\\\n&= -\\frac{n}{2}\\text{tr}  \\left\\{ \\Sigma^{-1} \\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}}\\Sigma \\right]\\right\\}  + \\frac{1}{2}\\sum_{i=1}^n (y_i - \\mu)^\\top \\Sigma^{-1}\\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}} \\Sigma\\right] \\Sigma^{-1} (y_i - \\mu)\\\\\n&= -\\frac{n}{2}\\text{tr}  \\left\\{ \\Sigma^{-1} \\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}}\\Sigma \\right] \\right\\}  + \\frac{1}{2}\\sum_{i=1}^n \\text{tr}\\left\\{(y_i - \\mu)(y_i - \\mu)^\\top \\Sigma^{-1}\\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}} \\Sigma\\right] \\Sigma^{-1} \\right\\} \\\\\n&= -\\frac{n}{2}\\text{tr}  \\left\\{ \\Sigma^{-1} \\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}}\\Sigma\\right] \\right\\}  + \\frac{n}{2} \\text{tr}\\left\\{ \\Sigma^{-1}S \\Sigma^{-1}\\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}} \\Sigma\\right]\\right\\}  \\\\\n&=\\frac{n}{2}\\text{tr}\\left\\{ E \\left[\\frac{\\partial}{\\partial \\theta_{\\Sigma, t}} \\Sigma\\right]\\right\\},\n\\end{align}\n\n\n\nwhere $S$ is the biased sample covariance matrix $n^{-1}\\sum_{i=1}^n(y_i - \\mu)(y_i - \\mu)^\\top$, and $E = \\Sigma^{-1}(S-\\Sigma)\\Sigma^{-1}$.\n\n#### A2 Growth curve model {.unnumbered}\n\nFor the growth curve model, we have a latent intercept $i$ and slope $s$. There are five parameters in the latent component: Two intercepts $\\alpha_1$ and $\\alpha_2$, and the three unique variances and covariances $\\Psi_{11}$, $\\Psi_{22}$, and $\\Psi_{12}$. The loadings for the intercept latent variable are all fixed to 1, whereas the loadings for the slope latent variable increment from 0 to 9. Thus, $\\Lambda$ is some fixed $10 \\times 2$ matrix. Furthermore, the observed variables $y_j$ share a common residual error variance $v$. In total, there are six parameters to be estimated in the model: $\\theta = (\\Psi_{11}, \\alpha_1, \\Psi_{22}, \\alpha_2, \\Psi_{12}, v)$. The two elements of the vector $\\alpha$ appear in $\\mu(\\theta)$, while the other parameters all appear in $\\Sigma(\\theta)$.\n\nThe gradients are given by:\n\n- $\\frac{\\partial\\ell(\\theta)}{\\partial \\alpha} = n \\Lambda^\\top \\Sigma^{-1} (\\bar y - \\mu)$\n<!-- - $\\frac{\\partial\\ell}{\\partial \\Psi} = \\frac{n}{2} \\Lambda E \\Lambda^\\top$. TODO: Edit -->\n- $\\frac{\\partial\\ell(\\theta)}{\\partial \\Psi_{ij}} = \\frac{n}{2} \\operatorname{tr} \\left\\{\\Lambda^\\top E \\Lambda \\frac{\\partial \\Psi}{\\partial \\Psi_{ij}} \\right\\}$\n- $\\frac{\\partial\\ell(\\theta)}{\\partial v} = \\frac{n}{2} \\operatorname{tr}(E)$\n\n#### A3 Two factor model {.unnumbered}\n\nFor the two factor model, we have two latent variables $\\eta_1$ and $\\eta_2$, each indicated by three observed variables, $(y_1, y_2, y_3)$ and $(y_4, y_5, y_6)$ respectively. Each observed variable has a corresponding error $\\epsilon_j \\sim N(0, \\Theta_{j,j})$, leading to six variance parameters. The latent variables have a regression path from $\\eta_1$ to $\\eta_2$ with parameter $\\beta$, and each have a variance parameter $\\Psi_{11}$ and $\\Psi_{22}$ respectively. For the factor loadings, we fix $\\Lambda_{11}$ and $\\Lambda_{42}$ to $1$ for identifiability. The thirteen parameters to be estimated in the two factor model are $\\theta = (\\Lambda_{21}, \\Lambda_{31}, \\Lambda_{52}, \\Lambda_{62}, \\beta, \\Theta_{11}, \\Theta_{22}, \\Theta_{33}, \\Theta_{44}, \\Theta_{55}, \\Theta_{66}, \\Psi_{11}, \\Psi_{22})$.\n\nWe express the covariance parameters as vectors of the diagonal entries of the covariance matrices, which have all other entries as zero. For the two factor model, all elements of $\\theta$ appear only in $\\Sigma(\\theta)$, with derivatives given by\n\n- $\\frac{\\partial \\ell(\\theta)}{\\partial \\Lambda_{ij}} = n \\operatorname{tr}\\left\\{ M \\Lambda^\\top E \\frac{\\partial\\Lambda}{\\partial \\Lambda_{ij}} \\right\\} = n[M\\Lambda^\\top E]_{j i}$\n- $\\frac{\\partial \\ell(\\theta)}{\\partial \\beta} = n \\operatorname{tr}\\left\\{ \\Psi (I + B^\\top) \\Lambda^\\top E \\Lambda \\frac{\\partial B}{\\partial \\beta} \\right\\} = n[\\Psi (I + B^\\top) \\Lambda^\\top E \\Lambda]_{12}$\n- $\\frac{\\partial \\ell(\\theta)}{\\partial \\text{diag}(\\Theta)} = \\frac{n}{2}\\text{diag}(E)$\n- $\\frac{\\partial \\ell(\\theta)}{\\partial \\text{diag}(\\Psi)} = \\frac{n}{2}\\text{diag}\\left((I - B)^{-\\top}\\Lambda^\\top E \\Lambda (I-B)^{-1}\\right)$,\n\n\nwhere $M = (I - B)^{-1} \\Psi (I - B)^{-\\top}$.\n\n### B Tables {.unnumbered}\n\n#### Two-factor model\n\n\n::: {#tbl-bias-twofac-80 .cell .column-page tbl-cap='Results (trimmed) two-factor model reliability 0.80.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_twofac_80_df, \"twofac\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.15 & -0.11 & -0.05 & -0.01 & 0.00 & -0.20 & -0.14 & -0.07 & -0.02 & -0.01 & -0.19 & -0.14 & -0.07 & -0.02 & 0.00 & -0.30 & -0.22 & -0.11 & -0.04 & -0.01 & 0.54 & 0.45 & 0.27 & 0.20 & 0.06 & 0.68 & 0.61 & 0.40 & 0.28 & 0.09 \\\\ \neRBM & -0.10 & -0.08 & -0.03 & 0.00 & 0.00 & -0.21 & -0.13 & -0.05 & -0.01 & 0.00 & -0.15 & -0.11 & -0.05 & -0.01 & 0.00 & -0.31 & -0.21 & -0.10 & -0.03 & 0.00 & 0.52 & 0.44 & 0.27 & 0.20 & 0.06 & 0.64 & 0.58 & 0.40 & 0.29 & 0.09 \\\\ \niRBM & -0.11 & -0.07 & -0.03 & 0.00 & 0.00 & -0.16 & -0.11 & -0.05 & -0.01 & 0.00 & -0.15 & -0.11 & -0.05 & -0.01 & 0.00 & -0.27 & -0.19 & -0.10 & -0.03 & 0.00 & 0.53 & 0.45 & 0.27 & 0.20 & 0.06 & 0.69 & 0.61 & 0.40 & 0.29 & 0.09 \\\\ \nJackknife & 0.00 & 0.00 & -0.01 & 0.01 & 0.00 & -0.04 & -0.03 & -0.03 & 0.00 & 0.00 & -0.04 & -0.03 & -0.03 & 0.00 & 0.00 & -0.18 & -0.12 & -0.08 & -0.02 & 0.00 & 0.66 & 0.51 & 0.28 & 0.20 & 0.06 & 0.87 & 0.71 & 0.41 & 0.29 & 0.09 \\\\ \nBootstrap & -0.04 & -0.02 & -0.01 & 0.01 & 0.00 & -0.10 & -0.06 & -0.04 & -0.01 & 0.00 & -0.07 & -0.05 & -0.03 & 0.00 & 0.00 & -0.20 & -0.14 & -0.08 & -0.02 & -0.01 & 0.59 & 0.50 & 0.28 & 0.20 & 0.06 & 0.73 & 0.65 & 0.42 & 0.29 & 0.09 \\\\ \nOzenne et al. & -0.09 & -0.06 & -0.03 & 0.00 & 0.00 & -0.14 & -0.10 & -0.06 & -0.01 & 0.00 & -0.14 & -0.09 & -0.05 & -0.01 & 0.00 & -0.25 & -0.18 & -0.10 & -0.03 & -0.01 & 0.56 & 0.47 & 0.28 & 0.20 & 0.06 & 0.71 & 0.63 & 0.41 & 0.29 & 0.09 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.08 & -0.05 & -0.02 & -0.02 & 0.00 & -0.09 & -0.09 & -0.04 & -0.02 & 0.00 & -0.11 & -0.08 & -0.03 & -0.02 & 0.00 & -0.20 & -0.17 & -0.07 & -0.04 & 0.00 & 0.40 & 0.35 & 0.22 & 0.16 & 0.05 & 0.57 & 0.50 & 0.35 & 0.26 & 0.08 \\\\ \neRBM & -0.07 & -0.06 & -0.03 & -0.02 & 0.00 & -0.04 & -0.06 & -0.04 & -0.03 & 0.00 & -0.11 & -0.08 & -0.04 & -0.03 & 0.00 & -0.14 & -0.15 & -0.08 & -0.05 & 0.00 & 0.40 & 0.35 & 0.22 & 0.16 & 0.05 & 0.56 & 0.50 & 0.35 & 0.26 & 0.08 \\\\ \niRBM & -0.09 & -0.06 & -0.03 & -0.02 & 0.00 & -0.09 & -0.09 & -0.04 & -0.02 & 0.00 & -0.12 & -0.09 & -0.04 & -0.02 & 0.00 & -0.19 & -0.18 & -0.08 & -0.05 & 0.00 & 0.40 & 0.35 & 0.22 & 0.16 & 0.05 & 0.57 & 0.51 & 0.35 & 0.26 & 0.08 \\\\ \nJackknife & 0.01 & 0.00 & -0.01 & -0.01 & 0.00 & 0.01 & -0.01 & -0.02 & -0.02 & 0.00 & -0.04 & -0.03 & -0.02 & -0.02 & 0.00 & -0.13 & -0.11 & -0.06 & -0.04 & 0.00 & 0.47 & 0.39 & 0.22 & 0.16 & 0.05 & 0.70 & 0.58 & 0.36 & 0.26 & 0.08 \\\\ \nBootstrap & 0.03 & 0.03 & 0.01 & 0.00 & 0.00 & 0.01 & 0.02 & 0.03 & 0.02 & 0.00 & -0.02 & 0.01 & 0.00 & -0.01 & 0.00 & -0.12 & -0.09 & -0.01 & -0.02 & 0.00 & 0.45 & 0.39 & 0.23 & 0.16 & 0.05 & 0.66 & 0.58 & 0.39 & 0.28 & 0.08 \\\\ \nOzenne et al. & -0.01 & 0.00 & 0.00 & -0.01 & 0.00 & -0.03 & -0.04 & -0.02 & -0.01 & 0.00 & -0.05 & -0.03 & -0.01 & -0.01 & 0.00 & -0.15 & -0.13 & -0.05 & -0.03 & 0.00 & 0.42 & 0.36 & 0.22 & 0.16 & 0.05 & 0.60 & 0.52 & 0.35 & 0.26 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.15 & -0.12 & -0.05 & -0.02 & 0.00 & -0.19 & -0.15 & -0.07 & -0.04 & 0.00 & -0.20 & -0.15 & -0.05 & -0.03 & 0.00 & -0.28 & -0.23 & -0.11 & -0.06 & 0.00 & 0.41 & 0.37 & 0.23 & 0.16 & 0.05 & 0.54 & 0.50 & 0.35 & 0.25 & 0.08 \\\\ \neRBM & -0.07 & -0.07 & -0.03 & -0.01 & 0.00 & -0.09 & -0.09 & -0.05 & -0.03 & 0.00 & -0.13 & -0.10 & -0.04 & -0.02 & 0.00 & -0.19 & -0.17 & -0.09 & -0.05 & 0.00 & 0.41 & 0.37 & 0.23 & 0.16 & 0.05 & 0.54 & 0.50 & 0.35 & 0.25 & 0.08 \\\\ \niRBM & -0.10 & -0.08 & -0.03 & -0.01 & 0.00 & -0.14 & -0.12 & -0.05 & -0.03 & 0.00 & -0.15 & -0.12 & -0.04 & -0.02 & 0.00 & -0.23 & -0.20 & -0.09 & -0.05 & 0.00 & 0.41 & 0.37 & 0.23 & 0.16 & 0.05 & 0.54 & 0.51 & 0.35 & 0.25 & 0.08 \\\\ \nJackknife & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & -0.04 & -0.04 & -0.03 & -0.02 & 0.00 & -0.06 & -0.05 & -0.02 & -0.01 & 0.00 & -0.14 & -0.13 & -0.07 & -0.04 & 0.00 & 0.47 & 0.39 & 0.24 & 0.16 & 0.05 & 0.63 & 0.55 & 0.36 & 0.25 & 0.08 \\\\ \nBootstrap & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.05 & -0.02 & 0.01 & 0.00 & 0.00 & -0.07 & -0.05 & -0.01 & -0.01 & 0.00 & -0.16 & -0.13 & -0.04 & -0.03 & 0.00 & 0.45 & 0.39 & 0.24 & 0.16 & 0.05 & 0.61 & 0.56 & 0.38 & 0.27 & 0.08 \\\\ \nOzenne et al. & -0.09 & -0.07 & -0.03 & -0.01 & 0.00 & -0.13 & -0.11 & -0.05 & -0.03 & 0.00 & -0.14 & -0.11 & -0.04 & -0.02 & 0.00 & -0.23 & -0.19 & -0.09 & -0.05 & 0.00 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.56 & 0.51 & 0.35 & 0.25 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\beta\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.03 & 0.05 & -0.02 & 0.01 & 0.00 & 0.03 & -0.04 & -0.02 & 0.01 & 0.00 & -0.01 & 0.02 & -0.02 & 0.01 & 0.00 & -0.06 & -0.11 & -0.05 & 0.00 & 0.00 & 1.21 & 1.02 & 0.57 & 0.40 & 0.12 & 1.32 & 1.07 & 0.59 & 0.40 & 0.12 \\\\ \neRBM & 0.00 & 0.03 & -0.02 & 0.01 & 0.00 & -0.04 & -0.08 & -0.04 & 0.00 & 0.00 & -0.05 & 0.00 & -0.03 & 0.01 & 0.00 & -0.16 & -0.17 & -0.07 & -0.01 & 0.00 & 1.15 & 1.00 & 0.57 & 0.40 & 0.12 & 1.20 & 1.01 & 0.59 & 0.40 & 0.12 \\\\ \niRBM & 0.00 & 0.04 & -0.02 & 0.01 & 0.00 & -0.04 & -0.08 & -0.03 & 0.00 & 0.00 & -0.04 & 0.02 & -0.03 & 0.01 & 0.00 & -0.15 & -0.15 & -0.07 & 0.00 & 0.00 & 1.17 & 1.01 & 0.57 & 0.40 & 0.12 & 1.26 & 1.03 & 0.59 & 0.40 & 0.12 \\\\ \nJackknife & -0.06 & 0.00 & -0.03 & 0.01 & 0.00 & -0.09 & -0.12 & -0.05 & 0.00 & 0.00 & -0.09 & -0.02 & -0.03 & 0.01 & 0.00 & -0.23 & -0.25 & -0.08 & -0.01 & 0.00 & 1.29 & 1.03 & 0.57 & 0.40 & 0.12 & 1.71 & 1.24 & 0.59 & 0.40 & 0.12 \\\\ \nBootstrap & -0.02 & 0.02 & -0.03 & 0.01 & 0.00 & -0.02 & -0.09 & -0.05 & -0.01 & 0.00 & -0.09 & -0.01 & -0.03 & 0.00 & 0.00 & -0.15 & -0.19 & -0.09 & -0.01 & 0.00 & 1.18 & 1.00 & 0.57 & 0.40 & 0.12 & 1.37 & 1.07 & 0.59 & 0.40 & 0.12 \\\\ \nOzenne et al. & 0.03 & 0.05 & -0.02 & 0.01 & 0.00 & 0.03 & -0.04 & -0.02 & 0.01 & 0.00 & -0.01 & 0.02 & -0.02 & 0.01 & 0.00 & -0.06 & -0.11 & -0.05 & 0.00 & 0.00 & 1.21 & 1.02 & 0.57 & 0.40 & 0.12 & 1.33 & 1.07 & 0.59 & 0.40 & 0.12 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\lambda_{2,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.02 & 0.01 & 0.00 & 0.00 & 0.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.21 & 0.18 & 0.10 & 0.07 & 0.02 & 0.23 & 0.19 & 0.11 & 0.07 & 0.02 \\\\ \neRBM & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.02 & -0.02 & -0.01 & 0.00 & 0.00 & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & 0.19 & 0.16 & 0.10 & 0.07 & 0.02 & 0.19 & 0.17 & 0.10 & 0.07 & 0.02 \\\\ \niRBM & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & -0.01 & 0.00 & 0.00 & 0.21 & 0.17 & 0.10 & 0.07 & 0.02 & 0.23 & 0.19 & 0.11 & 0.07 & 0.02 \\\\ \nJackknife & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.04 & -0.02 & -0.02 & 0.00 & 0.00 & 0.22 & 0.17 & 0.10 & 0.07 & 0.02 & 0.31 & 0.22 & 0.11 & 0.07 & 0.02 \\\\ \nBootstrap & -0.01 & -0.02 & -0.01 & 0.00 & 0.00 & -0.01 & -0.01 & -0.02 & 0.00 & 0.00 & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.02 & -0.01 & -0.02 & 0.00 & 0.00 & 0.21 & 0.17 & 0.10 & 0.07 & 0.02 & 0.25 & 0.20 & 0.11 & 0.07 & 0.02 \\\\ \nOzenne et al. & 0.02 & 0.01 & 0.00 & 0.00 & 0.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.21 & 0.18 & 0.10 & 0.07 & 0.02 & 0.23 & 0.19 & 0.11 & 0.07 & 0.02 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n::: {#tbl-bias-twofac-50 .cell .column-page tbl-cap='Results (trimmed) two-factor model reliability 0.50.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_twofac_50_df, \"twofac\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.16 & -0.13 & -0.06 & -0.02 & 0.00 & -0.23 & -0.18 & -0.08 & -0.03 & -0.01 & -0.17 & -0.13 & -0.07 & -0.02 & 0.00 & -0.31 & -0.23 & -0.12 & -0.05 & -0.01 & 0.57 & 0.49 & 0.29 & 0.20 & 0.06 & 0.69 & 0.63 & 0.42 & 0.29 & 0.09 \\\\ \neRBM & -0.21 & -0.15 & -0.03 & 0.00 & 0.00 & -0.32 & -0.26 & -0.07 & -0.01 & 0.00 & -0.24 & -0.16 & -0.04 & -0.01 & 0.00 & -0.41 & -0.30 & -0.10 & -0.03 & -0.01 & 0.49 & 0.41 & 0.27 & 0.20 & 0.06 & 0.60 & 0.53 & 0.38 & 0.28 & 0.09 \\\\ \niRBM & -0.11 & -0.08 & -0.03 & 0.00 & 0.00 & -0.19 & -0.12 & -0.06 & -0.01 & 0.00 & -0.12 & -0.08 & -0.04 & -0.01 & 0.00 & -0.27 & -0.19 & -0.09 & -0.03 & -0.01 & 0.58 & 0.51 & 0.29 & 0.20 & 0.06 & 0.72 & 0.66 & 0.42 & 0.29 & 0.09 \\\\ \nJackknife & 0.04 & 0.03 & 0.00 & 0.01 & 0.00 & 0.00 & 0.02 & -0.01 & 0.00 & 0.00 & 0.01 & 0.03 & -0.01 & 0.01 & 0.00 & -0.11 & -0.09 & -0.06 & -0.02 & 0.00 & 1.05 & 0.85 & 0.30 & 0.20 & 0.06 & 1.23 & 1.06 & 0.45 & 0.29 & 0.09 \\\\ \nBootstrap & -0.06 & -0.03 & 0.00 & 0.01 & 0.00 & -0.13 & -0.07 & -0.02 & 0.01 & 0.00 & -0.05 & -0.01 & -0.01 & 0.00 & 0.00 & -0.22 & -0.14 & -0.06 & -0.02 & 0.00 & 0.68 & 0.60 & 0.31 & 0.20 & 0.06 & 0.79 & 0.74 & 0.47 & 0.30 & 0.09 \\\\ \nOzenne et al. & -0.10 & -0.08 & -0.04 & -0.01 & 0.00 & -0.17 & -0.13 & -0.07 & -0.02 & -0.01 & -0.11 & -0.08 & -0.05 & -0.01 & 0.00 & -0.25 & -0.19 & -0.10 & -0.04 & -0.01 & 0.61 & 0.50 & 0.29 & 0.20 & 0.06 & 0.73 & 0.65 & 0.42 & 0.29 & 0.09 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.02 & 0.01 & 0.00 & -0.01 & 0.00 & 0.06 & 0.04 & 0.02 & -0.01 & 0.00 & -0.08 & -0.07 & -0.03 & -0.03 & 0.00 & -0.11 & -0.09 & -0.03 & -0.04 & 0.00 & 0.66 & 0.60 & 0.39 & 0.27 & 0.08 & 0.79 & 0.70 & 0.49 & 0.34 & 0.11 \\\\ \neRBM & 0.20 & 0.12 & -0.02 & -0.03 & 0.00 & 0.42 & 0.26 & 0.02 & -0.03 & 0.00 & 0.10 & 0.05 & -0.06 & -0.05 & 0.00 & 0.25 & 0.13 & -0.04 & -0.06 & -0.01 & 0.59 & 0.54 & 0.37 & 0.27 & 0.08 & 0.88 & 0.70 & 0.44 & 0.33 & 0.11 \\\\ \niRBM & -0.01 & -0.02 & -0.03 & -0.02 & 0.00 & 0.02 & -0.01 & -0.01 & -0.03 & 0.00 & -0.12 & -0.11 & -0.06 & -0.04 & 0.00 & -0.15 & -0.13 & -0.06 & -0.06 & -0.01 & 0.68 & 0.63 & 0.39 & 0.27 & 0.08 & 0.81 & 0.73 & 0.49 & 0.34 & 0.11 \\\\ \nJackknife & 0.02 & -0.02 & -0.02 & -0.02 & 0.00 & 0.05 & 0.01 & 0.00 & -0.02 & 0.00 & -0.19 & -0.17 & -0.06 & -0.04 & 0.00 & -0.25 & -0.19 & -0.07 & -0.06 & 0.00 & 1.34 & 1.07 & 0.42 & 0.27 & 0.08 & 1.47 & 1.21 & 0.55 & 0.34 & 0.11 \\\\ \nBootstrap & 0.02 & 0.01 & -0.01 & -0.02 & 0.00 & 0.04 & 0.01 & 0.02 & -0.02 & 0.00 & -0.12 & -0.10 & -0.05 & -0.04 & 0.00 & -0.14 & -0.14 & -0.04 & -0.05 & 0.00 & 0.81 & 0.72 & 0.42 & 0.28 & 0.08 & 0.92 & 0.83 & 0.55 & 0.36 & 0.11 \\\\ \nOzenne et al. & 0.09 & 0.06 & 0.02 & 0.00 & 0.00 & 0.13 & 0.09 & 0.04 & 0.00 & 0.00 & -0.02 & -0.02 & -0.01 & -0.02 & 0.00 & -0.05 & -0.04 & -0.01 & -0.03 & 0.00 & 0.71 & 0.63 & 0.39 & 0.27 & 0.08 & 0.85 & 0.74 & 0.50 & 0.34 & 0.11 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.13 & -0.12 & -0.04 & -0.02 & 0.00 & -0.14 & -0.11 & -0.05 & -0.03 & 0.00 & -0.25 & -0.18 & -0.07 & -0.04 & 0.00 & -0.30 & -0.21 & -0.10 & -0.06 & 0.00 & 0.65 & 0.57 & 0.38 & 0.26 & 0.08 & 0.74 & 0.66 & 0.46 & 0.32 & 0.10 \\\\ \neRBM & 0.06 & 0.04 & -0.03 & -0.02 & 0.00 & 0.10 & 0.12 & -0.02 & -0.03 & 0.00 & -0.01 & -0.04 & -0.06 & -0.04 & 0.00 & -0.06 & -0.02 & -0.07 & -0.06 & 0.00 & 0.62 & 0.52 & 0.38 & 0.26 & 0.08 & 0.71 & 0.65 & 0.44 & 0.32 & 0.10 \\\\ \niRBM & -0.09 & -0.07 & -0.04 & -0.02 & 0.00 & -0.08 & -0.07 & -0.05 & -0.03 & 0.00 & -0.20 & -0.16 & -0.07 & -0.03 & 0.00 & -0.25 & -0.19 & -0.09 & -0.06 & 0.00 & 0.69 & 0.62 & 0.40 & 0.26 & 0.08 & 0.81 & 0.72 & 0.47 & 0.33 & 0.10 \\\\ \nJackknife & 0.01 & -0.01 & -0.02 & -0.01 & 0.00 & -0.01 & -0.01 & -0.02 & -0.02 & 0.00 & -0.19 & -0.11 & -0.05 & -0.03 & 0.00 & -0.24 & -0.16 & -0.07 & -0.05 & 0.00 & 1.19 & 0.97 & 0.42 & 0.26 & 0.08 & 1.31 & 1.12 & 0.52 & 0.33 & 0.10 \\\\ \nBootstrap & -0.04 & -0.03 & -0.01 & -0.01 & 0.00 & -0.07 & -0.03 & -0.01 & -0.02 & 0.00 & -0.17 & -0.11 & -0.05 & -0.02 & 0.00 & -0.24 & -0.15 & -0.07 & -0.04 & 0.00 & 0.76 & 0.69 & 0.42 & 0.27 & 0.08 & 0.86 & 0.78 & 0.51 & 0.34 & 0.10 \\\\ \nOzenne et al. & -0.08 & -0.07 & -0.02 & -0.01 & 0.00 & -0.08 & -0.06 & -0.03 & -0.02 & 0.00 & -0.20 & -0.14 & -0.05 & -0.03 & 0.00 & -0.26 & -0.17 & -0.08 & -0.05 & 0.00 & 0.69 & 0.59 & 0.39 & 0.26 & 0.08 & 0.79 & 0.69 & 0.46 & 0.32 & 0.10 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\beta\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.09 & 0.14 & 0.00 & 0.03 & 0.00 & 0.16 & 0.05 & 0.00 & 0.01 & 0.01 & -0.09 & -0.03 & -0.05 & 0.01 & 0.00 & -0.11 & -0.13 & -0.06 & -0.02 & 0.00 & 1.79 & 1.54 & 0.79 & 0.53 & 0.16 & 1.95 & 1.55 & 0.82 & 0.55 & 0.16 \\\\ \neRBM & 0.22 & 0.21 & -0.03 & 0.01 & -0.01 & 0.09 & 0.01 & -0.07 & -0.02 & 0.00 & 0.20 & 0.12 & -0.06 & -0.01 & 0.00 & 0.01 & -0.05 & -0.11 & -0.05 & 0.00 & 1.46 & 1.27 & 0.73 & 0.52 & 0.16 & 1.51 & 1.27 & 0.74 & 0.54 & 0.16 \\\\ \niRBM & -0.01 & 0.02 & -0.03 & 0.02 & -0.01 & 0.02 & -0.08 & -0.05 & -0.01 & 0.00 & -0.22 & -0.16 & -0.08 & 0.00 & 0.00 & -0.30 & -0.33 & -0.11 & -0.04 & 0.00 & 1.61 & 1.43 & 0.78 & 0.52 & 0.16 & 1.84 & 1.49 & 0.82 & 0.55 & 0.16 \\\\ \nJackknife & 0.01 & 0.06 & -0.07 & 0.00 & -0.01 & 0.22 & -0.15 & -0.09 & -0.03 & 0.00 & -0.24 & -0.15 & -0.09 & -0.02 & 0.00 & -0.19 & -0.35 & -0.16 & -0.06 & 0.00 & 4.06 & 2.65 & 0.82 & 0.52 & 0.16 & 4.68 & 3.12 & 0.92 & 0.54 & 0.16 \\\\ \nBootstrap & 0.04 & 0.10 & -0.06 & 0.00 & -0.01 & 0.20 & -0.02 & -0.08 & -0.03 & 0.00 & -0.11 & -0.05 & -0.10 & -0.02 & 0.00 & -0.08 & -0.23 & -0.13 & -0.06 & 0.00 & 2.28 & 1.71 & 0.78 & 0.52 & 0.16 & 2.60 & 1.93 & 0.83 & 0.55 & 0.16 \\\\ \nOzenne et al. & 0.08 & 0.13 & 0.00 & 0.03 & 0.00 & 0.14 & 0.05 & 0.00 & 0.01 & 0.01 & -0.10 & -0.04 & -0.05 & 0.01 & 0.00 & -0.12 & -0.14 & -0.06 & -0.02 & 0.00 & 1.79 & 1.55 & 0.79 & 0.53 & 0.16 & 1.95 & 1.57 & 0.82 & 0.55 & 0.16 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\lambda_{2,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.05 & 0.05 & 0.04 & 0.01 & 0.00 & 0.01 & 0.03 & 0.01 & 0.02 & 0.00 & -0.03 & -0.02 & 0.00 & 0.00 & 0.00 & -0.05 & -0.03 & -0.02 & 0.00 & 0.00 & 0.53 & 0.47 & 0.28 & 0.18 & 0.06 & 0.55 & 0.47 & 0.29 & 0.19 & 0.06 \\\\ \neRBM & -0.09 & -0.10 & -0.03 & -0.01 & 0.00 & -0.07 & -0.06 & -0.05 & -0.01 & 0.00 & -0.10 & -0.10 & -0.05 & -0.02 & 0.00 & -0.09 & -0.08 & -0.06 & -0.02 & -0.01 & 0.31 & 0.31 & 0.23 & 0.17 & 0.06 & 0.32 & 0.29 & 0.23 & 0.18 & 0.06 \\\\ \niRBM & 0.08 & 0.05 & 0.02 & 0.00 & 0.00 & 0.07 & 0.04 & 0.00 & 0.01 & 0.00 & -0.03 & -0.03 & -0.01 & -0.01 & 0.00 & -0.05 & -0.03 & -0.03 & -0.01 & -0.01 & 0.64 & 0.55 & 0.28 & 0.18 & 0.06 & 0.69 & 0.58 & 0.30 & 0.19 & 0.06 \\\\ \nJackknife & 0.03 & -0.01 & -0.04 & -0.01 & 0.00 & 0.10 & 0.03 & -0.06 & -0.01 & 0.00 & -0.14 & -0.12 & -0.06 & -0.02 & 0.00 & -0.10 & -0.08 & -0.07 & -0.02 & -0.01 & 1.37 & 0.89 & 0.26 & 0.17 & 0.06 & 1.53 & 1.15 & 0.31 & 0.18 & 0.06 \\\\ \nBootstrap & 0.10 & 0.04 & -0.02 & -0.01 & 0.00 & 0.10 & 0.08 & -0.03 & -0.01 & 0.00 & -0.06 & -0.08 & -0.05 & -0.02 & 0.00 & -0.05 & -0.03 & -0.06 & -0.02 & -0.01 & 0.76 & 0.59 & 0.28 & 0.18 & 0.06 & 0.82 & 0.66 & 0.31 & 0.19 & 0.06 \\\\ \nOzenne et al. & 0.05 & 0.05 & 0.04 & 0.01 & 0.00 & 0.02 & 0.04 & 0.01 & 0.02 & 0.00 & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & -0.05 & -0.02 & -0.02 & 0.00 & 0.00 & 0.55 & 0.48 & 0.28 & 0.18 & 0.06 & 0.56 & 0.48 & 0.29 & 0.19 & 0.06 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n::: {#tbl-covr-twofac .cell .column-page tbl-cap='Coverage rates 95% confidence interval for the two-factor model.'}\n\n```{.r .cell-code .hidden}\ntab_covr(covr_twofac_df, \"twofac\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Normal data} & \\multicolumn{10}{c}{Non-Normal data} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21}\n & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\theta_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.85 & 0.89 & 0.93 & 0.95 & 0.95 & 0.89 & 0.92 & 0.95 & 0.95 & 0.95 & 0.75 & 0.76 & 0.78 & 0.82 & 0.81 & 0.76 & 0.78 & 0.80 & 0.82 & 0.81 \\\\ \neRBM & 0.90 & 0.93 & 0.95 & 0.95 & 0.95 & 0.89 & 0.93 & 0.96 & 0.96 & 0.95 & 0.77 & 0.79 & 0.81 & 0.83 & 0.81 & 0.73 & 0.78 & 0.84 & 0.83 & 0.81 \\\\ \niRBM & 0.89 & 0.92 & 0.95 & 0.95 & 0.95 & 0.91 & 0.94 & 0.96 & 0.96 & 0.95 & 0.78 & 0.78 & 0.80 & 0.83 & 0.81 & 0.78 & 0.79 & 0.81 & 0.83 & 0.81 \\\\ \nlav & 0.84 & 0.88 & 0.93 & 0.95 & 0.95 & 0.87 & 0.91 & 0.94 & 0.95 & 0.95 & 0.73 & 0.75 & 0.78 & 0.82 & 0.81 & 0.75 & 0.77 & 0.79 & 0.82 & 0.81 \\\\ \nOzenne et al. & 0.88 & 0.91 & 0.94 & 0.95 & 0.95 & 0.91 & 0.93 & 0.95 & 0.95 & 0.95 & 0.77 & 0.78 & 0.79 & 0.82 & 0.81 & 0.78 & 0.79 & 0.80 & 0.82 & 0.81 \\\\ \nJackknife & 0.82 & 0.87 & 0.94 & 0.95 & 0.95 & 0.72 & 0.78 & 0.93 & 0.96 & 0.95 & 0.72 & 0.77 & 0.87 & 0.90 & 0.94 & 0.65 & 0.69 & 0.86 & 0.92 & 0.94 \\\\ \nBootstrap & 0.75 & 0.81 & 0.91 & 0.94 & 0.94 & 0.69 & 0.75 & 0.90 & 0.95 & 0.95 & 0.64 & 0.69 & 0.83 & 0.89 & 0.94 & 0.57 & 0.62 & 0.79 & 0.89 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.86 & 0.88 & 0.93 & 0.94 & 0.94 & 0.85 & 0.88 & 0.91 & 0.93 & 0.94 & 0.72 & 0.73 & 0.74 & 0.75 & 0.77 & 0.79 & 0.81 & 0.85 & 0.86 & 0.88 \\\\ \neRBM & 0.84 & 0.86 & 0.91 & 0.93 & 0.94 & 0.89 & 0.87 & 0.83 & 0.88 & 0.94 & 0.72 & 0.73 & 0.73 & 0.74 & 0.77 & 0.86 & 0.85 & 0.79 & 0.81 & 0.88 \\\\ \niRBM & 0.84 & 0.87 & 0.91 & 0.93 & 0.94 & 0.76 & 0.76 & 0.85 & 0.90 & 0.94 & 0.71 & 0.71 & 0.73 & 0.74 & 0.77 & 0.69 & 0.70 & 0.78 & 0.83 & 0.88 \\\\ \nlav & 0.86 & 0.88 & 0.93 & 0.94 & 0.94 & 0.84 & 0.86 & 0.90 & 0.93 & 0.94 & 0.72 & 0.73 & 0.74 & 0.75 & 0.77 & 0.78 & 0.80 & 0.84 & 0.85 & 0.88 \\\\ \nOzenne et al. & 0.88 & 0.90 & 0.93 & 0.94 & 0.94 & 0.86 & 0.87 & 0.91 & 0.93 & 0.94 & 0.76 & 0.76 & 0.75 & 0.76 & 0.77 & 0.81 & 0.82 & 0.85 & 0.86 & 0.88 \\\\ \nJackknife & 0.81 & 0.84 & 0.92 & 0.93 & 0.94 & 0.64 & 0.71 & 0.89 & 0.93 & 0.94 & 0.70 & 0.73 & 0.82 & 0.86 & 0.94 & 0.58 & 0.64 & 0.84 & 0.90 & 0.94 \\\\ \nBootstrap & 0.70 & 0.73 & 0.85 & 0.89 & 0.94 & 0.63 & 0.69 & 0.85 & 0.92 & 0.94 & 0.54 & 0.58 & 0.71 & 0.79 & 0.94 & 0.56 & 0.61 & 0.80 & 0.88 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.80 & 0.83 & 0.89 & 0.93 & 0.94 & 0.79 & 0.82 & 0.88 & 0.93 & 0.94 & 0.68 & 0.69 & 0.73 & 0.75 & 0.77 & 0.73 & 0.77 & 0.83 & 0.85 & 0.87 \\\\ \neRBM & 0.84 & 0.86 & 0.90 & 0.94 & 0.94 & 0.82 & 0.86 & 0.83 & 0.91 & 0.94 & 0.73 & 0.72 & 0.74 & 0.76 & 0.77 & 0.83 & 0.84 & 0.80 & 0.82 & 0.87 \\\\ \niRBM & 0.83 & 0.85 & 0.90 & 0.94 & 0.94 & 0.73 & 0.76 & 0.84 & 0.92 & 0.94 & 0.71 & 0.71 & 0.74 & 0.75 & 0.77 & 0.68 & 0.72 & 0.79 & 0.83 & 0.87 \\\\ \nlav & 0.80 & 0.83 & 0.89 & 0.93 & 0.94 & 0.77 & 0.80 & 0.88 & 0.93 & 0.94 & 0.67 & 0.69 & 0.73 & 0.75 & 0.77 & 0.72 & 0.76 & 0.82 & 0.84 & 0.87 \\\\ \nOzenne et al. & 0.84 & 0.86 & 0.90 & 0.94 & 0.94 & 0.80 & 0.82 & 0.89 & 0.93 & 0.94 & 0.70 & 0.72 & 0.75 & 0.76 & 0.77 & 0.75 & 0.78 & 0.83 & 0.85 & 0.87 \\\\ \nJackknife & 0.82 & 0.85 & 0.90 & 0.93 & 0.94 & 0.68 & 0.73 & 0.87 & 0.94 & 0.94 & 0.72 & 0.74 & 0.82 & 0.88 & 0.93 & 0.65 & 0.69 & 0.84 & 0.89 & 0.94 \\\\ \nBootstrap & 0.72 & 0.76 & 0.85 & 0.91 & 0.94 & 0.66 & 0.70 & 0.85 & 0.93 & 0.94 & 0.59 & 0.63 & 0.74 & 0.82 & 0.93 & 0.59 & 0.64 & 0.80 & 0.88 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\beta\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.91 & 0.91 & 0.93 & 0.95 & 0.95 & 0.88 & 0.91 & 0.93 & 0.94 & 0.95 & 0.91 & 0.92 & 0.95 & 0.95 & 0.96 & 0.89 & 0.91 & 0.93 & 0.94 & 0.95 \\\\ \neRBM & 0.92 & 0.91 & 0.94 & 0.96 & 0.95 & 0.91 & 0.93 & 0.94 & 0.94 & 0.95 & 0.91 & 0.92 & 0.95 & 0.95 & 0.96 & 0.88 & 0.92 & 0.94 & 0.94 & 0.95 \\\\ \niRBM & 0.92 & 0.91 & 0.94 & 0.96 & 0.95 & 0.81 & 0.84 & 0.92 & 0.94 & 0.95 & 0.89 & 0.91 & 0.95 & 0.95 & 0.96 & 0.79 & 0.80 & 0.89 & 0.94 & 0.95 \\\\ \nlav & 0.90 & 0.89 & 0.93 & 0.95 & 0.95 & 0.88 & 0.89 & 0.92 & 0.94 & 0.95 & 0.90 & 0.91 & 0.94 & 0.95 & 0.96 & 0.89 & 0.89 & 0.92 & 0.94 & 0.95 \\\\ \nOzenne et al. & 0.91 & 0.90 & 0.93 & 0.95 & 0.95 & 0.89 & 0.90 & 0.93 & 0.94 & 0.95 & 0.91 & 0.92 & 0.95 & 0.95 & 0.96 & 0.90 & 0.90 & 0.92 & 0.94 & 0.95 \\\\ \nJackknife & 0.93 & 0.93 & 0.95 & 0.95 & 0.95 & 0.86 & 0.88 & 0.93 & 0.95 & 0.95 & 0.92 & 0.93 & 0.94 & 0.94 & 0.95 & 0.82 & 0.85 & 0.92 & 0.95 & 0.96 \\\\ \nBootstrap & 0.96 & 0.95 & 0.94 & 0.96 & 0.95 & 0.95 & 0.96 & 0.97 & 0.97 & 0.95 & 0.96 & 0.97 & 0.96 & 0.95 & 0.95 & 0.94 & 0.96 & 0.97 & 0.97 & 0.95 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\lambda_{2,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.93 & 0.92 & 0.95 & 0.95 & 0.95 & 0.90 & 0.91 & 0.94 & 0.95 & 0.95 & 0.90 & 0.93 & 0.94 & 0.94 & 0.95 & 0.86 & 0.90 & 0.93 & 0.95 & 0.95 \\\\ \neRBM & 0.93 & 0.92 & 0.95 & 0.95 & 0.95 & 0.92 & 0.89 & 0.93 & 0.94 & 0.95 & 0.91 & 0.92 & 0.94 & 0.95 & 0.95 & 0.87 & 0.91 & 0.91 & 0.93 & 0.95 \\\\ \niRBM & 0.93 & 0.92 & 0.95 & 0.95 & 0.95 & 0.86 & 0.87 & 0.93 & 0.94 & 0.95 & 0.90 & 0.91 & 0.94 & 0.95 & 0.95 & 0.83 & 0.85 & 0.91 & 0.93 & 0.95 \\\\ \nlav & 0.92 & 0.92 & 0.95 & 0.94 & 0.95 & 0.89 & 0.90 & 0.93 & 0.95 & 0.95 & 0.90 & 0.92 & 0.94 & 0.94 & 0.95 & 0.85 & 0.89 & 0.92 & 0.94 & 0.95 \\\\ \nOzenne et al. & 0.93 & 0.93 & 0.95 & 0.95 & 0.95 & 0.90 & 0.90 & 0.94 & 0.95 & 0.95 & 0.91 & 0.93 & 0.95 & 0.94 & 0.95 & 0.86 & 0.90 & 0.92 & 0.95 & 0.95 \\\\ \nJackknife & 0.91 & 0.92 & 0.94 & 0.94 & 0.95 & 0.78 & 0.82 & 0.92 & 0.95 & 0.95 & 0.88 & 0.92 & 0.94 & 0.95 & 0.95 & 0.77 & 0.81 & 0.90 & 0.95 & 0.95 \\\\ \nBootstrap & 0.94 & 0.92 & 0.95 & 0.94 & 0.95 & 0.89 & 0.89 & 0.93 & 0.95 & 0.95 & 0.93 & 0.94 & 0.94 & 0.95 & 0.95 & 0.87 & 0.89 & 0.92 & 0.95 & 0.95 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n#### Growth curve model\n\n\n::: {#tbl-bias-growth-80 .cell .column-page tbl-cap='Results (trimmed) growth curve model reliability 0.80.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_growth_80_df, \"growth\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \neRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \niRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.00 & 0.00 & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nJackknife & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nBootstrap & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & -0.01 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.05 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nOzenne et al. & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nREML & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.13 & -0.09 & -0.04 & -0.02 & 0.00 & -0.26 & -0.16 & -0.06 & -0.03 & 0.00 & -0.16 & -0.12 & -0.05 & -0.02 & 0.00 & -0.32 & -0.22 & -0.09 & -0.04 & 0.00 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.53 & 0.50 & 0.35 & 0.26 & 0.09 \\\\ \neRBM & -0.07 & -0.03 & -0.01 & -0.01 & 0.00 & -0.23 & -0.12 & -0.03 & -0.01 & 0.00 & -0.10 & -0.06 & -0.02 & -0.01 & 0.00 & -0.28 & -0.18 & -0.07 & -0.03 & 0.00 & 0.41 & 0.37 & 0.23 & 0.17 & 0.05 & 0.50 & 0.49 & 0.35 & 0.26 & 0.09 \\\\ \niRBM & -0.07 & -0.03 & -0.01 & -0.01 & 0.00 & -0.22 & -0.11 & -0.03 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.26 & -0.16 & -0.07 & -0.03 & 0.00 & 0.41 & 0.37 & 0.23 & 0.17 & 0.05 & 0.50 & 0.49 & 0.35 & 0.26 & 0.09 \\\\ \nJackknife & -0.06 & -0.02 & -0.01 & -0.01 & 0.00 & -0.27 & -0.18 & -0.06 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.31 & -0.23 & -0.08 & -0.03 & 0.00 & 0.42 & 0.37 & 0.23 & 0.17 & 0.05 & 0.50 & 0.46 & 0.33 & 0.26 & 0.09 \\\\ \nBootstrap & -0.06 & -0.02 & -0.01 & -0.01 & 0.00 & -0.23 & -0.17 & -0.05 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.29 & -0.21 & -0.08 & -0.03 & 0.00 & 0.43 & 0.37 & 0.23 & 0.17 & 0.05 & 0.52 & 0.47 & 0.34 & 0.26 & 0.09 \\\\ \nOzenne et al. & -0.06 & -0.02 & -0.01 & -0.01 & 0.00 & -0.21 & -0.11 & -0.03 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.27 & -0.18 & -0.07 & -0.03 & 0.00 & 0.41 & 0.37 & 0.23 & 0.17 & 0.05 & 0.51 & 0.49 & 0.35 & 0.26 & 0.09 \\\\ \nREML & -0.03 & -0.02 & -0.01 & -0.01 & 0.00 & -0.08 & -0.06 & -0.03 & -0.01 & 0.00 & -0.07 & -0.06 & -0.02 & -0.01 & 0.00 & -0.19 & -0.13 & -0.07 & -0.03 & 0.00 & 0.44 & 0.38 & 0.23 & 0.17 & 0.05 & 0.61 & 0.54 & 0.35 & 0.26 & 0.09 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.09 & -0.07 & -0.03 & -0.01 & 0.00 & -0.16 & -0.13 & -0.05 & -0.04 & 0.00 & -0.11 & -0.09 & -0.04 & -0.01 & 0.00 & -0.26 & -0.21 & -0.09 & -0.07 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.52 & 0.47 & 0.33 & 0.24 & 0.08 \\\\ \neRBM & -0.02 & -0.02 & -0.01 & 0.00 & 0.00 & -0.10 & -0.09 & -0.03 & -0.03 & 0.00 & -0.05 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \niRBM & -0.04 & -0.03 & -0.01 & 0.00 & 0.00 & -0.12 & -0.10 & -0.03 & -0.03 & 0.00 & -0.06 & -0.05 & -0.02 & 0.00 & 0.00 & -0.21 & -0.17 & -0.07 & -0.06 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.52 & 0.47 & 0.33 & 0.24 & 0.08 \\\\ \nJackknife & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.10 & -0.08 & -0.03 & -0.03 & 0.00 & -0.04 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.35 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \nBootstrap & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.11 & -0.08 & -0.03 & -0.03 & 0.00 & -0.06 & -0.04 & -0.02 & 0.00 & 0.00 & -0.22 & -0.17 & -0.08 & -0.05 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \nOzenne et al. & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.09 & -0.08 & -0.03 & -0.03 & 0.00 & -0.04 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.35 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \nREML & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.09 & -0.08 & -0.03 & -0.03 & 0.00 & -0.04 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.35 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.01 & 0.00 & -0.03 & 0.00 & 0.00 & -0.07 & -0.06 & -0.04 & -0.02 & 0.00 & -0.03 & 0.01 & -0.03 & 0.00 & 0.00 & -0.15 & -0.13 & -0.06 & -0.05 & 0.00 & 1.50 & 1.35 & 0.87 & 0.62 & 0.20 & 1.41 & 1.27 & 0.87 & 0.63 & 0.20 \\\\ \neRBM & 0.01 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.01 & 0.03 & -0.02 & 0.00 & 0.00 & -0.14 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \niRBM & 0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.04 & -0.02 & -0.03 & -0.02 & 0.00 & 0.00 & 0.02 & -0.02 & 0.00 & 0.00 & -0.09 & -0.07 & -0.05 & -0.04 & 0.00 & 1.57 & 1.39 & 0.88 & 0.63 & 0.20 & 1.47 & 1.31 & 0.88 & 0.64 & 0.20 \\\\ \nJackknife & 0.01 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.14 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \nBootstrap & 0.00 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.03 & 0.00 & 0.00 & -0.14 & -0.12 & -0.06 & -0.04 & 0.00 & 1.58 & 1.42 & 0.89 & 0.63 & 0.20 & 1.47 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \nOzenne et al. & 0.01 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.14 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \nREML & 0.00 & 0.01 & -0.02 & 0.00 & 0.00 & -0.07 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.15 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n::: {#tbl-bias-growth-50 .cell .column-page tbl-cap='Results (trimmed) growth curve model reliability 0.50.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_growth_50_df, \"growth\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \neRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \niRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.00 & 0.00 & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nJackknife & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.03 & -0.02 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.04 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.19 & 0.17 & 0.12 & 0.08 & 0.03 \\\\ \nBootstrap & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.02 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & -0.01 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.05 & 0.01 & 0.19 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nOzenne et al. & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nREML & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.02 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.06 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.27 & -0.17 & -0.07 & -0.03 & -0.01 & -0.35 & -0.22 & -0.08 & -0.04 & -0.01 & -0.32 & -0.23 & -0.10 & -0.03 & -0.01 & -0.45 & -0.30 & -0.12 & -0.06 & -0.01 & 0.85 & 0.74 & 0.48 & 0.34 & 0.11 & 0.93 & 0.87 & 0.57 & 0.41 & 0.13 \\\\ \neRBM & -0.16 & -0.05 & -0.02 & 0.00 & 0.00 & -0.27 & -0.13 & -0.03 & -0.01 & 0.00 & -0.19 & -0.11 & -0.05 & -0.01 & 0.00 & -0.34 & -0.21 & -0.07 & -0.03 & -0.01 & 0.83 & 0.74 & 0.49 & 0.34 & 0.11 & 0.90 & 0.85 & 0.58 & 0.41 & 0.13 \\\\ \niRBM & -0.17 & -0.07 & -0.03 & 0.00 & 0.00 & -0.24 & -0.12 & -0.03 & -0.02 & 0.00 & -0.20 & -0.13 & -0.05 & -0.01 & 0.00 & -0.32 & -0.19 & -0.07 & -0.03 & -0.01 & 0.82 & 0.74 & 0.49 & 0.34 & 0.11 & 0.89 & 0.84 & 0.57 & 0.41 & 0.13 \\\\ \nJackknife & -0.12 & -0.05 & -0.02 & 0.00 & 0.00 & -0.28 & -0.17 & -0.03 & -0.01 & 0.00 & -0.16 & -0.11 & -0.04 & -0.01 & 0.00 & -0.36 & -0.24 & -0.07 & -0.03 & -0.01 & 0.86 & 0.74 & 0.49 & 0.34 & 0.11 & 0.90 & 0.82 & 0.57 & 0.41 & 0.13 \\\\ \nBootstrap & -0.13 & -0.05 & -0.02 & 0.00 & 0.00 & -0.24 & -0.15 & -0.03 & -0.01 & 0.00 & -0.20 & -0.10 & -0.05 & 0.00 & 0.00 & -0.35 & -0.22 & -0.08 & -0.03 & -0.01 & 0.88 & 0.75 & 0.49 & 0.34 & 0.11 & 0.93 & 0.84 & 0.57 & 0.41 & 0.13 \\\\ \nOzenne et al. & -0.14 & -0.05 & -0.02 & 0.00 & 0.00 & -0.24 & -0.12 & -0.02 & -0.01 & 0.00 & -0.18 & -0.10 & -0.04 & -0.01 & 0.00 & -0.32 & -0.20 & -0.07 & -0.03 & -0.01 & 0.83 & 0.75 & 0.49 & 0.34 & 0.11 & 0.91 & 0.86 & 0.58 & 0.41 & 0.13 \\\\ \nREML & 0.02 & 0.01 & -0.02 & 0.00 & 0.00 & 0.00 & 0.01 & -0.02 & -0.01 & 0.00 & -0.11 & -0.07 & -0.04 & -0.01 & 0.00 & -0.16 & -0.13 & -0.07 & -0.03 & -0.01 & 0.75 & 0.67 & 0.47 & 0.34 & 0.11 & 0.84 & 0.78 & 0.55 & 0.41 & 0.13 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.11 & -0.08 & -0.04 & -0.01 & 0.00 & -0.17 & -0.14 & -0.05 & -0.04 & 0.00 & -0.14 & -0.11 & -0.05 & -0.02 & 0.00 & -0.27 & -0.22 & -0.09 & -0.07 & -0.01 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.58 & 0.52 & 0.36 & 0.26 & 0.08 \\\\ \neRBM & -0.03 & -0.02 & -0.02 & 0.00 & 0.00 & -0.10 & -0.09 & -0.02 & -0.03 & 0.00 & -0.06 & -0.05 & -0.03 & 0.00 & 0.00 & -0.21 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \niRBM & -0.05 & -0.03 & -0.02 & 0.00 & 0.00 & -0.11 & -0.09 & -0.03 & -0.03 & 0.00 & -0.07 & -0.06 & -0.03 & 0.00 & 0.00 & -0.21 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.52 & 0.37 & 0.26 & 0.08 \\\\ \nJackknife & -0.03 & -0.02 & -0.02 & 0.00 & 0.00 & -0.09 & -0.08 & -0.02 & -0.03 & 0.00 & -0.05 & -0.05 & -0.03 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \nBootstrap & -0.05 & -0.02 & -0.02 & 0.00 & 0.00 & -0.11 & -0.08 & -0.02 & -0.03 & 0.00 & -0.07 & -0.05 & -0.03 & 0.00 & 0.00 & -0.22 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \nOzenne et al. & -0.03 & -0.02 & -0.02 & 0.00 & 0.00 & -0.09 & -0.08 & -0.02 & -0.03 & 0.00 & -0.05 & -0.05 & -0.03 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \nREML & -0.01 & -0.01 & -0.01 & 0.00 & 0.00 & -0.07 & -0.06 & -0.02 & -0.03 & 0.00 & -0.04 & -0.04 & -0.03 & 0.00 & 0.00 & -0.18 & -0.15 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.57 & 0.52 & 0.36 & 0.26 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.30 & 0.19 & 0.05 & 0.02 & 0.01 & 0.24 & 0.17 & 0.03 & 0.04 & 0.01 & 0.37 & 0.30 & 0.07 & 0.03 & 0.02 & 0.26 & 0.18 & 0.07 & 0.06 & 0.00 & 2.49 & 2.20 & 1.44 & 1.03 & 0.32 & 2.47 & 2.25 & 1.49 & 1.05 & 0.34 \\\\ \neRBM & 0.15 & 0.07 & 0.00 & 0.00 & 0.00 & 0.08 & 0.05 & -0.02 & 0.02 & 0.01 & 0.23 & 0.19 & 0.02 & 0.01 & 0.02 & 0.12 & 0.07 & 0.02 & 0.04 & 0.00 & 2.63 & 2.30 & 1.47 & 1.04 & 0.32 & 2.61 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \niRBM & 0.17 & 0.09 & 0.01 & 0.00 & 0.00 & 0.09 & 0.05 & -0.02 & 0.02 & 0.01 & 0.25 & 0.21 & 0.02 & 0.01 & 0.02 & 0.10 & 0.06 & 0.01 & 0.04 & 0.00 & 2.61 & 2.28 & 1.47 & 1.04 & 0.32 & 2.59 & 2.33 & 1.52 & 1.06 & 0.34 \\\\ \nJackknife & 0.14 & 0.07 & 0.00 & -0.01 & 0.00 & 0.07 & 0.05 & -0.02 & 0.02 & 0.01 & 0.22 & 0.18 & 0.02 & 0.01 & 0.02 & 0.12 & 0.07 & 0.01 & 0.04 & 0.00 & 2.64 & 2.30 & 1.47 & 1.04 & 0.32 & 2.63 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \nBootstrap & 0.19 & 0.07 & 0.00 & 0.00 & 0.00 & 0.12 & 0.06 & -0.02 & 0.02 & 0.01 & 0.29 & 0.21 & 0.02 & 0.01 & 0.01 & 0.15 & 0.09 & 0.02 & 0.03 & 0.00 & 2.60 & 2.30 & 1.47 & 1.04 & 0.32 & 2.58 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \nOzenne et al. & 0.14 & 0.07 & 0.00 & -0.01 & 0.00 & 0.07 & 0.05 & -0.02 & 0.02 & 0.01 & 0.22 & 0.18 & 0.02 & 0.01 & 0.02 & 0.12 & 0.07 & 0.01 & 0.04 & 0.00 & 2.64 & 2.30 & 1.47 & 1.04 & 0.32 & 2.63 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \nREML & -0.13 & -0.08 & -0.03 & -0.01 & 0.00 & -0.29 & -0.21 & -0.08 & 0.01 & 0.01 & 0.07 & 0.06 & 0.01 & 0.00 & 0.02 & -0.23 & -0.13 & -0.02 & 0.04 & 0.00 & 2.42 & 2.17 & 1.44 & 1.04 & 0.32 & 2.38 & 2.18 & 1.48 & 1.06 & 0.34 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n::: {#tbl-covr-growth .cell .column-page tbl-cap='Coverage rates 95% confidence interval for the growth curve model.'}\n\n```{.r .cell-code .hidden}\ntab_covr(covr_growth_df, \"growth\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Normal data} & \\multicolumn{10}{c}{Non-Normal data} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21}\n & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\theta_{1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 \\\\ \neRBM & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 \\\\ \niRBM & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.71 & 0.72 & 0.71 & 0.73 & 0.72 & 0.70 & 0.72 & 0.71 \\\\ \nlav & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 \\\\ \nOzenne et al. & 0.94 & 0.95 & 0.95 & 0.94 & 0.96 & 0.94 & 0.95 & 0.95 & 0.94 & 0.96 & 0.73 & 0.72 & 0.71 & 0.72 & 0.71 & 0.73 & 0.72 & 0.71 & 0.72 & 0.71 \\\\ \nJackknife & 0.92 & 0.92 & 0.94 & 0.94 & 0.96 & 0.92 & 0.92 & 0.94 & 0.94 & 0.96 & 0.88 & 0.90 & 0.92 & 0.94 & 0.95 & 0.88 & 0.90 & 0.92 & 0.94 & 0.95 \\\\ \nBootstrap & 0.88 & 0.91 & 0.94 & 0.94 & 0.96 & 0.88 & 0.91 & 0.94 & 0.94 & 0.96 & 0.84 & 0.89 & 0.91 & 0.94 & 0.95 & 0.83 & 0.89 & 0.91 & 0.94 & 0.95 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.84 & 0.87 & 0.92 & 0.93 & 0.95 & 0.84 & 0.88 & 0.91 & 0.93 & 0.95 & 0.68 & 0.73 & 0.76 & 0.76 & 0.78 & 0.80 & 0.82 & 0.87 & 0.89 & 0.87 \\\\ \neRBM & 0.88 & 0.90 & 0.93 & 0.94 & 0.95 & 0.88 & 0.92 & 0.93 & 0.94 & 0.95 & 0.73 & 0.77 & 0.77 & 0.77 & 0.78 & 0.84 & 0.86 & 0.88 & 0.90 & 0.87 \\\\ \niRBM & 0.88 & 0.90 & 0.93 & 0.94 & 0.95 & 0.88 & 0.92 & 0.93 & 0.94 & 0.95 & 0.74 & 0.78 & 0.78 & 0.77 & 0.78 & 0.85 & 0.87 & 0.88 & 0.90 & 0.87 \\\\ \nlav & 0.84 & 0.87 & 0.92 & 0.93 & 0.95 & 0.84 & 0.88 & 0.91 & 0.93 & 0.95 & 0.68 & 0.73 & 0.76 & 0.76 & 0.78 & 0.80 & 0.82 & 0.87 & 0.89 & 0.87 \\\\ \nOzenne et al. & 0.88 & 0.90 & 0.93 & 0.94 & 0.95 & 0.88 & 0.92 & 0.93 & 0.94 & 0.95 & 0.73 & 0.77 & 0.77 & 0.77 & 0.78 & 0.83 & 0.86 & 0.88 & 0.89 & 0.87 \\\\ \nJackknife & 0.83 & 0.88 & 0.92 & 0.93 & 0.95 & 0.84 & 0.89 & 0.91 & 0.93 & 0.95 & 0.69 & 0.73 & 0.83 & 0.88 & 0.94 & 0.80 & 0.83 & 0.90 & 0.92 & 0.94 \\\\ \nBootstrap & 0.76 & 0.85 & 0.91 & 0.92 & 0.94 & 0.77 & 0.86 & 0.90 & 0.93 & 0.94 & 0.63 & 0.71 & 0.82 & 0.87 & 0.94 & 0.75 & 0.81 & 0.89 & 0.92 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.85 & 0.88 & 0.91 & 0.94 & 0.95 & 0.84 & 0.87 & 0.91 & 0.94 & 0.95 & 0.66 & 0.66 & 0.69 & 0.68 & 0.70 & 0.71 & 0.73 & 0.76 & 0.76 & 0.77 \\\\ \neRBM & 0.90 & 0.91 & 0.92 & 0.94 & 0.95 & 0.89 & 0.90 & 0.92 & 0.95 & 0.95 & 0.71 & 0.71 & 0.71 & 0.69 & 0.70 & 0.76 & 0.78 & 0.78 & 0.77 & 0.77 \\\\ \niRBM & 0.89 & 0.91 & 0.92 & 0.94 & 0.95 & 0.88 & 0.90 & 0.92 & 0.95 & 0.95 & 0.70 & 0.70 & 0.71 & 0.69 & 0.70 & 0.76 & 0.78 & 0.78 & 0.77 & 0.77 \\\\ \nlav & 0.85 & 0.88 & 0.91 & 0.94 & 0.95 & 0.84 & 0.87 & 0.91 & 0.94 & 0.95 & 0.66 & 0.66 & 0.69 & 0.68 & 0.70 & 0.71 & 0.73 & 0.76 & 0.76 & 0.77 \\\\ \nOzenne et al. & 0.89 & 0.91 & 0.92 & 0.94 & 0.95 & 0.89 & 0.90 & 0.92 & 0.95 & 0.95 & 0.70 & 0.70 & 0.70 & 0.69 & 0.70 & 0.76 & 0.77 & 0.77 & 0.77 & 0.77 \\\\ \nJackknife & 0.86 & 0.88 & 0.91 & 0.93 & 0.95 & 0.85 & 0.87 & 0.91 & 0.93 & 0.95 & 0.70 & 0.73 & 0.82 & 0.85 & 0.93 & 0.74 & 0.77 & 0.84 & 0.87 & 0.94 \\\\ \nBootstrap & 0.79 & 0.85 & 0.90 & 0.92 & 0.95 & 0.78 & 0.85 & 0.90 & 0.92 & 0.95 & 0.63 & 0.70 & 0.81 & 0.84 & 0.93 & 0.67 & 0.74 & 0.83 & 0.86 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{1,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.98 & 0.97 & 0.95 & 0.95 & 0.96 & 0.94 & 0.95 & 0.94 & 0.94 & 0.95 & 0.97 & 0.96 & 0.94 & 0.94 & 0.94 & 0.93 & 0.93 & 0.94 & 0.94 & 0.94 \\\\ \neRBM & 0.98 & 0.98 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.95 & 0.94 & 0.95 & 0.98 & 0.98 & 0.95 & 0.94 & 0.94 & 0.95 & 0.95 & 0.94 & 0.94 & 0.94 \\\\ \niRBM & 0.98 & 0.98 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.95 & 0.94 & 0.95 & 0.98 & 0.98 & 0.95 & 0.94 & 0.94 & 0.95 & 0.95 & 0.94 & 0.94 & 0.94 \\\\ \nlav & 0.98 & 0.97 & 0.95 & 0.95 & 0.96 & 0.94 & 0.95 & 0.94 & 0.94 & 0.95 & 0.97 & 0.96 & 0.94 & 0.94 & 0.94 & 0.93 & 0.93 & 0.94 & 0.94 & 0.94 \\\\ \nOzenne et al. & 0.98 & 0.97 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.95 & 0.94 & 0.95 & 0.98 & 0.97 & 0.95 & 0.94 & 0.94 & 0.94 & 0.94 & 0.94 & 0.94 & 0.94 \\\\ \nJackknife & 0.93 & 0.93 & 0.94 & 0.94 & 0.95 & 0.91 & 0.92 & 0.94 & 0.93 & 0.95 & 0.93 & 0.93 & 0.94 & 0.94 & 0.95 & 0.92 & 0.93 & 0.95 & 0.94 & 0.95 \\\\ \nBootstrap & 0.86 & 0.91 & 0.93 & 0.94 & 0.95 & 0.83 & 0.90 & 0.92 & 0.93 & 0.95 & 0.85 & 0.90 & 0.93 & 0.93 & 0.95 & 0.84 & 0.90 & 0.93 & 0.94 & 0.95 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}