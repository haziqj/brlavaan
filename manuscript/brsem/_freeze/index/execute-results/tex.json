{
  "hash": "8de5870832e1fe5934ef4141ac66c3ee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Bias-Reduced Estimation of Structural Equation Models\nauthors:\n  - name: Haziq Jamil\n    orcid: 0000-0003-3298-1010\n    email: haziq.jamil@ubd.edu.bn\n    # corresponding: true\n    url: \"https://haziqj.ml\"\n    affiliations:\n      - name: Universiti Brunei Darussalam\n        id: ubd\n        department: Mathematical Sciences, Faculty of Science\n        address: Universiti Brunei Darussalam, Jalan Tungku Link\n        city: Bandar Seri Begawan\n        country: Brunei\n        postal-code: BE 1410\n      - name: London School of Economics and Political Science\n        id: lse\n        department: Department of Statistics\n        address: Columbia House, Houghton Street\n        city: London\n        country: United Kingdom\n        postal-code: WC2A 2AE\n  - name: Yves Rosseel\n    orcid: 0000-0002-4129-4477\n    email: Yves.Rosseel@UGent.be\n    affiliations:\n      - name: Ghent University\n        id: ugent\n        department: Center for Data Analysis and Statistical Science\n        address: Krijgslaan 281 - S9\n        city: Gent\n        country: Belgium\n        postal-code: 9000\n  - name: Oliver Kemp\n    # orcid: \n    email: Oliver.Kemp@warwick.ac.uk\n    affiliations:\n      - name: University of Warwick\n        id: warwick\n        department: Department of Statistics\n        address: Zeeman Building, University of Warwick\n        city: Coventry\n        country: United Kingdom\n        postal-code: CV4 7AL\n  - name: Ioannis Kosmidis\n    email: ioannis.kosmidis@warwick.ac.uk\n    orcid: 0000-0003-1556-0302\n    affiliations:\n      - ref: warwick\ndate-modified: last-modified\nabstract: |\n  An exploration of explicit and implicit bias-reduced maximum likelihood estimation for structural equation models (SEMs) and latent growth curve models (LGCMs) is presented. Comparisons to prior work on resampling-based methods like Bootstrap and Jackknife are made. Performance of our methods are favourable.\nkeywords:\n  - Structural Equation Models\n  - Growth Curve Models\n  - Bias Reduction\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.2     v tibble    3.2.1\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.0.4     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(brlavaan)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: lavaan\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(gt)\nlibrary(glue)\nlibrary(semPlot)\nlibrary(semptools)\nload(\"results.RData\")\n```\n:::\n\n## Introduction\n\n- Same old song: Bias is a problem in small sample settings of SEM.\n- Answer sounds obvious: get more samples. But in psychometric applications, this is not feasible or just downright impossible. A practical example??\n- Previous studies: Resampling methods [@dhaene2022resampling], REML, Ozenne method.\n- Limitations of previous studies.\n- What we want to do: Empirical bias reducing adjustments to the ML estimation procedure. Briefly, what are the advantages?\n\n<!-- [ADD TIMING FOR BOOTSTRAP AND JACKKNIFE, AND COMPARE TIMING FOR OURS]{fg=\"primary\" bg=\"yellow\"} -->\n\n\n\n::: {#tbl-timing .cell tbl-cap='Comparison of mean computation times (in seconds, with standard deviations in parentheses) for estimating the two-factor SEM and latent growth curve models for $n=1000$.'}\n\n```{.r .cell-code .hidden}\nres_timing_n1000 |>\n  rowwise() |>\n  mutate(\n    thing = glue::glue(\"{gt::vec_fmt_number(mean)} ({gt::vec_fmt_number(sd)})\"),\n    method = factor(\n      method,\n      levels = c(\"ML\", \"lav\", \"eRBM\", \"iRBM\", \"JB\", \"BB\", \"Ozenne\", \"REML\"),\n      labels = names(mycols)\n    )\n  ) |>\n  select(-mean, -sd) |>\n  pivot_wider(names_from = model, values_from = thing) |>\n  arrange(method) |>\n  gt() |>\n  sub_missing(missing_text = \"\") |>\n  cols_label(\n    method = \"Method\",\n    twofac = \"Two-factor SEM\",\n    growth = \"Latent growth curve model\"\n  ) |>\n  cols_width(\n    twofac ~ px(180),\n    growth ~ px(180)\n  )\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}c>{\\centering\\arraybackslash}p{\\dimexpr 135.00pt -2\\tabcolsep-1.5\\arrayrulewidth}>{\\centering\\arraybackslash}p{\\dimexpr 135.00pt -2\\tabcolsep-1.5\\arrayrulewidth}}\n\\toprule\nMethod & Two-factor SEM & Latent growth curve model \\\\ \n\\midrule\\addlinespace[2.5pt]\nML & 0.07 (0.02) & 0.03 (0.01) \\\\ \neRBM & 2.45 (0.15) & 1.36 (0.07) \\\\ \niRBM & 43.03 (11.97) & 8.70 (1.10) \\\\ \nJackknife & 105.85 (7.54) & 94.79 (3.37) \\\\ \nBootstrap & 52.34 (4.13) & 45.24 (1.97) \\\\ \nOzenne et al. & 0.49 (0.05) & 0.11 (0.02) \\\\ \nREML &  & 0.61 (0.10) \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n## Methodology\n\n## Structural Equation Models\n\nA Structural equation model has two parts, a measurement part and a structural part. For observation $i$, the measurement part of the model is defined as \n\n$$\ny_i = \\nu + \\Lambda \\eta_i + \\epsilon_i,\n$${#eq-measurement}\n\nwhere $y_i$ is a $m \\times 1$ vector of observed variables, $\\nu$ is a $m \\times 1$ vector of measurement intercepts, $\\gamma$ is a $m \\times k$ matrix of factor loadings, $\\eta_i$ is a $k \\times 1$ vector of latent variables, and $\\epsilon_i$ is a $m \\times 1$ vector of measurement errors, assumed to be normally distributed with mean zero and covariance matrix $\\Phi$.\n\nThe structural part of the model is defined as\n\n$$\n\\eta_i =\\alpha + B \\eta_i + \\zeta_i,\n$$ {#eq-structural}\n\nwhere $\\alpha$ is a $k \\times 1$ vector of factor means, $B$ is a $k \\times k$ matrix of regression coefficients for which $\\text{diag}(B) = 0$ and $I-B$ is invertible, and $\\zeta_i$ is a $k \\times 1$ vector of structural disturbances, assumed to be normally distributed with mean zero and covariance matrix $\\Psi$. We also assume that $\\text{cov}(\\eta_i, \\epsilon_i) = \\text{cov}(\\epsilon_i, \\zeta_i) = 0$.\n\n<!-- $$ -->\n<!-- \\epsilon \\sim N(0, \\Theta_\\epsilon)\\\\ -->\n<!-- \\zeta \\sim N(0, \\Psi)\\\\ -->\n<!-- $$ {#eq-sem} -->\n\n## Estimation methods\n\n### Maximum likelihood estimation\n\nParameter estimation in a SEM is typically performed using maximum likelihood estimation, where given some observed data, we estimate parameters as those which make the observations most likely under the assumed model. The specific parameters to estimate depend on the model, since which parameters from @eq-measurement and @eq-structural are present in the model and free depend on the specific SEM. In any case, as described in @dhaene2022resampling, we can combine all free parameters to be estimated in to a single $p \\times 1$ vector $\\theta$, and define the model mean vector and covariance matrix as\n\n$$\n\\mu(\\theta) = \\nu + \\Lambda (I - B)^{-1} \\alpha \\\\\n\\Sigma(\\theta) = \\Lambda (I - B)^{-1} \\Psi (I-B)^{-T} \\Lambda^T + \\Phi,\n$$\nwhere for an invertible matrix $A$, $A^{-T} = (A^{-1})^T$.\n\n\nThen given independent and identically distributed data $Y = \\{y_1, \\dots, y_n \\}$ sampled from $N(\\mu(\\theta), \\Sigma(\\theta))$, parameter estimation is achieved by maximising the multivariate normal log likelihood defined as\n\n$$\nl(\\theta|Y) = \\frac{-nm}{2}\\log(2\\pi) -\\frac{n}{2}\\log|\\Sigma(\\theta)| - \\frac{1}{2}\\sum_{i=1}^n(y_i - \\mu(\\theta))^T \\Sigma(\\theta)^{-1}(y_i - \\mu(\\theta)).\n$${#eq-sem_lik}\n\n\nMaximising @eq-sem_lik with respect to $\\theta$ gives the maximum likelihood estimator of $\\theta$, which we denote by $\\hat{\\theta}$. Equivalently, we can solve the equations $\\nabla l(\\theta) = 0_p$, where here $\\nabla$ denotes gradient with respect to $\\theta$, $0_p$ is a $p-$vector of zeroes.\n\n### Reduced-bias estimation\n\nWe apply the method of Reduced-bias M-estimation of @kosmidis2024empirical. The method works by adding an appropriate penalty to the log-likelihood, so that rather than maximising $l(\\theta)$, we instead maximise\n\n$$\nl(\\theta) + P(\\theta)\n$$\n\nfor an appropriate penalty term $P(\\theta)$. The penalty term is chosen such that the resulting estimator, denoted $\\tilde{\\theta}$ has asymptotically reduced bias compared to $\\hat{\\theta}$, while still retaining the same asymptotic properties. This concept started in @firth1993bias for regular maximum likelihood estimators, by adding a bias-reducing adjustment term to the score equations, the derivative of the log likelihood. When estimating the canonical parameter of an exponential family model, @jeffreys1946prior invariant prior can be added as a penalty term to the log-likelihood to achieve the same result.\n\n@kosmidis2024empirical generalised this adjustment to generic M-estimators, which also has the significant advantage of giving rise to a penalised likelihood for any M-estimator estimated by maximising an objective function, rather than only when estimating canonical parameters in an exponential family model.\n\nThe penalty term $P(\\theta)$ is defined as\n\n$$\nP(\\theta) = -\\frac{1}{2}\\text{trace}\\{j(\\theta)^{-1} e(\\theta)\\},\n$$\nwhere $j(\\theta)$ is the negative Hessian matrix $-\\nabla \\nabla^T l(\\theta)$ and $e(\\theta)$ is the first order information, which we can write as $e(\\theta) = E(\\theta)^T E(\\theta)$ where $E(\\theta)$ is an $n \\times p$ matrix with $t-$th column $\\frac{\\partial}{\\partial \\theta_t} \\nabla l(\\theta)$.\n\n\nSolving this penalised SEM likelihood function results in implicit RBM-estimation. However, we can also define an explicit form of RBM-estimation by obtaining an explicit expression for the bias and subtracting this from the original maximum likelihood estimator. The explicit estimator is\n\n$$\n\\theta^{ex} = \\hat{\\theta} + j(\\hat{\\theta})^{-1} A(\\hat{\\theta}),\n$$\nwhere $A(\\theta) = \\nabla P(\\theta)$. Both implicit and explicit RBM-estimators are to be investigated throughout this paper.\n\n### Jackknife, Bootstrap, Ozenne...?\n\naaa\n\n\n\n\n## Simulation Study\n\n---\noutput: html_document\neditor_options: \n  chunk_output_type: console\n# execute:\n#   freeze: auto\n#   cache: true\n#   echo: false\n# \n# bibliography: \n#   - ../refs-HJ.bib\n#   - ../refs-OK.bib\n---\n\n\n<!-- ```{r} -->\n<!-- #| label: setup -->\n<!-- #| include: false -->\n<!-- library(tidyverse) -->\n<!-- library(brlavaan) -->\n<!-- library(gt) -->\n<!-- library(glue) -->\n<!-- library(semPlot) -->\n<!-- library(semptools) -->\n<!-- load(\"results.RData\") -->\n<!-- ``` -->\n\nTo evaluate the effectiveness of our proposed explicit and implicit bias reduction methods, we conducted an extensive simulation study mirroring the work done by @dhaene2022resampling.\nWe replicated the models and experimental settings exactly, allowing for a direct comparison with existing resampling-based and other methods for bias correction.\nThis section describes the models, simulation scenarios, and the outcome of interest of our simulations.\n\n### Models\n\nTwo distinct structural equation models (SEM) were considered: a) A two-factor SEM with a latent regression; and b) a latent growth curve model (GCM), \nThese models represent commonly used models used by practitioners and researchers alike in the social sciences.\n\nThe two-factor SEM contains two latent variables, $\\eta_{1}$ and $\\eta_{2}$, each indicated by three observed variables: \n$y_{1}$, $y_{2}$, $y_{3}$ for $\\eta_{1}$ and $y_{4}$, $y_{5}$, $y_{6}$ for $\\eta_{2}$. \nEach observed measure has an associated error term $\\epsilon_{j}\\sim\\operatorname{N}(0,\\Theta_{jj})$. \nThe latent regression path from $\\eta_{1}$ to $\\eta_{2}$ is labelled $\\beta$. \nFactor loadings are represented by $\\Lambda_{ij}$, and the latent variances by $\\Psi_{11}$ and $\\Psi_{22}$.\nAltogether, the model has 13 estimable parameters, as specified in @tbl-truth-twofac.\nThe corresponding path diagram is illustrated in @fig-path-twofac.\n\n::: {#fig-path-twofac}\n::: {.cell}\n\n```{.tex .cell-code .hidden}\n\\begin{tikzpicture}\n\\node[inner sep=3pt] {%\n\\begin{tikzpicture}[%\n  >=stealth,\n  auto,\n  node distance=1.6cm,\n  every node/.style={font=\\small},\n  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},\n  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},\n  error/.style={circle, draw, minimum size=6mm, inner sep=0mm}\n]\n\n% --------------------------------------------------------\n% 1) LATENT VARIABLES\n% --------------------------------------------------------\n\\node[latent] (eta1) at (-1.5, -2) {$\\eta_{1}$};\n\\node[latent] (eta2) at ( 3, -2) {$\\eta_{2}$};\n\n% --------------------------------------------------------\n% 2) OBSERVED VARIABLES + ERROR TERMS\n%    Place y1..y3 (for eta1) on the left, y4..y6 (for eta2) on the right.\n% --------------------------------------------------------\n\\node[obs] (y1) at (-3, 0) {$y_{1}$};\n\\node[obs] (y2) at (-1.5, 0) {$y_{2}$};\n\\node[obs] (y3) at ( 0, 0) {$y_{3}$};\n\n\\node[obs] (y4) at ( 1.5, 0) {$y_{4}$};\n\\node[obs] (y5) at ( 3, 0) {$y_{5}$};\n\\node[obs] (y6) at ( 4.5, 0) {$y_{6}$};\n\n% Error terms above each observed variable\n\\node[error] (e1) at (-3, 1.5) {$\\epsilon_{1}$};\n\\node[error] (e2) at (-1.5, 1.5) {$\\epsilon_{2}$};\n\\node[error] (e3) at ( 0, 1.5) {$\\epsilon_{3}$};\n\\node[error] (e4) at ( 1.5, 1.5) {$\\epsilon_{4}$};\n\\node[error] (e5) at ( 3, 1.5) {$\\epsilon_{5}$};\n\\node[error] (e6) at ( 4.5, 1.5) {$\\epsilon_{6}$};\n\n% --------------------------------------------------------\n% 3) PATHS FROM ERROR TERMS TO OBSERVED VARIABLES\n% --------------------------------------------------------\n\\draw[->] (e1) -- (y1);\n\\draw[->] (e2) -- (y2);\n\\draw[->] (e3) -- (y3);\n\\draw[->] (e4) -- (y4);\n\\draw[->] (e5) -- (y5);\n\\draw[->] (e6) -- (y6);\n\n% --------------------------------------------------------\n% 4) FACTOR LOADINGS\n%    \\eta_1 -> y1, y2, y3\n%    \\eta_2 -> y4, y5, y6\n% --------------------------------------------------------\n\\draw[->] (eta1) -- node[pos=0.5, right] {$\\Lambda_{11}$} (y1);\n\\draw[->] (eta1) -- node[pos=0.5, right] {$\\Lambda_{21}$} (y2);\n\\draw[->] (eta1) -- node[pos=0.5, right] {$\\Lambda_{31}$} (y3);\n\n\\draw[->] (eta2) -- node[pos=0.5, right] {$\\Lambda_{42}$} (y4);\n\\draw[->] (eta2) -- node[pos=0.5, right] {$\\Lambda_{52}$} (y5);\n\\draw[->] (eta2) -- node[pos=0.5, right] {$\\Lambda_{62}$} (y6);\n\n% --------------------------------------------------------\n% 5) REGRESSION BETWEEN LATENT VARIABLES\n%    \\eta_1 -> \\eta_2, labeled beta\n% --------------------------------------------------------\n\\draw[->] (eta1) -- node[midway, above] {$\\beta$} (eta2);\n\n% --------------------------------------------------------\n% 6) VARIANCES OF LATENT VARIABLES\n%    Double-headed arrows for psi_{11} and psi_{22}\n% --------------------------------------------------------\n\\draw[<->] (eta1) to[out=200, in=230, looseness=4] \n  node[left] {$\\Psi_{11}$} (eta1);\n\n\\draw[<->] (eta2) to[out=-50, in=-20, looseness=4] \n  node[right] {$\\Psi_{22}$} (eta2);\n\n\\end{tikzpicture}\n};\n\\end{tikzpicture}\n```\n\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/pathtwofac-1.pdf){width=90%}\n:::\n:::\nThe path diagram for the two-factor structural equation model. \n:::\n\nThe latent GCM on the other hand characterises longitudinally observed continuous variables measured across ten time points $y_{1},\\dots,y_{10}$ using two latent factors labelled $i$ for *intercept*, and $s$ for *slope*.\nThe intercept factor loadings are fixed to unity, while slope factor loadings increment from 0 to 9. \nLatent variances and covariance are represented by parameters $\\Psi_{11}$, $\\Psi_{22}$, and $\\Psi_{12}$. \nAdditionally, latent intercepts for both growth factors are freely estimated, representing average initial status and growth trajectory, respectively. \n\nThis model, illustrated in @fig-path-growth, involves a total of 6 parameters (see @tbl-truth-growth).\nThe reduce number of parameters is due to contraining all error variances $\\operatorname{Var}(\\epsilon_{j})$ to be equal (to $\\Theta$) across all measurement occasions, rather than being subscripted by $j$ as in the regular SEM.\nThe reason for this is that the latent GCM model can equivalently be viewed as a random intercept and random slope (multilevel) model, where repeated observations over time are nested within individuals. \nUnder this hierarchical perspective, it is customary--and theoretically justified--to assume homogeneous measurement error variances across repeated measures, reflecting consistent reliability of observations at each measurement occasion. \nThis equivalence also motivates our examination of Restricted Maximum Likelihood (REML) estimation, which is commonly employed in multilevel modelling contexts to reduce bias in the estimation of variance components, particularly when dealing with limited sample sizes.\nA description of the REML method is provided in the next subsection.\n\n::: {#fig-path-growth}\n::: {.cell}\n\n```{.tex .cell-code .hidden}\n\\usetikzlibrary{shapes.geometric,arrows,calc,positioning}\n\\begin{tikzpicture}\n\\node[inner sep=3pt] {%\n\\begin{tikzpicture}[%\n  >=stealth,              % for arrow tips\n  auto,                   % automatic label positioning\n  node distance=1.6cm,\n  every node/.style={font=\\small},\n  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},\n  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},\n  error/.style={circle, draw, minimum size=6mm, inner sep=0mm},\nintercept/.style={regular polygon, regular polygon sides=3, draw, inner sep=0pt, minimum size=6mm}\n  ]\n\n%-------------------------------------------------------\n% 1) LATENT FACTORS (intercept i, slope s)\n%-------------------------------------------------------\n\\node[latent] (i) at (-2,-3) {$i$};\n\\node[latent] (s) at ( 2,-3) {$s$};\n\\node[intercept] (int1) at (-2,-4.2) {\\footnotesize 1};\n\\node[intercept] (int2) at (2,-4.2) {\\footnotesize 1};\n\n%-------------------------------------------------------\n% 2) COVARIANCES AMONG LATENT FACTORS\n%    - psi_{11} on i\n%    - psi_{22} on s\n%    - psi_{21} between i and s\n%-------------------------------------------------------\n% i <-> s\n\\draw[<->] (i) to[out=-45, in=225] node[below] {$\\Psi_{12}$} (s);\n\n% Variance of i\n\\draw[<->] (i) to[out=190, in=220, looseness=4] \n  node[left] {$\\Psi_{11}$} (i);\n\n% Variance of s\n\\draw[<->] (s) to[out=-40, in=-10, looseness=4] \n  node[right] {$\\Psi_{22}$} (s);\n\n\\draw[->] (int1) -- node[right,pos=0.3] {$\\alpha_1$} (i);\n\\draw[->] (int2) -- node[right,pos=0.3] {$\\alpha_2$} (s);\n\n%-------------------------------------------------------\n% 3) OBSERVED VARIABLES (y1..y10) + ERRORS (eps1..eps10)\n%    Arrange them in a horizontal row above i and s.\n%-------------------------------------------------------\n\\foreach \\j in {1,...,10} {\n  % X-position for y_j (shift them so they are roughly centered)\n  \\pgfmathsetmacro{\\x}{(\\j*1.3 - 6.5)}\n\n  % Observed variable y_j\n  \\node[obs] (y\\j) at (\\x,0) {$y_{\\j}$};\n\n  % Error term epsilon_j above y_j\n  \\node[error] (e\\j) at (\\x,1.5) {$\\epsilon_{\\j}$};\n\n  % Arrow from error to observed\n  \\draw[->] (e\\j) -- (y\\j);\n\n  % Intercept factor loadings: all = 1\n  \\draw[->] (i) -- node[pos=0.45,right] {\\footnotesize 1} (y\\j);\n\n  % Slope factor loadings: 0..9\n  \\pgfmathtruncatemacro{\\loading}{\\j - 1}\n  \\draw[->] (s) -- node[pos=0.7,right] {\\footnotesize\\loading} (y\\j);\n}\n\n\\end{tikzpicture}\n};\n\\end{tikzpicture}\n```\n\n\n::: {.cell-output-display}\n![](index_files/figure-pdf/pathgrowth-1.pdf)\n:::\n:::\nPath diagram for the growth curve model.\n:::\n\n### Competitor methods\n\nIn @dhaene2022resampling, the authors compared four bias reduction methods on the two SEM models above namely the Bootstrap, Jackknife, Ozenne et al.'s [-@ozenne2020small] method, and Restricted Maximum Likelihood (REML).\nThe first two of these methods involve resampling, while @ozenne2020small and REML are analytical methods.\n\nExplain jackknife...\n\nExplain bootstrap...\n\nExplain Ozenne...\n\nExplain REML...\n\n### Simulation scenarios\n\nAssessing bias reduction under these various settings is crucial because estimation methods may behave differently when a departure from ideal condition occurs.\nTo thoroughly assess robustness and performance of the proposed methods, we considered multiple scenarios varying by three experimental factors: \n\n- Sample size $n=15,20,50,100,1000$\n- Reliability of the observed variables $\\text{rel}=0.5,0.8$\n- Distribution of $\\boldsymbol\\eta$ and $\\boldsymbol\\epsilon$, one of \"Normal\", \"Kurtosis\", or \"Non-normal\".\n\nReliability in this context refers to the extent to which an observed indicator accurately reflects the underlying latent construct, with higher reliability indicating that a greater proportion of the indicator’s variance is attributable to the true score, and a lower proportion to measurement error. \nLower reliability (indicating high measurement error) can exacerbate finite sample bias, particularly in the estimation of variance components, and thus influence the overall performance of bias-correction techniques. \nThe chosen values of 0.8 and 0.5 are justified as they represent two distinct and informative measurement scenarios: a reliability of 0.8 simulates a close to ideal condition where the indicators capture 80% of the true variability, thereby reflecting a high-quality measurement scenario, while a reliability of 0.5 simulates a more challenging condition.\n\n::: {#tbl-panel layout-ncol=2}\n\n\n::: {#tbl-truth-twofac .cell tbl-cap='Two-factor model parameters.'}\n\n```{.r .cell-code .hidden}\ntibble(\n  rel = paste0(\"Reliability = \", c(0.8, 0.5))\n) |>\n  mutate(\n    truth = list(truth_twofac(0.8), truth_twofac(0.5)),\n    param = map(truth, ~ names(.x))\n  ) |>\n  unnest(c(truth, param)) |> \n  mutate(param = case_when(\n    param == \"fx=~x2\" ~ \"$\\\\Lambda_{2,1}$\",\n    param == \"fx=~x3\" ~ \"$\\\\Lambda_{3,1}$\",\n    param == \"fy=~y2\" ~ \"$\\\\Lambda_{5,2}$\",\n    param == \"fy=~y3\" ~ \"$\\\\Lambda_{6,2}$\",\n    param == \"fy~fx\"  ~ \"$\\\\beta$\",\n    param == \"x1~~x1\" ~ \"$\\\\Theta_{1,1}$\",\n    param == \"x2~~x2\" ~ \"$\\\\Theta_{2,2}$\",\n    param == \"x3~~x3\" ~ \"$\\\\Theta_{3,3}$\",\n    param == \"y1~~y1\" ~ \"$\\\\Theta_{4,4}$\",\n    param == \"y2~~y2\" ~ \"$\\\\Theta_{5,5}$\",\n    param == \"y3~~y3\" ~ \"$\\\\Theta_{6,6}$\",\n    param == \"fx~~fx\" ~ \"$\\\\Psi_{1,1}$\",\n    param == \"fy~~fy\" ~ \"$\\\\Psi_{2,2}$\"\n  )) |>\n  mutate(truth = as.character(truth)) |>\n  pivot_wider(names_from = rel, values_from = truth) |>\n  gt(rowname_col = \"param\") |>\n  fmt_markdown(\"param\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rr}\n\\toprule\n & Reliability = 0.8 & Reliability = 0.5 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\(\\Lambda_{2,1}\\) & 0.7 & 0.7 \\\\ \n\\(\\Lambda_{3,1}\\) & 0.6 & 0.6 \\\\ \n\\(\\Lambda_{5,2}\\) & 0.7 & 0.7 \\\\ \n\\(\\Lambda_{6,2}\\) & 0.6 & 0.6 \\\\ \n\\(\\beta\\) & 0.25 & 0.25 \\\\ \n\\(\\Theta_{1,1}\\) & 0.25 & 1 \\\\ \n\\(\\Theta_{2,2}\\) & 0.1225 & 0.49 \\\\ \n\\(\\Theta_{3,3}\\) & 0.09 & 0.36 \\\\ \n\\(\\Theta_{4,4}\\) & 0.25 & 1 \\\\ \n\\(\\Theta_{5,5}\\) & 0.1225 & 0.49 \\\\ \n\\(\\Theta_{6,6}\\) & 0.09 & 0.36 \\\\ \n\\(\\Psi_{1,1}\\) & 1 & 1 \\\\ \n\\(\\Psi_{2,2}\\) & 1 & 1 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n::: {#tbl-truth-growth .cell tbl-cap='Growth curve model parameters.'}\n\n```{.r .cell-code .hidden}\ntibble(\n  rel = paste0(\"Reliability = \", c(0.8, 0.5))\n) |>\n  mutate(\n    truth = list(truth_growth(0.8), truth_growth(0.5)),\n    param = map(truth, ~ names(.x))\n  ) |>\n  unnest(c(truth, param)) |>\n  distinct(rel, truth, param, .keep_all = TRUE) |>\n  mutate(param = case_when(\n    param == \"v\" ~ \"$\\\\theta$\",\n    param == \"i~~i\" ~ \"$\\\\Psi_{1,1}$\",\n    param == \"s~~s\" ~ \"$\\\\Psi_{2,2}$\",\n    param == \"i~~s\" ~ \"$\\\\Psi_{1,2}$\",\n    param == \"i~1\" ~ \"$\\\\alpha_1$\",\n    param == \"s~1\" ~ \"$\\\\alpha_2$\",\n  )) |>\n  mutate(ord = case_when(\n    grepl(\"Psi\", param) ~ 3,\n    grepl(\"alpha\", param) ~ 1,\n    grepl(\"theta\", param) ~ 2\n  )) |>\n  arrange(ord) |>\n  select(-ord) |>\n  mutate(truth = as.character(truth)) |>\n  pivot_wider(names_from = rel, values_from = truth) |>\n  gt(rowname_col = \"param\") |>\n  fmt_markdown(\"param\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rr}\n\\toprule\n & Reliability = 0.8 & Reliability = 0.5 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\(\\alpha_1\\) & 0 & 0 \\\\ \n\\(\\alpha_2\\) & 0 & 0 \\\\ \n\\(\\theta\\) & 500 & 1300 \\\\ \n\\(\\Psi_{1,1}\\) & 550 & 275 \\\\ \n\\(\\Psi_{2,2}\\) & 100 & 50 \\\\ \n\\(\\Psi_{1,2}\\) & 40 & 20 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\nTrue parameter values used in the simulations for the each reliability setting.\n:::\n\nDistribution-free estimation methods exist for SEM, which are initially derived from the Gaussian likelihood, but is generally thought to be quite robust to non-normality.\nInvestigating the effect of departing from the baseline normality assumption in SEM is also of interest.\nThis was done by replacing the normal distribution of *both* the latent factors and the measurement errors with one that has higher-order moments, yet still retains the same covariance structure.\nIn other words, every simulated dataset is constructed to have the same underlying covariance matrix, but the distributional shape (e.g., the bell-shaped curve in the normal case or the altered spread and tail behaviour in the non-normal and kurtosis cases) differs. \nThe `{covsim}` [@gronneberg2022covsim] package was utilised for this purpose.\nThe function allows to generate multivariate data with a target covariance matrix while controlling for the excess kurtosis and skewness of the distribution.\nIn the baseline Normal distribution, both the skewness and excess kurtosis were set to $0$.\nIn the \"Kurtosis\" condition, excess kurtosis was set to $6$ (with skewness held at $0$) to induce heavier tails; and in the \"Non-normal\" condition, skewness was set to $-2$ with excess kurtosis of $6$, producing asymmetric heavy-tailed distributions.\n@fig-compare-dist compares the shapes of the distributions for the three conditions.\nOnce the data were generated, the observed variables were computed as per equations ... for each model.\n\n::: {#cell-fig-compare-dist .cell}\n\n```{.r .cell-code .hidden}\nn <- 1000\nrho <- 0.3\nSigma <- matrix(c(1, rho, rho, 1), nrow = 2)\ndat <- list()\ndat$Normal <-\n  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(0, 2))[[1]]\ndat$Kurtosis <-  # kurtosis\n  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(6, 2))[[1]]\ndat$`Non-normal` <-  # non-normal\n  covsim::rIG(n, Sigma, skewness = rep(-2, 2), excesskurtosis = rep(6, 2))[[1]]\n\n# get max and min for x and y axes\nxyminmax <- \n  map(dat, as.data.frame) |>\n  bind_rows() |>\n  summarise(\n    x_min = min(V1),\n    x_max = max(V1),\n    y_min = min(V2),\n    y_max = max(V2)\n  ) |> \n  unlist() \n\nallplots <- imap(dat, \\(x, idx) {\n  mycols <- rev(RColorBrewer::brewer.pal(3, \"Set1\"))\n  names(mycols) <- names(dat)\n  \n  p <-\n    as.data.frame(x) |>\n    ggplot(aes(x = V1, y = V2)) +\n    geom_point(alpha = 0.5, size = 1, col = mycols[idx]) +\n    geom_density_2d(col = \"black\") +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    scale_x_continuous(limits = c(xyminmax[\"x_min\"], xyminmax[\"x_max\"])) +\n    scale_y_continuous(limits = c(xyminmax[\"y_min\"], xyminmax[\"y_max\"])) +\n    labs(title = glue::glue(\"{idx}\"), x = NULL, y = NULL)\n  ggExtra:::ggMarginal(p, fill = mycols[idx], alpha = 0.5, size = 10, \n                       trim = idx != \"Non-normal\") \n})\ncowplot::plot_grid(plotlist = allplots, nrow = 1, labels = \"auto\")\n```\n\n::: {.cell-output-display}\n![Scatterplots illustrating the three distributional conditions used in the simulation study. Each plot displays 1000 randomly generated observations drawn from a distribution with zero mean vector and covariance matrix $\\begin{bmatrix} 1 & 0.3 \\\\ 0.3 & 1 \\end{bmatrix}$. Marginal density plots on the axes highlight differences in symmetry and tail behavior across conditions.](index_files/figure-pdf/fig-compare-dist-1.pdf){#fig-compare-dist width=100%}\n:::\n:::\n\n### Performance measures\n\nAcross these $5\\times 2 \\times 3 = 30$ distinct scenarios, 1000 replications of the data were generated and fitted using the ML, our two bias reduction methods (eBR and iBR), as well as the competitor methods.\nThe performance of each method was evaluated using the criteria below.\nIn what follows, let $r=1,\\dots,R$ index the replications, $\\hat{\\theta}^{(r)}$ be the parameter estimates from the $r$-th replication, and $\\theta$ be the true parameter value.\n\n1. Mean bias\n  $$\n  \\operatorname{Bias}(\\hat{\\theta}) = \\frac{1}{R}\\sum_{r=1}^{R}(\\hat{\\theta}^{(r)} - \\theta)\n  $$\n\n2. Probability of underestimation (median)\n  $$\n  \\operatorname{P}(\\hat{\\theta}^{(r)} < \\theta) = \\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{1}(\\hat{\\theta}^{(r)} < \\theta)\n  $$\n\n3. Root Mean Square Error (RMSE)\n  $$\n  \\text{RMSE}(\\hat{\\theta}) = \\sqrt{\\frac{1}{R}\\sum_{r=1}^{R}(\\hat{\\theta}^{(r)} - \\theta)^2}\n  $$\n\n4. Coverage of 95% Wald Confidence Intervals\n  $$\n  \\text{Coverage} = \\frac{1}{R}\\sum_{r=1}^{R}\\mathbb{1}\\left(\\hat{\\theta}^{(r)} - 1.96\\sqrt{\\widehat{\\text{Var}}(\\hat{\\theta}^{(r)})} \\leq \\theta \\leq \\hat{\\theta}^{(r)} + 1.96\\sqrt{\\widehat{\\text{Var}}(\\hat{\\theta}^{(r)})}\\right)\n  $$\n\nResults summarising these outcomes are discussed in detail in the next section.\n\n\n## Results\n\n### Success rates\n\n::: {#tbl-convsucc .cell tbl-cap='Success rates for estimation of the two-factor model for the ML, eBR, and iBR methods. Highlighted in red are the scenarios which produced success rates below 50%.'}\n\n```{.r .cell-code .hidden}\ngt(res_conv) |>\n  tab_spanner(\n    columns = matches(\"twofac.*0\\\\.8\"),\n    label = \"Reliability = 0.8\",\n    id = \"twofac80\"\n  ) |>\n  tab_spanner(\n    columns = matches(\"twofac.*0\\\\.5\"),\n    label = \"Reliability = 0.5\",\n    id = \"twofac50\"\n  ) |>\n  tab_spanner(\n    columns = starts_with(\"twofac\"),\n    label = \"Two-factor model\"\n  ) |>\n  tab_spanner(\n    columns = matches(\"growth.*0\\\\.8\"),\n    label = \"Reliability = 0.8\",\n    id = \"growth80\"\n  ) |>\n  tab_spanner(\n    columns = matches(\"growth.*0\\\\.5\"),\n    label = \"Reliability = 0.5\",\n    id = \"growth50\"\n  ) |>\n  tab_spanner(\n    columns = starts_with(\"growth\"),\n    label = \"Growth curve model\"\n  ) |>\n  fmt_percent(contains(\"0.\"), decimals = 1) |>\n  cols_label(\n    contains(\"ML\") ~ \"ML\",\n    contains(\"eRBM\") ~ \"eRBM\",\n    contains(\"iRBM\") ~ \"iRBM\"\n  ) |>\n  data_color(\n    columns = -all_of(\"n\"),\n    domain = c(0, 1.1),\n    palette = c(\"red3\", \"white\",  \"white\")\n  ) |>\n  tab_options(table.font.size = \"11px\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{8.2pt}{9.9pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}rrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{6}{c}{Two-factor model} & \\multicolumn{6}{c}{Growth curve model} \\\\ \n\\cmidrule(lr){2-7} \\cmidrule(lr){8-13}\n & \\multicolumn{3}{c}{Reliability = 0.8} & \\multicolumn{3}{c}{Reliability = 0.5} & \\multicolumn{3}{c}{Reliability = 0.8} & \\multicolumn{3}{c}{Reliability = 0.5} \\\\ \n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\nn & ML & eRBM & iRBM & ML & eRBM & iRBM & ML & eRBM & iRBM & ML & eRBM & iRBM \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{13}{l}{Normal} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\n15 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{91.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.3\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFD0C4}{\\textcolor[HTML]{000000}{42.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{92.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{95.5\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.7\\%}}} \\\\ \n20 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{96.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFF3F0}{\\textcolor[HTML]{000000}{51.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.6\\%}}} \\\\ \n50 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{91.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.6\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n100 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n1000 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{13}{l}{Kurtosis} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\n15 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{87.3\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFC8B9}{\\textcolor[HTML]{000000}{40.6\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{88.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{87.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{95.7\\%}}} \\\\ \n20 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.3\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFEBE5}{\\textcolor[HTML]{000000}{49.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{89.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{93.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.4\\%}}} \\\\ \n50 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{87.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.6\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.8\\%}}} \\\\ \n100 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} \\\\ \n1000 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{13}{l}{Non-normal} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\n15 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{81.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{89.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFC1B1}{\\textcolor[HTML]{000000}{38.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{81.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{81.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{95.2\\%}}} \\\\ \n20 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{90.4\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{94.5\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFEAE4}{\\textcolor[HTML]{000000}{49.5\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{86.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{86.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.4\\%}}} \\\\ \n50 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.9\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.8\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{85.7\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{97.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{98.2\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.8\\%}}} \\\\ \n100 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{99.1\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n1000 & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} & {\\cellcolor[HTML]{FFFFFF}{\\textcolor[HTML]{000000}{100.0\\%}}} \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\nBefore evaluating parameter estimates, we first examined the success rate of the estimation methods across all simulation scenarios. \nA successful estimation was determined if \n1) the optimiser reported successful convergence; and \n2) standard errors of estimates fell within acceptable numerical ranges. \nSpecifically, for the two-factor model, all standard errors were required to be less than 5, whereas for the growth curve model, reflecting differences in the scaling of observed variables, standard errors needed to be less than 500. \nAdditionally, in all cases, we required that the implied covariance matrix $\\hat{\\Sigma}$ to be positive definite.\n\nIn summarising the results we first filtered out the unsuccessful estimation cases.\nOur eRBM and iRBM methods demonstrated high success rates overall across simulation conditions, with a few exceptions. \nFor sample sizes $n \\geq 50$, the success rates were consistently above 85%, reaching 100% convergence for all scenarios with $n=100$ and $n=1000$.\n\nHowever, we observed notably lower success rates for smaller sample sizes ($n=15,20$), especially pronounced in the two-factor model when employing the eRBM. \nSince convergence criteria were met by the maximum likelihood step, these failures stemmed specifically from numerical instabilities arising during the computation of the derivatives required for the correction term.\nMoreover, it is possible that the post-hoc correction brings the parameter estimates outside the feasible parameter space, leading to non-positive definite covariance matrices.\nConsequently, caution should be exercised when interpreting eRBM estimates obtained from very small sample scenarios in the two-factor setting.\n\n### Two-factor model results\n\nWe first focus on the two-factor model under the scenario with normally distributed errors and high reliability (0.8), as this scenario yielded the highest convergence rates across all estimation methods, enabling a fair and direct comparison.\nSimilar to the work by @dhaene2022resampling, we focus our evaluation on a subset of key parameters, namely the error variance for item $y_1$, $\\Theta_{1,1}$, the second factor loading for item $y_1$, $\\Lambda_{2,1}$, the two variance parameters for the latent factors, $\\Psi_{1,1}$ and $\\Psi_{2,2}$, and the regression coefficient $\\beta$.\nThis selection spans a range of parameter types, providing a comprehensive overview of estimation performance across different components of the model.\n\nThe distribution plots of the centred parameter estimates (see @fig-centdist-twofac) revealed substantial variability across all methods at smaller sample sizes.\nHowever, variability decreased notably as the sample size increased. \n@fig-perf-twofac presents the performance metrics for the two-factor model, including relative bias, RMSE, probability of underestimation, and coverage rates.\nAcross all evaluated conditions, our proposed methods (eRBM and iRBM) consistently achieved reductions in mean bias compared to maximum likelihood (ML), particularly at smaller sample sizes ($n = 15, 20, 50$). \nRoot mean square error (RMSE), however, remained approximately equivalent across all methods, indicating similar levels of overall variability, a result visually corroborated by the comparative distribution plots.\nOur bias-reduction methods also provided notable improvements in confidence interval coverage, with increases of up to approximately 5% over ML, again primarily in smaller-sample scenarios.\n\n::: {#cell-fig-centdist-twofac .cell}\n\n```{.r .cell-code .hidden}\np1 <-\n  plot_df |>\n  filter(param %in% twofacpars) |> \n  ggplot(aes(bias, param, fill = method)) +\n  geom_boxplot(alpha = 0.8, outlier.size = 0.3, linewidth = 0.3) +\n  # geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(beta),\n    expression(Lambda[\"2,1\"])\n  ))) +\n  facet_grid(n ~ .) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"inside\")) +\n  theme_bw() +\n  theme(\n    legend.position.inside = c(0.91, 0.095), \n    legend.background = element_rect(fill = NA), \n    legend.text = element_text(size = 8), \n    legend.title = element_text(size = 9),\n    strip.background = element_blank(),\n    strip.text = element_blank()    \n  ) +\n  labs(x = NULL, y = NULL, fill = \"Method\", subtitle = glue::glue(\"{plot_df$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df$rel[1])}\"))\n\np2 <-\n  plot_df |>\n  filter(param %in% twofacpars) |> \n  summarise(count = n(), .by = dist:param) |>\n  ggplot(aes(count, param, fill = method)) +\n  geom_col(width = 0.8, position = position_dodge()) +\n  geom_vline(xintercept = 2000, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_x_continuous(expand = c(0, 0, 0, 100)) +\n  facet_grid(n ~ .) +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(), \n    axis.ticks.y = element_blank(), \n    axis.text.x = element_text(size = 7.5), \n    legend.position = \"none\"\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \" \")\n\ncowplot::plot_grid(p1, p2, rel_widths = c(1, 1 / 3))\n```\n\n::: {.cell-output-display}\n![Centered distributions of estimates for the two-factor model (left panel) using normally generated data at 80% reliability. Non-convergence cases and extreme estimates have been excluded. The right panel displays the number of estimates used to compute the summary statistics.](index_files/figure-pdf/fig-centdist-twofac-1.pdf){#fig-centdist-twofac width=100%}\n:::\n:::\n\n\n::: {#cell-fig-perf-twofac .cell}\n\n```{.r .cell-code .hidden}\nplot_df |> \n  filter(param %in% twofacpars) |> \n  summarise(\n    B = mean(bias, na.rm = TRUE, trim = 0.05),\n    rmse = sqrt(mean(bias ^ 2, na.rm = TRUE, trim = 0.05)),\n    pu = mean(bias < 0),\n    covr = mean(covered, na.rm = TRUE),\n    .by = c(dist:param)\n  ) |>\n  pivot_longer(B:covr, names_to = \"metric\", values_to = \"value\") |>\n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"B\", \"rmse\", \"pu\", \"covr\"),\n      labels = c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")\n    )\n  ) |>\n  ggplot(aes(value, param, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  geom_vline(\n    data = tibble(\n      metric = factor(c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")),\n      value = c(0, 0, 0.5, 0.95)\n    ),\n    aes(xintercept = value),\n    linetype = \"dashed\"\n  ) +\n  scale_fill_manual(values = mycols) +\n  facet_grid(n ~ metric, scales = \"free_x\") +\n  ggh4x::facetted_pos_scales(\n    x = list(\n      scale_x_continuous(),\n      scale_x_continuous(expand = c(0, 0, 0, 0.1)),\n      scale_x_continuous(limits = c(0.3, 0.7), labels = scales::percent),\n      scale_x_continuous(limits = c(0.6, 1), labels = scales::percent)\n    )\n  ) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(beta),\n    expression(Lambda[\"2,1\"])\n  ))) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"bottom\")) +\n  theme_bw() +\n  theme(\n    axis.text.x = element_text(size = 7.5),\n    legend.key.height = unit(1, \"pt\"), \n    legend.key.width = unit(9, \"pt\")\n  ) +\n  labs(x = NULL, y = NULL, fill = NULL, subtitle = glue::glue(\"{plot_df$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df$rel[1])}\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: `is.ggproto()` was deprecated in ggplot2 3.5.2.\ni Please use `is_ggproto()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Performance metrics (relative bias, RMSE, probability of understimation, and coverage) of the ML, eBR, and iBR methods for estimation of the two-factor model. Vertical dashed lines indicate the ideal values for each metric.](index_files/figure-pdf/fig-perf-twofac-1.pdf){#fig-perf-twofac width=100%}\n:::\n:::\n\nComparisons with the resampling-based approaches of @dhaene2022resampling--including the Jackknife, Bootstrap--and the analytic Ozenne method, showed that our methods consistently improved relative mean bias across all reliability settings and distributional scenarios. \n@fig-meanbias-twofac and @fig-rmse-twofac demonstrate the relative mean bias and RMSE of our methods compared to aforementioned methods, respectively.\nFor brevity, we excluded the Kurtosis scenario from the plots, as it produced roughly similar results to the normal distribution.\nThe full table of results are available in the Appendix.\n\nWhile the bias performance of our methods was generally comparable to these established approaches, the resampling-based methods tended to achieve slightly lower bias at the cost of greater variability, particularly pronounced in scenarios with non-normal errors. \nOur eRBM and iRBM methods did not share this disadvantage, providing robust and stable estimates across scenarios.\nOne notable exception occurred in non-normal conditions with small sample sizes $(n = 15, 20)$, where the eRBM method encountered estimation difficulties, leading to rejection of approximately 60% of parameter estimates. \nThis reduced the observed bias performance in these specific cases. \nDespite this, median bias remained close to zero, suggesting that the inflated mean bias was driven predominantly by a small number of extreme estimates attributable to numerical instabilities.\n\n\n::: {#cell-fig-meanbias-twofac .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"twofac\", !method %in% c(\"lav\")) |>\n  ggplot(aes(n, relbias, col = method)) +\n  geom_line(linewidth = 0.75, alpha = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_color_manual(values = mycols) +\n  scale_x_continuous(labels = c(15, 20, 50, 100, 1000)) +\n  scale_y_continuous(labels = scales::percent) +\n  # coord_cartesian(ylim = c(-0.3, 0.3)) +\n  guides(colour = guide_legend(nrow = 1, reverse = TRUE, position = \"top\")) +\n  theme_bw() +\n  labs(x = \"Sample size (n)\", y = glue(\"Relative mean bias\"), col = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of relative mean bias of the ML, eBR, and iBR methods against the D&R methods for estimation of the two-factor model.](index_files/figure-pdf/fig-meanbias-twofac-1.pdf){#fig-meanbias-twofac width=100%}\n:::\n:::\n\n::: {#cell-fig-rmse-twofac .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"twofac\", !method %in% c(\"lav\")) |>\n  mutate(method = fct_rev(method)) |>\n  ggplot(aes(n, rmse, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  scale_x_continuous(breaks = 1:5, labels = c(15, 20, 50, 100, 1000)) +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_fill_manual(values = mycols) +\n  guides(fill = guide_legend(nrow = 1, position = \"bottom\")) +\n  theme_bw() +\n  theme(legend.key.height = unit(1, \"pt\"), legend.key.width = unit(9, \"pt\")) +\n  labs(x = \"Sample size (n)\", y = \"RMSE\", fill = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of RMSE of the ML, eBR, and iBR methods against the D&R methods for estimation of the two-factor model.](index_files/figure-pdf/fig-rmse-twofac-1.pdf){#fig-rmse-twofac width=100%}\n:::\n:::\n\n### Growth curve model results\n\nThe GCM saw more successful estimation cases than the two-factor SEM, even at smaller sample sizes and low reliability.\nWe focus on the normal distribution and low reliability (0.5) scenario as this is considered a more challenging case for estimation.\nIn total there were six free parameters to estimate, but we excluded the intercepts from analyses as these were not particularly interesting. \nThe four remaining parameters are all the components in the factor covariance matrix $\\Psi$, as well as the equality constrained error variance $\\Theta_{1,1}$.\n\n@fig-centdist-growth displays the distribution of estimates for the growth curve model, highlighting the centered distributions of estimates across methods.\nParameter estimates exhibited markedly lower variability--or at least, produced fewer outlier estimates in the boxplots--compared to the two-factor model\nThis is possibly attributable to the more parsimonious structure of the growth model (6 vs 13 parameters).\n@fig-perf-growth presents the performance metrics for the growth curve model, similar to before in the two-factor SEM experiments.\nAlso consistent with findings from before, both eRBM and iRBM methods yielded substantial improvements in bias and coverage relative to ML, particularly for small to moderate sample sizes. \n\nRMSE values remained comparable across methods, indicating consistent variability in parameter estimates across different approaches.\nNotably, our bias-reduced methods achieved better calibration in terms of median bias. Specifically, our methods consistently resulted in probabilities of underestimating parameters closer to the nominal 50% level, whereas ML tended to systematically overshoot, thus introducing bias.\nIt was interesting to see that the variance $\\Psi_{1,1}$ for the latent factor $\\eta_1$ benefited the most from the bias-reduction methods.\nFor context, the true value of this parameter was set to $\\Psi_{1,1}=275$, and the bias was reduced by 30 units (10.9%) at best in the $n=15$ scenario.\n\n::: {#cell-fig-centdist-growth .cell}\n\n```{.r .cell-code .hidden}\np1 <-\n  plot_df50 |>\n  filter(param %in% growthpars) |> \n  ggplot(aes(bias, param, fill = method)) +\n  geom_boxplot(alpha = 0.8, outlier.size = 0.3, linewidth = 0.3) +\n  # geom_vline(xintercept = 0, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(Psi[\"1,2\"])\n  ))) +\n  facet_grid(n ~ .) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"inside\")) +\n  theme_bw() +\n  theme(\n    legend.position.inside = c(0.89, 0.095), \n    legend.background = element_rect(fill = NA), \n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 9),\n    strip.background = element_blank(),\n    strip.text = element_blank()     \n  ) +\n  labs(x = NULL, y = NULL, fill = \"Method\", subtitle =  glue::glue(\"{plot_df50$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df50$rel[1])}\"))\n\np2 <-\n  plot_df |>\n  filter(param %in% growthpars) |> \n  summarise(count = n(), .by = dist:param) |>\n  ggplot(aes(count, param, fill = method)) +\n  geom_col(width = 0.8, position = position_dodge()) +\n  geom_vline(xintercept = 2000, linetype = \"dashed\") +\n  scale_fill_manual(values = mycols) +\n  scale_x_continuous(expand = c(0, 0, 0, 100)) +\n  facet_grid(n ~ .) +\n  theme_bw() +\n  theme(\n    axis.text.y = element_blank(), \n    axis.ticks.y = element_blank(), \n    axis.text.x = element_text(size = 7.5), \n    legend.position = \"none\"\n  ) +\n  labs(x = NULL, y = NULL, subtitle = \" \")\n\ncowplot::plot_grid(p1, p2, rel_widths = c(1, 1 / 3))\n```\n\n::: {.cell-output-display}\n![Centered distributions of estimates for the growth curve model (left panel) using normally generated data at 50% reliability. Non-convergence cases and extreme estimates have been excluded. The right panel displays the number of estimates used to compute the summary statistics.](index_files/figure-pdf/fig-centdist-growth-1.pdf){#fig-centdist-growth width=100%}\n:::\n:::\n\n\n::: {#cell-fig-perf-growth .cell}\n\n```{.r .cell-code .hidden}\nplot_df |>\n  filter(param %in% growthpars) |> \n  summarise(\n    B = mean(bias, na.rm = TRUE, trim = 0.05),\n    rmse = sqrt(mean(bias ^ 2, na.rm = TRUE, trim = 0.05)),\n    pu = mean(bias < 0),\n    covr = mean(covered, na.rm = TRUE),\n    .by = c(dist:param)\n  ) |>\n  pivot_longer(B:covr, names_to = \"metric\", values_to = \"value\") |>\n  mutate(\n    metric = factor(\n      metric,\n      levels = c(\"B\", \"rmse\", \"pu\", \"covr\"),\n      labels = c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")\n    )\n  ) |>\n  ggplot(aes(value, param, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  geom_vline(\n    data = tibble(\n      metric = factor(c(\"Bias\", \"RMSE\", \"Prob. underest.\", \"Coverage\")),\n      value = c(0, 0, 0.5, 0.95)\n    ),\n    aes(xintercept = value),\n    linetype = \"dashed\"\n  ) +\n  scale_fill_manual(values = mycols) +\n  facet_grid(n ~ metric, scales = \"free_x\") +\n  ggh4x::facetted_pos_scales(\n    x = list(\n      scale_x_continuous(),\n      scale_x_continuous(expand = c(0, 0, 0, 10)),\n      scale_x_continuous(limits = c(0.35, 0.65), labels = scales::percent),\n      scale_x_continuous(limits = c(0.6, 1), labels = scales::percent)\n    )\n  ) +\n  scale_y_discrete(labels = rev(c(\n    expression(Theta[\"1,1\"]),\n    expression(Psi[\"1,1\"]),\n    expression(Psi[\"2,2\"]),\n    expression(Psi[\"1,2\"])\n  ))) +\n  guides(fill = guide_legend(reverse = TRUE, position = \"bottom\")) +\n  theme_bw() +\n  theme(\n    axis.text.x = element_text(size = 7.5),\n    legend.key.height = unit(1, \"pt\"), \n    legend.key.width = unit(9, \"pt\")\n  ) +  \n  labs(x = NULL, y = NULL, fill = NULL, subtitle = glue::glue(\"{plot_df$dist[1]} distribution, reliability = {gsub('Rel = ', '', plot_df$rel[1])}\"))\n```\n\n::: {.cell-output-display}\n![Performance metrics (relative bias, RMSE, probability of understimation, and coverage) of the ML, eBR, and iBR methods for estimation of the growth curve model. Vertical dashed lines indicate the ideal values for each metric.](index_files/figure-pdf/fig-perf-growth-1.pdf){#fig-perf-growth width=100%}\n:::\n:::\n\nNext, we compared our methods to the competitor methods, including REML.\nIt is noted that the constraint of equal error variances across all 10 items contributed to the stabilisation of estimates, promoting bias reduction via averages.\nThis is seen clearly in the previous figures, as well as when comparing across methods (see @fig-meanbias-growth).\nIn the scenario involving normally distributed data, REML generally provided the lowest relative mean bias among all methods.\nThis result was expected given REML’s theoretical optimality in estimating variance components under normality. \nOn the flip side, REML exhibited substantially poorer performance under non-normal data conditions, underscoring its lack of robustness against departures from normality.\n\nThe results are, on the whole, expected.\nThe bias performance of our proposed eRBM and iRBM methods was consistently comparable--and occasionally superior--to existing resampling-based methods (Jackknife, Bootstrap) and the analytic Ozenne approach. \nUnlike REML, our methods maintained robustness in non-normal settings without compromising performance.\n\nIn low reliability scenarios, the estimate of the latent slope-intercept covariance $\\Psi_{1,2}$ tended to be the most unstable in small samples. \nOur methods did a great job in reducing the bias of this parameter, considering both ML and REML gave much greater bias.\nNotably, for $n\\geq 100$, differences between methods converge, with all methods yielding similar performance.\nThis emphasises that bias reduction is most valueable in small to moderate sample regimes, where ML is particularly vulnerable.\n\n::: {#cell-fig-meanbias-growth .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"growth\", !method %in% c(\"lav\")) |>\n  ggplot(aes(n, relbias, col = method)) +\n  geom_line(linewidth = 0.75, alpha = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_color_manual(values = mycols) +\n  scale_x_continuous(labels = c(15, 20, 50, 100, 1000)) +\n  scale_y_continuous(labels = scales::percent) +\n  # coord_cartesian(ylim = c(-0.25, 0.25)) +\n  guides(colour = guide_legend(nrow = 1, reverse = TRUE, position = \"top\")) +\n  theme_bw() +\n  labs(x = \"Sample size (n)\", y = \"Relative mean bias\", col = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of relative mean bias of the ML, eBR, and iBR methods against the D&R methods for estimation of the growth curve model.](index_files/figure-pdf/fig-meanbias-growth-1.pdf){#fig-meanbias-growth width=100%}\n:::\n:::\n\n::: {#cell-fig-rmse-growth .cell}\n\n```{.r .cell-code .hidden}\nplot_drcomp |>\n  filter(model == \"growth\", !method %in% c(\"lav\")) |>\n  mutate(method = fct_rev(method)) |>\n  ggplot(aes(n, rmse, fill = method)) +\n  geom_col(position = \"dodge\", width = 0.75) +\n  scale_x_continuous(breaks = 1:5, labels = c(15, 20, 50, 100, 1000)) +\n  ggh4x::facet_nested(param ~ rel + dist, labeller = label_parsed) +\n  scale_fill_manual(values = mycols) +\n  # coord_cartesian(ylim = c(0, 280)) +\n  guides(fill = guide_legend(nrow = 1, position = \"bottom\")) +\n  theme_bw() +\n  theme(legend.key.height = unit(1, \"pt\"), legend.key.width = unit(9, \"pt\")) +\n  labs(x = \"Sample size (n)\", y = \"RMSE\", fill = NULL)\n```\n\n::: {.cell-output-display}\n![Comparison of RMSE of the ML, eBR, and iBR methods against the D&R methods for estimation of the growth curve model.](index_files/figure-pdf/fig-rmse-growth-1.pdf){#fig-rmse-growth width=100%}\n:::\n:::\n\n\n## Discussion\n\n-Both the implicit (iRBM) and explicit (eRBM) bias-reduction methods demonstrated high convergence rates across various simulation scenarios, particularly for sample sizes $n\\geq 50$.\n- In both the two-factor model and latent GCM, iRBM and eRBM consistently reduced bias compared to maximum likelihood (ML) estimates, especially in small sample sizes. These methods also enhanced the coverage probabilities of confidence intervals. The eRBM and iRBM also showed robustness under non-normal data distributions for the GCM especially.\n- While resampling-based methods like Jackknife and Bootstrap occasionally achieved slightly lower bias, they often exhibited higher variability. Our methods provided a favourable balance between bias reduction and estimate stability.\n- Restricted Maximum Likelihood (REML) performed well under normal data conditions but showed significant bias when the data deviated from normality, highlighting its sensitivity to distributional assumptions.\n- The eRBM and iRBM method offered a computationally efficient alternative to traditional resampling techniques, achieving bias reduction with significantly lower computational cost. Further improvements could be made to the codebase to increase efficiency--at the moment it is a very naive implementation.\n\n\n\n## References {.unnumbered}\n\n::: {#refs}\n:::\n\n## Appendix {.unnumbered}\n\n### A Derivatives {.unnumbered}\n\n{{< lipsum 1 >}}\n\n### B Tables {.unnumbered}\n\n\n::: {#tbl-bias-twofac-80 .cell .column-page tbl-cap='Results (trimmed) two-factor model reliability 0.80.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_twofac_80_df, \"twofac\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.15 & -0.11 & -0.05 & -0.01 & 0.00 & -0.20 & -0.14 & -0.07 & -0.02 & -0.01 & -0.19 & -0.14 & -0.07 & -0.02 & 0.00 & -0.30 & -0.22 & -0.11 & -0.04 & -0.01 & 0.54 & 0.45 & 0.27 & 0.20 & 0.06 & 0.68 & 0.61 & 0.40 & 0.28 & 0.09 \\\\ \neRBM & -0.10 & -0.08 & -0.03 & 0.00 & 0.00 & -0.21 & -0.13 & -0.05 & -0.01 & 0.00 & -0.15 & -0.11 & -0.05 & -0.01 & 0.00 & -0.31 & -0.21 & -0.10 & -0.03 & 0.00 & 0.52 & 0.44 & 0.27 & 0.20 & 0.06 & 0.64 & 0.58 & 0.40 & 0.29 & 0.09 \\\\ \niRBM & -0.11 & -0.07 & -0.03 & 0.00 & 0.00 & -0.16 & -0.11 & -0.05 & -0.01 & 0.00 & -0.15 & -0.11 & -0.05 & -0.01 & 0.00 & -0.27 & -0.19 & -0.10 & -0.03 & 0.00 & 0.53 & 0.45 & 0.27 & 0.20 & 0.06 & 0.69 & 0.61 & 0.40 & 0.29 & 0.09 \\\\ \nJackknife & 0.00 & 0.00 & -0.01 & 0.01 & 0.00 & -0.04 & -0.03 & -0.03 & 0.00 & 0.00 & -0.04 & -0.03 & -0.03 & 0.00 & 0.00 & -0.18 & -0.12 & -0.08 & -0.02 & 0.00 & 0.66 & 0.51 & 0.28 & 0.20 & 0.06 & 0.87 & 0.71 & 0.41 & 0.29 & 0.09 \\\\ \nBootstrap & -0.04 & -0.02 & -0.01 & 0.01 & 0.00 & -0.10 & -0.06 & -0.04 & -0.01 & 0.00 & -0.07 & -0.05 & -0.03 & 0.00 & 0.00 & -0.20 & -0.14 & -0.08 & -0.02 & -0.01 & 0.59 & 0.50 & 0.28 & 0.20 & 0.06 & 0.73 & 0.65 & 0.42 & 0.29 & 0.09 \\\\ \nOzenne et al. & -0.09 & -0.06 & -0.03 & 0.00 & 0.00 & -0.14 & -0.10 & -0.06 & -0.01 & 0.00 & -0.14 & -0.09 & -0.05 & -0.01 & 0.00 & -0.25 & -0.18 & -0.10 & -0.03 & -0.01 & 0.56 & 0.47 & 0.28 & 0.20 & 0.06 & 0.71 & 0.63 & 0.41 & 0.29 & 0.09 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.08 & -0.05 & -0.02 & -0.02 & 0.00 & -0.09 & -0.09 & -0.04 & -0.02 & 0.00 & -0.11 & -0.08 & -0.03 & -0.02 & 0.00 & -0.20 & -0.17 & -0.07 & -0.04 & 0.00 & 0.40 & 0.35 & 0.22 & 0.16 & 0.05 & 0.57 & 0.50 & 0.35 & 0.26 & 0.08 \\\\ \neRBM & -0.07 & -0.06 & -0.03 & -0.02 & 0.00 & -0.04 & -0.06 & -0.04 & -0.03 & 0.00 & -0.11 & -0.08 & -0.04 & -0.03 & 0.00 & -0.14 & -0.15 & -0.08 & -0.05 & 0.00 & 0.40 & 0.35 & 0.22 & 0.16 & 0.05 & 0.56 & 0.50 & 0.35 & 0.26 & 0.08 \\\\ \niRBM & -0.09 & -0.06 & -0.03 & -0.02 & 0.00 & -0.09 & -0.09 & -0.04 & -0.02 & 0.00 & -0.12 & -0.09 & -0.04 & -0.02 & 0.00 & -0.19 & -0.18 & -0.08 & -0.05 & 0.00 & 0.40 & 0.35 & 0.22 & 0.16 & 0.05 & 0.57 & 0.51 & 0.35 & 0.26 & 0.08 \\\\ \nJackknife & 0.01 & 0.00 & -0.01 & -0.01 & 0.00 & 0.01 & -0.01 & -0.02 & -0.02 & 0.00 & -0.04 & -0.03 & -0.02 & -0.02 & 0.00 & -0.13 & -0.11 & -0.06 & -0.04 & 0.00 & 0.47 & 0.39 & 0.22 & 0.16 & 0.05 & 0.70 & 0.58 & 0.36 & 0.26 & 0.08 \\\\ \nBootstrap & 0.03 & 0.03 & 0.01 & 0.00 & 0.00 & 0.01 & 0.02 & 0.03 & 0.02 & 0.00 & -0.02 & 0.01 & 0.00 & -0.01 & 0.00 & -0.12 & -0.09 & -0.01 & -0.02 & 0.00 & 0.45 & 0.39 & 0.23 & 0.16 & 0.05 & 0.66 & 0.58 & 0.39 & 0.28 & 0.08 \\\\ \nOzenne et al. & -0.01 & 0.00 & 0.00 & -0.01 & 0.00 & -0.03 & -0.04 & -0.02 & -0.01 & 0.00 & -0.05 & -0.03 & -0.01 & -0.01 & 0.00 & -0.15 & -0.13 & -0.05 & -0.03 & 0.00 & 0.42 & 0.36 & 0.22 & 0.16 & 0.05 & 0.60 & 0.52 & 0.35 & 0.26 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.15 & -0.12 & -0.05 & -0.02 & 0.00 & -0.19 & -0.15 & -0.07 & -0.04 & 0.00 & -0.20 & -0.15 & -0.05 & -0.03 & 0.00 & -0.28 & -0.23 & -0.11 & -0.06 & 0.00 & 0.41 & 0.37 & 0.23 & 0.16 & 0.05 & 0.54 & 0.50 & 0.35 & 0.25 & 0.08 \\\\ \neRBM & -0.07 & -0.07 & -0.03 & -0.01 & 0.00 & -0.09 & -0.09 & -0.05 & -0.03 & 0.00 & -0.13 & -0.10 & -0.04 & -0.02 & 0.00 & -0.19 & -0.17 & -0.09 & -0.05 & 0.00 & 0.41 & 0.37 & 0.23 & 0.16 & 0.05 & 0.54 & 0.50 & 0.35 & 0.25 & 0.08 \\\\ \niRBM & -0.10 & -0.08 & -0.03 & -0.01 & 0.00 & -0.14 & -0.12 & -0.05 & -0.03 & 0.00 & -0.15 & -0.12 & -0.04 & -0.02 & 0.00 & -0.23 & -0.20 & -0.09 & -0.05 & 0.00 & 0.41 & 0.37 & 0.23 & 0.16 & 0.05 & 0.54 & 0.51 & 0.35 & 0.25 & 0.08 \\\\ \nJackknife & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & -0.04 & -0.04 & -0.03 & -0.02 & 0.00 & -0.06 & -0.05 & -0.02 & -0.01 & 0.00 & -0.14 & -0.13 & -0.07 & -0.04 & 0.00 & 0.47 & 0.39 & 0.24 & 0.16 & 0.05 & 0.63 & 0.55 & 0.36 & 0.25 & 0.08 \\\\ \nBootstrap & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.05 & -0.02 & 0.01 & 0.00 & 0.00 & -0.07 & -0.05 & -0.01 & -0.01 & 0.00 & -0.16 & -0.13 & -0.04 & -0.03 & 0.00 & 0.45 & 0.39 & 0.24 & 0.16 & 0.05 & 0.61 & 0.56 & 0.38 & 0.27 & 0.08 \\\\ \nOzenne et al. & -0.09 & -0.07 & -0.03 & -0.01 & 0.00 & -0.13 & -0.11 & -0.05 & -0.03 & 0.00 & -0.14 & -0.11 & -0.04 & -0.02 & 0.00 & -0.23 & -0.19 & -0.09 & -0.05 & 0.00 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.56 & 0.51 & 0.35 & 0.25 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\beta\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.03 & 0.05 & -0.02 & 0.01 & 0.00 & 0.03 & -0.04 & -0.02 & 0.01 & 0.00 & -0.01 & 0.02 & -0.02 & 0.01 & 0.00 & -0.06 & -0.11 & -0.05 & 0.00 & 0.00 & 1.21 & 1.02 & 0.57 & 0.40 & 0.12 & 1.32 & 1.07 & 0.59 & 0.40 & 0.12 \\\\ \neRBM & 0.00 & 0.03 & -0.02 & 0.01 & 0.00 & -0.04 & -0.08 & -0.04 & 0.00 & 0.00 & -0.05 & 0.00 & -0.03 & 0.01 & 0.00 & -0.16 & -0.17 & -0.07 & -0.01 & 0.00 & 1.15 & 1.00 & 0.57 & 0.40 & 0.12 & 1.20 & 1.01 & 0.59 & 0.40 & 0.12 \\\\ \niRBM & 0.00 & 0.04 & -0.02 & 0.01 & 0.00 & -0.04 & -0.08 & -0.03 & 0.00 & 0.00 & -0.04 & 0.02 & -0.03 & 0.01 & 0.00 & -0.15 & -0.15 & -0.07 & 0.00 & 0.00 & 1.17 & 1.01 & 0.57 & 0.40 & 0.12 & 1.26 & 1.03 & 0.59 & 0.40 & 0.12 \\\\ \nJackknife & -0.06 & 0.00 & -0.03 & 0.01 & 0.00 & -0.09 & -0.12 & -0.05 & 0.00 & 0.00 & -0.09 & -0.02 & -0.03 & 0.01 & 0.00 & -0.23 & -0.25 & -0.08 & -0.01 & 0.00 & 1.29 & 1.03 & 0.57 & 0.40 & 0.12 & 1.71 & 1.24 & 0.59 & 0.40 & 0.12 \\\\ \nBootstrap & -0.02 & 0.02 & -0.03 & 0.01 & 0.00 & -0.02 & -0.09 & -0.05 & -0.01 & 0.00 & -0.09 & -0.01 & -0.03 & 0.00 & 0.00 & -0.15 & -0.19 & -0.09 & -0.01 & 0.00 & 1.18 & 1.00 & 0.57 & 0.40 & 0.12 & 1.37 & 1.07 & 0.59 & 0.40 & 0.12 \\\\ \nOzenne et al. & 0.03 & 0.05 & -0.02 & 0.01 & 0.00 & 0.03 & -0.04 & -0.02 & 0.01 & 0.00 & -0.01 & 0.02 & -0.02 & 0.01 & 0.00 & -0.06 & -0.11 & -0.05 & 0.00 & 0.00 & 1.21 & 1.02 & 0.57 & 0.40 & 0.12 & 1.33 & 1.07 & 0.59 & 0.40 & 0.12 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\lambda_{2,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.02 & 0.01 & 0.00 & 0.00 & 0.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.21 & 0.18 & 0.10 & 0.07 & 0.02 & 0.23 & 0.19 & 0.11 & 0.07 & 0.02 \\\\ \neRBM & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.02 & -0.02 & -0.01 & 0.00 & 0.00 & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & 0.19 & 0.16 & 0.10 & 0.07 & 0.02 & 0.19 & 0.17 & 0.10 & 0.07 & 0.02 \\\\ \niRBM & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & -0.01 & 0.00 & 0.00 & 0.21 & 0.17 & 0.10 & 0.07 & 0.02 & 0.23 & 0.19 & 0.11 & 0.07 & 0.02 \\\\ \nJackknife & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.04 & -0.02 & -0.02 & 0.00 & 0.00 & 0.22 & 0.17 & 0.10 & 0.07 & 0.02 & 0.31 & 0.22 & 0.11 & 0.07 & 0.02 \\\\ \nBootstrap & -0.01 & -0.02 & -0.01 & 0.00 & 0.00 & -0.01 & -0.01 & -0.02 & 0.00 & 0.00 & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.02 & -0.01 & -0.02 & 0.00 & 0.00 & 0.21 & 0.17 & 0.10 & 0.07 & 0.02 & 0.25 & 0.20 & 0.11 & 0.07 & 0.02 \\\\ \nOzenne et al. & 0.02 & 0.01 & 0.00 & 0.00 & 0.00 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.21 & 0.18 & 0.10 & 0.07 & 0.02 & 0.23 & 0.19 & 0.11 & 0.07 & 0.02 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n::: {#tbl-bias-twofac-50 .cell .column-page tbl-cap='Results (trimmed) two-factor model reliability 0.50.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_twofac_50_df, \"twofac\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.16 & -0.13 & -0.06 & -0.02 & 0.00 & -0.23 & -0.18 & -0.08 & -0.03 & -0.01 & -0.17 & -0.13 & -0.07 & -0.02 & 0.00 & -0.31 & -0.23 & -0.12 & -0.05 & -0.01 & 0.57 & 0.49 & 0.29 & 0.20 & 0.06 & 0.69 & 0.63 & 0.42 & 0.29 & 0.09 \\\\ \neRBM & -0.21 & -0.15 & -0.03 & 0.00 & 0.00 & -0.32 & -0.26 & -0.07 & -0.01 & 0.00 & -0.24 & -0.16 & -0.04 & -0.01 & 0.00 & -0.41 & -0.30 & -0.10 & -0.03 & -0.01 & 0.49 & 0.41 & 0.27 & 0.20 & 0.06 & 0.60 & 0.53 & 0.38 & 0.28 & 0.09 \\\\ \niRBM & -0.11 & -0.08 & -0.03 & 0.00 & 0.00 & -0.19 & -0.12 & -0.06 & -0.01 & 0.00 & -0.12 & -0.08 & -0.04 & -0.01 & 0.00 & -0.27 & -0.19 & -0.09 & -0.03 & -0.01 & 0.58 & 0.51 & 0.29 & 0.20 & 0.06 & 0.72 & 0.66 & 0.42 & 0.29 & 0.09 \\\\ \nJackknife & 0.04 & 0.03 & 0.00 & 0.01 & 0.00 & 0.00 & 0.02 & -0.01 & 0.00 & 0.00 & 0.01 & 0.03 & -0.01 & 0.01 & 0.00 & -0.11 & -0.09 & -0.06 & -0.02 & 0.00 & 1.05 & 0.85 & 0.30 & 0.20 & 0.06 & 1.23 & 1.06 & 0.45 & 0.29 & 0.09 \\\\ \nBootstrap & -0.06 & -0.03 & 0.00 & 0.01 & 0.00 & -0.13 & -0.07 & -0.02 & 0.01 & 0.00 & -0.05 & -0.01 & -0.01 & 0.00 & 0.00 & -0.22 & -0.14 & -0.06 & -0.02 & 0.00 & 0.68 & 0.60 & 0.31 & 0.20 & 0.06 & 0.79 & 0.74 & 0.47 & 0.30 & 0.09 \\\\ \nOzenne et al. & -0.10 & -0.08 & -0.04 & -0.01 & 0.00 & -0.17 & -0.13 & -0.07 & -0.02 & -0.01 & -0.11 & -0.08 & -0.05 & -0.01 & 0.00 & -0.25 & -0.19 & -0.10 & -0.04 & -0.01 & 0.61 & 0.50 & 0.29 & 0.20 & 0.06 & 0.73 & 0.65 & 0.42 & 0.29 & 0.09 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.02 & 0.01 & 0.00 & -0.01 & 0.00 & 0.06 & 0.04 & 0.02 & -0.01 & 0.00 & -0.08 & -0.07 & -0.03 & -0.03 & 0.00 & -0.11 & -0.09 & -0.03 & -0.04 & 0.00 & 0.66 & 0.60 & 0.39 & 0.27 & 0.08 & 0.79 & 0.70 & 0.49 & 0.34 & 0.11 \\\\ \neRBM & 0.20 & 0.12 & -0.02 & -0.03 & 0.00 & 0.42 & 0.26 & 0.02 & -0.03 & 0.00 & 0.10 & 0.05 & -0.06 & -0.05 & 0.00 & 0.25 & 0.13 & -0.04 & -0.06 & -0.01 & 0.59 & 0.54 & 0.37 & 0.27 & 0.08 & 0.88 & 0.70 & 0.44 & 0.33 & 0.11 \\\\ \niRBM & -0.01 & -0.02 & -0.03 & -0.02 & 0.00 & 0.02 & -0.01 & -0.01 & -0.03 & 0.00 & -0.12 & -0.11 & -0.06 & -0.04 & 0.00 & -0.15 & -0.13 & -0.06 & -0.06 & -0.01 & 0.68 & 0.63 & 0.39 & 0.27 & 0.08 & 0.81 & 0.73 & 0.49 & 0.34 & 0.11 \\\\ \nJackknife & 0.02 & -0.02 & -0.02 & -0.02 & 0.00 & 0.05 & 0.01 & 0.00 & -0.02 & 0.00 & -0.19 & -0.17 & -0.06 & -0.04 & 0.00 & -0.25 & -0.19 & -0.07 & -0.06 & 0.00 & 1.34 & 1.07 & 0.42 & 0.27 & 0.08 & 1.47 & 1.21 & 0.55 & 0.34 & 0.11 \\\\ \nBootstrap & 0.02 & 0.01 & -0.01 & -0.02 & 0.00 & 0.04 & 0.01 & 0.02 & -0.02 & 0.00 & -0.12 & -0.10 & -0.05 & -0.04 & 0.00 & -0.14 & -0.14 & -0.04 & -0.05 & 0.00 & 0.81 & 0.72 & 0.42 & 0.28 & 0.08 & 0.92 & 0.83 & 0.55 & 0.36 & 0.11 \\\\ \nOzenne et al. & 0.09 & 0.06 & 0.02 & 0.00 & 0.00 & 0.13 & 0.09 & 0.04 & 0.00 & 0.00 & -0.02 & -0.02 & -0.01 & -0.02 & 0.00 & -0.05 & -0.04 & -0.01 & -0.03 & 0.00 & 0.71 & 0.63 & 0.39 & 0.27 & 0.08 & 0.85 & 0.74 & 0.50 & 0.34 & 0.11 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.13 & -0.12 & -0.04 & -0.02 & 0.00 & -0.14 & -0.11 & -0.05 & -0.03 & 0.00 & -0.25 & -0.18 & -0.07 & -0.04 & 0.00 & -0.30 & -0.21 & -0.10 & -0.06 & 0.00 & 0.65 & 0.57 & 0.38 & 0.26 & 0.08 & 0.74 & 0.66 & 0.46 & 0.32 & 0.10 \\\\ \neRBM & 0.06 & 0.04 & -0.03 & -0.02 & 0.00 & 0.10 & 0.12 & -0.02 & -0.03 & 0.00 & -0.01 & -0.04 & -0.06 & -0.04 & 0.00 & -0.06 & -0.02 & -0.07 & -0.06 & 0.00 & 0.62 & 0.52 & 0.38 & 0.26 & 0.08 & 0.71 & 0.65 & 0.44 & 0.32 & 0.10 \\\\ \niRBM & -0.09 & -0.07 & -0.04 & -0.02 & 0.00 & -0.08 & -0.07 & -0.05 & -0.03 & 0.00 & -0.20 & -0.16 & -0.07 & -0.03 & 0.00 & -0.25 & -0.19 & -0.09 & -0.06 & 0.00 & 0.69 & 0.62 & 0.40 & 0.26 & 0.08 & 0.81 & 0.72 & 0.47 & 0.33 & 0.10 \\\\ \nJackknife & 0.01 & -0.01 & -0.02 & -0.01 & 0.00 & -0.01 & -0.01 & -0.02 & -0.02 & 0.00 & -0.19 & -0.11 & -0.05 & -0.03 & 0.00 & -0.24 & -0.16 & -0.07 & -0.05 & 0.00 & 1.19 & 0.97 & 0.42 & 0.26 & 0.08 & 1.31 & 1.12 & 0.52 & 0.33 & 0.10 \\\\ \nBootstrap & -0.04 & -0.03 & -0.01 & -0.01 & 0.00 & -0.07 & -0.03 & -0.01 & -0.02 & 0.00 & -0.17 & -0.11 & -0.05 & -0.02 & 0.00 & -0.24 & -0.15 & -0.07 & -0.04 & 0.00 & 0.76 & 0.69 & 0.42 & 0.27 & 0.08 & 0.86 & 0.78 & 0.51 & 0.34 & 0.10 \\\\ \nOzenne et al. & -0.08 & -0.07 & -0.02 & -0.01 & 0.00 & -0.08 & -0.06 & -0.03 & -0.02 & 0.00 & -0.20 & -0.14 & -0.05 & -0.03 & 0.00 & -0.26 & -0.17 & -0.08 & -0.05 & 0.00 & 0.69 & 0.59 & 0.39 & 0.26 & 0.08 & 0.79 & 0.69 & 0.46 & 0.32 & 0.10 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\beta\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.09 & 0.14 & 0.00 & 0.03 & 0.00 & 0.16 & 0.05 & 0.00 & 0.01 & 0.01 & -0.09 & -0.03 & -0.05 & 0.01 & 0.00 & -0.11 & -0.13 & -0.06 & -0.02 & 0.00 & 1.79 & 1.54 & 0.79 & 0.53 & 0.16 & 1.95 & 1.55 & 0.82 & 0.55 & 0.16 \\\\ \neRBM & 0.22 & 0.21 & -0.03 & 0.01 & -0.01 & 0.09 & 0.01 & -0.07 & -0.02 & 0.00 & 0.20 & 0.12 & -0.06 & -0.01 & 0.00 & 0.01 & -0.05 & -0.11 & -0.05 & 0.00 & 1.46 & 1.27 & 0.73 & 0.52 & 0.16 & 1.51 & 1.27 & 0.74 & 0.54 & 0.16 \\\\ \niRBM & -0.01 & 0.02 & -0.03 & 0.02 & -0.01 & 0.02 & -0.08 & -0.05 & -0.01 & 0.00 & -0.22 & -0.16 & -0.08 & 0.00 & 0.00 & -0.30 & -0.33 & -0.11 & -0.04 & 0.00 & 1.61 & 1.43 & 0.78 & 0.52 & 0.16 & 1.84 & 1.49 & 0.82 & 0.55 & 0.16 \\\\ \nJackknife & 0.01 & 0.06 & -0.07 & 0.00 & -0.01 & 0.22 & -0.15 & -0.09 & -0.03 & 0.00 & -0.24 & -0.15 & -0.09 & -0.02 & 0.00 & -0.19 & -0.35 & -0.16 & -0.06 & 0.00 & 4.06 & 2.65 & 0.82 & 0.52 & 0.16 & 4.68 & 3.12 & 0.92 & 0.54 & 0.16 \\\\ \nBootstrap & 0.04 & 0.10 & -0.06 & 0.00 & -0.01 & 0.20 & -0.02 & -0.08 & -0.03 & 0.00 & -0.11 & -0.05 & -0.10 & -0.02 & 0.00 & -0.08 & -0.23 & -0.13 & -0.06 & 0.00 & 2.28 & 1.71 & 0.78 & 0.52 & 0.16 & 2.60 & 1.93 & 0.83 & 0.55 & 0.16 \\\\ \nOzenne et al. & 0.08 & 0.13 & 0.00 & 0.03 & 0.00 & 0.14 & 0.05 & 0.00 & 0.01 & 0.01 & -0.10 & -0.04 & -0.05 & 0.01 & 0.00 & -0.12 & -0.14 & -0.06 & -0.02 & 0.00 & 1.79 & 1.55 & 0.79 & 0.53 & 0.16 & 1.95 & 1.57 & 0.82 & 0.55 & 0.16 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\lambda_{2,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.05 & 0.05 & 0.04 & 0.01 & 0.00 & 0.01 & 0.03 & 0.01 & 0.02 & 0.00 & -0.03 & -0.02 & 0.00 & 0.00 & 0.00 & -0.05 & -0.03 & -0.02 & 0.00 & 0.00 & 0.53 & 0.47 & 0.28 & 0.18 & 0.06 & 0.55 & 0.47 & 0.29 & 0.19 & 0.06 \\\\ \neRBM & -0.09 & -0.10 & -0.03 & -0.01 & 0.00 & -0.07 & -0.06 & -0.05 & -0.01 & 0.00 & -0.10 & -0.10 & -0.05 & -0.02 & 0.00 & -0.09 & -0.08 & -0.06 & -0.02 & -0.01 & 0.31 & 0.31 & 0.23 & 0.17 & 0.06 & 0.32 & 0.29 & 0.23 & 0.18 & 0.06 \\\\ \niRBM & 0.08 & 0.05 & 0.02 & 0.00 & 0.00 & 0.07 & 0.04 & 0.00 & 0.01 & 0.00 & -0.03 & -0.03 & -0.01 & -0.01 & 0.00 & -0.05 & -0.03 & -0.03 & -0.01 & -0.01 & 0.64 & 0.55 & 0.28 & 0.18 & 0.06 & 0.69 & 0.58 & 0.30 & 0.19 & 0.06 \\\\ \nJackknife & 0.03 & -0.01 & -0.04 & -0.01 & 0.00 & 0.10 & 0.03 & -0.06 & -0.01 & 0.00 & -0.14 & -0.12 & -0.06 & -0.02 & 0.00 & -0.10 & -0.08 & -0.07 & -0.02 & -0.01 & 1.37 & 0.89 & 0.26 & 0.17 & 0.06 & 1.53 & 1.15 & 0.31 & 0.18 & 0.06 \\\\ \nBootstrap & 0.10 & 0.04 & -0.02 & -0.01 & 0.00 & 0.10 & 0.08 & -0.03 & -0.01 & 0.00 & -0.06 & -0.08 & -0.05 & -0.02 & 0.00 & -0.05 & -0.03 & -0.06 & -0.02 & -0.01 & 0.76 & 0.59 & 0.28 & 0.18 & 0.06 & 0.82 & 0.66 & 0.31 & 0.19 & 0.06 \\\\ \nOzenne et al. & 0.05 & 0.05 & 0.04 & 0.01 & 0.00 & 0.02 & 0.04 & 0.01 & 0.02 & 0.00 & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & -0.05 & -0.02 & -0.02 & 0.00 & 0.00 & 0.55 & 0.48 & 0.28 & 0.18 & 0.06 & 0.56 & 0.48 & 0.29 & 0.19 & 0.06 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n::: {#tbl-covr-twofac .cell .column-page tbl-cap='Coverage rates 95% confidence interval for the two-factor model.'}\n\n```{.r .cell-code .hidden}\ntab_covr(covr_twofac_df, \"twofac\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Normal data} & \\multicolumn{10}{c}{Non-Normal data} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21}\n & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\theta_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.85 & 0.89 & 0.93 & 0.95 & 0.95 & 0.89 & 0.92 & 0.95 & 0.95 & 0.95 & 0.75 & 0.76 & 0.78 & 0.82 & 0.81 & 0.76 & 0.78 & 0.80 & 0.82 & 0.81 \\\\ \neRBM & 0.90 & 0.93 & 0.95 & 0.95 & 0.95 & 0.89 & 0.93 & 0.96 & 0.96 & 0.95 & 0.77 & 0.79 & 0.81 & 0.83 & 0.81 & 0.73 & 0.78 & 0.84 & 0.83 & 0.81 \\\\ \niRBM & 0.89 & 0.92 & 0.95 & 0.95 & 0.95 & 0.91 & 0.94 & 0.96 & 0.96 & 0.95 & 0.78 & 0.78 & 0.80 & 0.83 & 0.81 & 0.78 & 0.79 & 0.81 & 0.83 & 0.81 \\\\ \nlav & 0.84 & 0.88 & 0.93 & 0.95 & 0.95 & 0.87 & 0.91 & 0.94 & 0.95 & 0.95 & 0.73 & 0.75 & 0.78 & 0.82 & 0.81 & 0.75 & 0.77 & 0.79 & 0.82 & 0.81 \\\\ \nOzenne et al. & 0.88 & 0.91 & 0.94 & 0.95 & 0.95 & 0.91 & 0.93 & 0.95 & 0.95 & 0.95 & 0.77 & 0.78 & 0.79 & 0.82 & 0.81 & 0.78 & 0.79 & 0.80 & 0.82 & 0.81 \\\\ \nJackknife & 0.82 & 0.87 & 0.94 & 0.95 & 0.95 & 0.72 & 0.78 & 0.93 & 0.96 & 0.95 & 0.72 & 0.77 & 0.87 & 0.90 & 0.94 & 0.65 & 0.69 & 0.86 & 0.92 & 0.94 \\\\ \nBootstrap & 0.75 & 0.81 & 0.91 & 0.94 & 0.94 & 0.69 & 0.75 & 0.90 & 0.95 & 0.95 & 0.64 & 0.69 & 0.83 & 0.89 & 0.94 & 0.57 & 0.62 & 0.79 & 0.89 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.86 & 0.88 & 0.93 & 0.94 & 0.94 & 0.85 & 0.88 & 0.91 & 0.93 & 0.94 & 0.72 & 0.73 & 0.74 & 0.75 & 0.77 & 0.79 & 0.81 & 0.85 & 0.86 & 0.88 \\\\ \neRBM & 0.84 & 0.86 & 0.91 & 0.93 & 0.94 & 0.89 & 0.87 & 0.83 & 0.88 & 0.94 & 0.72 & 0.73 & 0.73 & 0.74 & 0.77 & 0.86 & 0.85 & 0.79 & 0.81 & 0.88 \\\\ \niRBM & 0.84 & 0.87 & 0.91 & 0.93 & 0.94 & 0.76 & 0.76 & 0.85 & 0.90 & 0.94 & 0.71 & 0.71 & 0.73 & 0.74 & 0.77 & 0.69 & 0.70 & 0.78 & 0.83 & 0.88 \\\\ \nlav & 0.86 & 0.88 & 0.93 & 0.94 & 0.94 & 0.84 & 0.86 & 0.90 & 0.93 & 0.94 & 0.72 & 0.73 & 0.74 & 0.75 & 0.77 & 0.78 & 0.80 & 0.84 & 0.85 & 0.88 \\\\ \nOzenne et al. & 0.88 & 0.90 & 0.93 & 0.94 & 0.94 & 0.86 & 0.87 & 0.91 & 0.93 & 0.94 & 0.76 & 0.76 & 0.75 & 0.76 & 0.77 & 0.81 & 0.82 & 0.85 & 0.86 & 0.88 \\\\ \nJackknife & 0.81 & 0.84 & 0.92 & 0.93 & 0.94 & 0.64 & 0.71 & 0.89 & 0.93 & 0.94 & 0.70 & 0.73 & 0.82 & 0.86 & 0.94 & 0.58 & 0.64 & 0.84 & 0.90 & 0.94 \\\\ \nBootstrap & 0.70 & 0.73 & 0.85 & 0.89 & 0.94 & 0.63 & 0.69 & 0.85 & 0.92 & 0.94 & 0.54 & 0.58 & 0.71 & 0.79 & 0.94 & 0.56 & 0.61 & 0.80 & 0.88 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.80 & 0.83 & 0.89 & 0.93 & 0.94 & 0.79 & 0.82 & 0.88 & 0.93 & 0.94 & 0.68 & 0.69 & 0.73 & 0.75 & 0.77 & 0.73 & 0.77 & 0.83 & 0.85 & 0.87 \\\\ \neRBM & 0.84 & 0.86 & 0.90 & 0.94 & 0.94 & 0.82 & 0.86 & 0.83 & 0.91 & 0.94 & 0.73 & 0.72 & 0.74 & 0.76 & 0.77 & 0.83 & 0.84 & 0.80 & 0.82 & 0.87 \\\\ \niRBM & 0.83 & 0.85 & 0.90 & 0.94 & 0.94 & 0.73 & 0.76 & 0.84 & 0.92 & 0.94 & 0.71 & 0.71 & 0.74 & 0.75 & 0.77 & 0.68 & 0.72 & 0.79 & 0.83 & 0.87 \\\\ \nlav & 0.80 & 0.83 & 0.89 & 0.93 & 0.94 & 0.77 & 0.80 & 0.88 & 0.93 & 0.94 & 0.67 & 0.69 & 0.73 & 0.75 & 0.77 & 0.72 & 0.76 & 0.82 & 0.84 & 0.87 \\\\ \nOzenne et al. & 0.84 & 0.86 & 0.90 & 0.94 & 0.94 & 0.80 & 0.82 & 0.89 & 0.93 & 0.94 & 0.70 & 0.72 & 0.75 & 0.76 & 0.77 & 0.75 & 0.78 & 0.83 & 0.85 & 0.87 \\\\ \nJackknife & 0.82 & 0.85 & 0.90 & 0.93 & 0.94 & 0.68 & 0.73 & 0.87 & 0.94 & 0.94 & 0.72 & 0.74 & 0.82 & 0.88 & 0.93 & 0.65 & 0.69 & 0.84 & 0.89 & 0.94 \\\\ \nBootstrap & 0.72 & 0.76 & 0.85 & 0.91 & 0.94 & 0.66 & 0.70 & 0.85 & 0.93 & 0.94 & 0.59 & 0.63 & 0.74 & 0.82 & 0.93 & 0.59 & 0.64 & 0.80 & 0.88 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\beta\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.91 & 0.91 & 0.93 & 0.95 & 0.95 & 0.88 & 0.91 & 0.93 & 0.94 & 0.95 & 0.91 & 0.92 & 0.95 & 0.95 & 0.96 & 0.89 & 0.91 & 0.93 & 0.94 & 0.95 \\\\ \neRBM & 0.92 & 0.91 & 0.94 & 0.96 & 0.95 & 0.91 & 0.93 & 0.94 & 0.94 & 0.95 & 0.91 & 0.92 & 0.95 & 0.95 & 0.96 & 0.88 & 0.92 & 0.94 & 0.94 & 0.95 \\\\ \niRBM & 0.92 & 0.91 & 0.94 & 0.96 & 0.95 & 0.81 & 0.84 & 0.92 & 0.94 & 0.95 & 0.89 & 0.91 & 0.95 & 0.95 & 0.96 & 0.79 & 0.80 & 0.89 & 0.94 & 0.95 \\\\ \nlav & 0.90 & 0.89 & 0.93 & 0.95 & 0.95 & 0.88 & 0.89 & 0.92 & 0.94 & 0.95 & 0.90 & 0.91 & 0.94 & 0.95 & 0.96 & 0.89 & 0.89 & 0.92 & 0.94 & 0.95 \\\\ \nOzenne et al. & 0.91 & 0.90 & 0.93 & 0.95 & 0.95 & 0.89 & 0.90 & 0.93 & 0.94 & 0.95 & 0.91 & 0.92 & 0.95 & 0.95 & 0.96 & 0.90 & 0.90 & 0.92 & 0.94 & 0.95 \\\\ \nJackknife & 0.93 & 0.93 & 0.95 & 0.95 & 0.95 & 0.86 & 0.88 & 0.93 & 0.95 & 0.95 & 0.92 & 0.93 & 0.94 & 0.94 & 0.95 & 0.82 & 0.85 & 0.92 & 0.95 & 0.96 \\\\ \nBootstrap & 0.96 & 0.95 & 0.94 & 0.96 & 0.95 & 0.95 & 0.96 & 0.97 & 0.97 & 0.95 & 0.96 & 0.97 & 0.96 & 0.95 & 0.95 & 0.94 & 0.96 & 0.97 & 0.97 & 0.95 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\lambda_{2,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.93 & 0.92 & 0.95 & 0.95 & 0.95 & 0.90 & 0.91 & 0.94 & 0.95 & 0.95 & 0.90 & 0.93 & 0.94 & 0.94 & 0.95 & 0.86 & 0.90 & 0.93 & 0.95 & 0.95 \\\\ \neRBM & 0.93 & 0.92 & 0.95 & 0.95 & 0.95 & 0.92 & 0.89 & 0.93 & 0.94 & 0.95 & 0.91 & 0.92 & 0.94 & 0.95 & 0.95 & 0.87 & 0.91 & 0.91 & 0.93 & 0.95 \\\\ \niRBM & 0.93 & 0.92 & 0.95 & 0.95 & 0.95 & 0.86 & 0.87 & 0.93 & 0.94 & 0.95 & 0.90 & 0.91 & 0.94 & 0.95 & 0.95 & 0.83 & 0.85 & 0.91 & 0.93 & 0.95 \\\\ \nlav & 0.92 & 0.92 & 0.95 & 0.94 & 0.95 & 0.89 & 0.90 & 0.93 & 0.95 & 0.95 & 0.90 & 0.92 & 0.94 & 0.94 & 0.95 & 0.85 & 0.89 & 0.92 & 0.94 & 0.95 \\\\ \nOzenne et al. & 0.93 & 0.93 & 0.95 & 0.95 & 0.95 & 0.90 & 0.90 & 0.94 & 0.95 & 0.95 & 0.91 & 0.93 & 0.95 & 0.94 & 0.95 & 0.86 & 0.90 & 0.92 & 0.95 & 0.95 \\\\ \nJackknife & 0.91 & 0.92 & 0.94 & 0.94 & 0.95 & 0.78 & 0.82 & 0.92 & 0.95 & 0.95 & 0.88 & 0.92 & 0.94 & 0.95 & 0.95 & 0.77 & 0.81 & 0.90 & 0.95 & 0.95 \\\\ \nBootstrap & 0.94 & 0.92 & 0.95 & 0.94 & 0.95 & 0.89 & 0.89 & 0.93 & 0.95 & 0.95 & 0.93 & 0.94 & 0.94 & 0.95 & 0.95 & 0.87 & 0.89 & 0.92 & 0.95 & 0.95 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n::: {#tbl-bias-growth-80 .cell .column-page tbl-cap='Results (trimmed) growth curve model reliability 0.80.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_growth_80_df, \"growth\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \neRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \niRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.00 & 0.00 & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nJackknife & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nBootstrap & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & -0.01 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.05 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nOzenne et al. & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nREML & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.13 & -0.09 & -0.04 & -0.02 & 0.00 & -0.26 & -0.16 & -0.06 & -0.03 & 0.00 & -0.16 & -0.12 & -0.05 & -0.02 & 0.00 & -0.32 & -0.22 & -0.09 & -0.04 & 0.00 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.53 & 0.50 & 0.35 & 0.26 & 0.09 \\\\ \neRBM & -0.07 & -0.03 & -0.01 & -0.01 & 0.00 & -0.23 & -0.12 & -0.03 & -0.01 & 0.00 & -0.10 & -0.06 & -0.02 & -0.01 & 0.00 & -0.28 & -0.18 & -0.07 & -0.03 & 0.00 & 0.41 & 0.37 & 0.23 & 0.17 & 0.05 & 0.50 & 0.49 & 0.35 & 0.26 & 0.09 \\\\ \niRBM & -0.07 & -0.03 & -0.01 & -0.01 & 0.00 & -0.22 & -0.11 & -0.03 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.26 & -0.16 & -0.07 & -0.03 & 0.00 & 0.41 & 0.37 & 0.23 & 0.17 & 0.05 & 0.50 & 0.49 & 0.35 & 0.26 & 0.09 \\\\ \nJackknife & -0.06 & -0.02 & -0.01 & -0.01 & 0.00 & -0.27 & -0.18 & -0.06 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.31 & -0.23 & -0.08 & -0.03 & 0.00 & 0.42 & 0.37 & 0.23 & 0.17 & 0.05 & 0.50 & 0.46 & 0.33 & 0.26 & 0.09 \\\\ \nBootstrap & -0.06 & -0.02 & -0.01 & -0.01 & 0.00 & -0.23 & -0.17 & -0.05 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.29 & -0.21 & -0.08 & -0.03 & 0.00 & 0.43 & 0.37 & 0.23 & 0.17 & 0.05 & 0.52 & 0.47 & 0.34 & 0.26 & 0.09 \\\\ \nOzenne et al. & -0.06 & -0.02 & -0.01 & -0.01 & 0.00 & -0.21 & -0.11 & -0.03 & -0.01 & 0.00 & -0.09 & -0.06 & -0.02 & -0.01 & 0.00 & -0.27 & -0.18 & -0.07 & -0.03 & 0.00 & 0.41 & 0.37 & 0.23 & 0.17 & 0.05 & 0.51 & 0.49 & 0.35 & 0.26 & 0.09 \\\\ \nREML & -0.03 & -0.02 & -0.01 & -0.01 & 0.00 & -0.08 & -0.06 & -0.03 & -0.01 & 0.00 & -0.07 & -0.06 & -0.02 & -0.01 & 0.00 & -0.19 & -0.13 & -0.07 & -0.03 & 0.00 & 0.44 & 0.38 & 0.23 & 0.17 & 0.05 & 0.61 & 0.54 & 0.35 & 0.26 & 0.09 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.09 & -0.07 & -0.03 & -0.01 & 0.00 & -0.16 & -0.13 & -0.05 & -0.04 & 0.00 & -0.11 & -0.09 & -0.04 & -0.01 & 0.00 & -0.26 & -0.21 & -0.09 & -0.07 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.52 & 0.47 & 0.33 & 0.24 & 0.08 \\\\ \neRBM & -0.02 & -0.02 & -0.01 & 0.00 & 0.00 & -0.10 & -0.09 & -0.03 & -0.03 & 0.00 & -0.05 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \niRBM & -0.04 & -0.03 & -0.01 & 0.00 & 0.00 & -0.12 & -0.10 & -0.03 & -0.03 & 0.00 & -0.06 & -0.05 & -0.02 & 0.00 & 0.00 & -0.21 & -0.17 & -0.07 & -0.06 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.52 & 0.47 & 0.33 & 0.24 & 0.08 \\\\ \nJackknife & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.10 & -0.08 & -0.03 & -0.03 & 0.00 & -0.04 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.35 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \nBootstrap & -0.03 & -0.02 & -0.01 & 0.00 & 0.00 & -0.11 & -0.08 & -0.03 & -0.03 & 0.00 & -0.06 & -0.04 & -0.02 & 0.00 & 0.00 & -0.22 & -0.17 & -0.08 & -0.05 & 0.00 & 0.34 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \nOzenne et al. & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.09 & -0.08 & -0.03 & -0.03 & 0.00 & -0.04 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.35 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \nREML & -0.02 & -0.01 & -0.01 & 0.00 & 0.00 & -0.09 & -0.08 & -0.03 & -0.03 & 0.00 & -0.04 & -0.04 & -0.02 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.35 & 0.30 & 0.19 & 0.13 & 0.04 & 0.53 & 0.47 & 0.34 & 0.24 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.01 & 0.00 & -0.03 & 0.00 & 0.00 & -0.07 & -0.06 & -0.04 & -0.02 & 0.00 & -0.03 & 0.01 & -0.03 & 0.00 & 0.00 & -0.15 & -0.13 & -0.06 & -0.05 & 0.00 & 1.50 & 1.35 & 0.87 & 0.62 & 0.20 & 1.41 & 1.27 & 0.87 & 0.63 & 0.20 \\\\ \neRBM & 0.01 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.01 & 0.03 & -0.02 & 0.00 & 0.00 & -0.14 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \niRBM & 0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.04 & -0.02 & -0.03 & -0.02 & 0.00 & 0.00 & 0.02 & -0.02 & 0.00 & 0.00 & -0.09 & -0.07 & -0.05 & -0.04 & 0.00 & 1.57 & 1.39 & 0.88 & 0.63 & 0.20 & 1.47 & 1.31 & 0.88 & 0.64 & 0.20 \\\\ \nJackknife & 0.01 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.14 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \nBootstrap & 0.00 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.03 & 0.00 & 0.00 & -0.14 & -0.12 & -0.06 & -0.04 & 0.00 & 1.58 & 1.42 & 0.89 & 0.63 & 0.20 & 1.47 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \nOzenne et al. & 0.01 & 0.01 & -0.02 & 0.00 & 0.00 & -0.06 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.14 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \nREML & 0.00 & 0.01 & -0.02 & 0.00 & 0.00 & -0.07 & -0.05 & -0.03 & -0.02 & 0.00 & -0.02 & 0.03 & -0.02 & 0.00 & 0.00 & -0.15 & -0.12 & -0.05 & -0.04 & 0.00 & 1.60 & 1.42 & 0.89 & 0.63 & 0.20 & 1.50 & 1.34 & 0.89 & 0.64 & 0.20 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n::: {#tbl-bias-growth-50 .cell .column-page tbl-cap='Results (trimmed) growth curve model reliability 0.50.'}\n\n```{.r .cell-code .hidden}\ntab_bias(bias_growth_50_df, \"growth\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{7.5pt}{9.0pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Relative mean bias} & \\multicolumn{10}{c}{Relative median bias} & \\multicolumn{10}{c}{Relative RMSE} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21} \\cmidrule(lr){22-31}\n & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} & \\multicolumn{5}{c}{Normal data} & \\multicolumn{5}{c}{Non-normal data} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21} \\cmidrule(lr){22-26} \\cmidrule(lr){27-31}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\theta_{1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \neRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \niRBM & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & 0.00 & 0.00 & 0.00 & 0.00 & -0.02 & -0.02 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nJackknife & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.03 & -0.02 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.04 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.19 & 0.17 & 0.12 & 0.08 & 0.03 \\\\ \nBootstrap & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.02 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & -0.01 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.05 & 0.01 & 0.19 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nOzenne et al. & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.07 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \nREML & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.02 & -0.01 & 0.00 & 0.00 & 0.00 & -0.01 & -0.01 & 0.00 & 0.00 & 0.00 & -0.03 & -0.03 & 0.00 & 0.00 & 0.00 & 0.11 & 0.10 & 0.06 & 0.04 & 0.01 & 0.20 & 0.18 & 0.12 & 0.08 & 0.03 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.27 & -0.17 & -0.07 & -0.03 & -0.01 & -0.35 & -0.22 & -0.08 & -0.04 & -0.01 & -0.32 & -0.23 & -0.10 & -0.03 & -0.01 & -0.45 & -0.30 & -0.12 & -0.06 & -0.01 & 0.85 & 0.74 & 0.48 & 0.34 & 0.11 & 0.93 & 0.87 & 0.57 & 0.41 & 0.13 \\\\ \neRBM & -0.16 & -0.05 & -0.02 & 0.00 & 0.00 & -0.27 & -0.13 & -0.03 & -0.01 & 0.00 & -0.19 & -0.11 & -0.05 & -0.01 & 0.00 & -0.34 & -0.21 & -0.07 & -0.03 & -0.01 & 0.83 & 0.74 & 0.49 & 0.34 & 0.11 & 0.90 & 0.85 & 0.58 & 0.41 & 0.13 \\\\ \niRBM & -0.17 & -0.07 & -0.03 & 0.00 & 0.00 & -0.24 & -0.12 & -0.03 & -0.02 & 0.00 & -0.20 & -0.13 & -0.05 & -0.01 & 0.00 & -0.32 & -0.19 & -0.07 & -0.03 & -0.01 & 0.82 & 0.74 & 0.49 & 0.34 & 0.11 & 0.89 & 0.84 & 0.57 & 0.41 & 0.13 \\\\ \nJackknife & -0.12 & -0.05 & -0.02 & 0.00 & 0.00 & -0.28 & -0.17 & -0.03 & -0.01 & 0.00 & -0.16 & -0.11 & -0.04 & -0.01 & 0.00 & -0.36 & -0.24 & -0.07 & -0.03 & -0.01 & 0.86 & 0.74 & 0.49 & 0.34 & 0.11 & 0.90 & 0.82 & 0.57 & 0.41 & 0.13 \\\\ \nBootstrap & -0.13 & -0.05 & -0.02 & 0.00 & 0.00 & -0.24 & -0.15 & -0.03 & -0.01 & 0.00 & -0.20 & -0.10 & -0.05 & 0.00 & 0.00 & -0.35 & -0.22 & -0.08 & -0.03 & -0.01 & 0.88 & 0.75 & 0.49 & 0.34 & 0.11 & 0.93 & 0.84 & 0.57 & 0.41 & 0.13 \\\\ \nOzenne et al. & -0.14 & -0.05 & -0.02 & 0.00 & 0.00 & -0.24 & -0.12 & -0.02 & -0.01 & 0.00 & -0.18 & -0.10 & -0.04 & -0.01 & 0.00 & -0.32 & -0.20 & -0.07 & -0.03 & -0.01 & 0.83 & 0.75 & 0.49 & 0.34 & 0.11 & 0.91 & 0.86 & 0.58 & 0.41 & 0.13 \\\\ \nREML & 0.02 & 0.01 & -0.02 & 0.00 & 0.00 & 0.00 & 0.01 & -0.02 & -0.01 & 0.00 & -0.11 & -0.07 & -0.04 & -0.01 & 0.00 & -0.16 & -0.13 & -0.07 & -0.03 & -0.01 & 0.75 & 0.67 & 0.47 & 0.34 & 0.11 & 0.84 & 0.78 & 0.55 & 0.41 & 0.13 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & -0.11 & -0.08 & -0.04 & -0.01 & 0.00 & -0.17 & -0.14 & -0.05 & -0.04 & 0.00 & -0.14 & -0.11 & -0.05 & -0.02 & 0.00 & -0.27 & -0.22 & -0.09 & -0.07 & -0.01 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.58 & 0.52 & 0.36 & 0.26 & 0.08 \\\\ \neRBM & -0.03 & -0.02 & -0.02 & 0.00 & 0.00 & -0.10 & -0.09 & -0.02 & -0.03 & 0.00 & -0.06 & -0.05 & -0.03 & 0.00 & 0.00 & -0.21 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \niRBM & -0.05 & -0.03 & -0.02 & 0.00 & 0.00 & -0.11 & -0.09 & -0.03 & -0.03 & 0.00 & -0.07 & -0.06 & -0.03 & 0.00 & 0.00 & -0.21 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.52 & 0.37 & 0.26 & 0.08 \\\\ \nJackknife & -0.03 & -0.02 & -0.02 & 0.00 & 0.00 & -0.09 & -0.08 & -0.02 & -0.03 & 0.00 & -0.05 & -0.05 & -0.03 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \nBootstrap & -0.05 & -0.02 & -0.02 & 0.00 & 0.00 & -0.11 & -0.08 & -0.02 & -0.03 & 0.00 & -0.07 & -0.05 & -0.03 & 0.00 & 0.00 & -0.22 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \nOzenne et al. & -0.03 & -0.02 & -0.02 & 0.00 & 0.00 & -0.09 & -0.08 & -0.02 & -0.03 & 0.00 & -0.05 & -0.05 & -0.03 & 0.00 & 0.00 & -0.20 & -0.17 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.24 & 0.16 & 0.05 & 0.59 & 0.53 & 0.37 & 0.26 & 0.08 \\\\ \nREML & -0.01 & -0.01 & -0.01 & 0.00 & 0.00 & -0.07 & -0.06 & -0.02 & -0.03 & 0.00 & -0.04 & -0.04 & -0.03 & 0.00 & 0.00 & -0.18 & -0.15 & -0.07 & -0.06 & 0.00 & 0.42 & 0.37 & 0.23 & 0.16 & 0.05 & 0.57 & 0.52 & 0.36 & 0.26 & 0.08 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{31}{l}{\\(\\mathbf\\Psi_{1,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.30 & 0.19 & 0.05 & 0.02 & 0.01 & 0.24 & 0.17 & 0.03 & 0.04 & 0.01 & 0.37 & 0.30 & 0.07 & 0.03 & 0.02 & 0.26 & 0.18 & 0.07 & 0.06 & 0.00 & 2.49 & 2.20 & 1.44 & 1.03 & 0.32 & 2.47 & 2.25 & 1.49 & 1.05 & 0.34 \\\\ \neRBM & 0.15 & 0.07 & 0.00 & 0.00 & 0.00 & 0.08 & 0.05 & -0.02 & 0.02 & 0.01 & 0.23 & 0.19 & 0.02 & 0.01 & 0.02 & 0.12 & 0.07 & 0.02 & 0.04 & 0.00 & 2.63 & 2.30 & 1.47 & 1.04 & 0.32 & 2.61 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \niRBM & 0.17 & 0.09 & 0.01 & 0.00 & 0.00 & 0.09 & 0.05 & -0.02 & 0.02 & 0.01 & 0.25 & 0.21 & 0.02 & 0.01 & 0.02 & 0.10 & 0.06 & 0.01 & 0.04 & 0.00 & 2.61 & 2.28 & 1.47 & 1.04 & 0.32 & 2.59 & 2.33 & 1.52 & 1.06 & 0.34 \\\\ \nJackknife & 0.14 & 0.07 & 0.00 & -0.01 & 0.00 & 0.07 & 0.05 & -0.02 & 0.02 & 0.01 & 0.22 & 0.18 & 0.02 & 0.01 & 0.02 & 0.12 & 0.07 & 0.01 & 0.04 & 0.00 & 2.64 & 2.30 & 1.47 & 1.04 & 0.32 & 2.63 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \nBootstrap & 0.19 & 0.07 & 0.00 & 0.00 & 0.00 & 0.12 & 0.06 & -0.02 & 0.02 & 0.01 & 0.29 & 0.21 & 0.02 & 0.01 & 0.01 & 0.15 & 0.09 & 0.02 & 0.03 & 0.00 & 2.60 & 2.30 & 1.47 & 1.04 & 0.32 & 2.58 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \nOzenne et al. & 0.14 & 0.07 & 0.00 & -0.01 & 0.00 & 0.07 & 0.05 & -0.02 & 0.02 & 0.01 & 0.22 & 0.18 & 0.02 & 0.01 & 0.02 & 0.12 & 0.07 & 0.01 & 0.04 & 0.00 & 2.64 & 2.30 & 1.47 & 1.04 & 0.32 & 2.63 & 2.35 & 1.52 & 1.06 & 0.34 \\\\ \nREML & -0.13 & -0.08 & -0.03 & -0.01 & 0.00 & -0.29 & -0.21 & -0.08 & 0.01 & 0.01 & 0.07 & 0.06 & 0.01 & 0.00 & 0.02 & -0.23 & -0.13 & -0.02 & 0.04 & 0.00 & 2.42 & 2.17 & 1.44 & 1.04 & 0.32 & 2.38 & 2.18 & 1.48 & 1.06 & 0.34 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n::: {#tbl-covr-growth .cell .column-page tbl-cap='Coverage rates 95% confidence interval for the growth curve model.'}\n\n```{.r .cell-code .hidden}\ntab_covr(covr_growth_df, \"growth\")\n```\n\n::: {.cell-output-display  html-table-processing=none}\n\\begin{table}\n\\fontsize{12.0pt}{14.4pt}\\selectfont\n\\begin{tabular*}{\\linewidth}{@{\\extracolsep{\\fill}}l|rrrrrrrrrrrrrrrrrrrr}\n\\toprule\n & \\multicolumn{10}{c}{Normal data} & \\multicolumn{10}{c}{Non-Normal data} \\\\ \n\\cmidrule(lr){2-11} \\cmidrule(lr){12-21}\n & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} & \\multicolumn{5}{c}{Reliability = 0.8} & \\multicolumn{5}{c}{Reliability = 0.5} \\\\ \n\\cmidrule(lr){2-6} \\cmidrule(lr){7-11} \\cmidrule(lr){12-16} \\cmidrule(lr){17-21}\n & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 & 15 & 20 & 50 & 100 & 1000 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\theta_{1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 \\\\ \neRBM & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 \\\\ \niRBM & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.71 & 0.72 & 0.71 & 0.73 & 0.72 & 0.70 & 0.72 & 0.71 \\\\ \nlav & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.94 & 0.94 & 0.95 & 0.94 & 0.96 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 & 0.72 & 0.71 & 0.70 & 0.72 & 0.71 \\\\ \nOzenne et al. & 0.94 & 0.95 & 0.95 & 0.94 & 0.96 & 0.94 & 0.95 & 0.95 & 0.94 & 0.96 & 0.73 & 0.72 & 0.71 & 0.72 & 0.71 & 0.73 & 0.72 & 0.71 & 0.72 & 0.71 \\\\ \nJackknife & 0.92 & 0.92 & 0.94 & 0.94 & 0.96 & 0.92 & 0.92 & 0.94 & 0.94 & 0.96 & 0.88 & 0.90 & 0.92 & 0.94 & 0.95 & 0.88 & 0.90 & 0.92 & 0.94 & 0.95 \\\\ \nBootstrap & 0.88 & 0.91 & 0.94 & 0.94 & 0.96 & 0.88 & 0.91 & 0.94 & 0.94 & 0.96 & 0.84 & 0.89 & 0.91 & 0.94 & 0.95 & 0.83 & 0.89 & 0.91 & 0.94 & 0.95 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{1,1}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.84 & 0.87 & 0.92 & 0.93 & 0.95 & 0.84 & 0.88 & 0.91 & 0.93 & 0.95 & 0.68 & 0.73 & 0.76 & 0.76 & 0.78 & 0.80 & 0.82 & 0.87 & 0.89 & 0.87 \\\\ \neRBM & 0.88 & 0.90 & 0.93 & 0.94 & 0.95 & 0.88 & 0.92 & 0.93 & 0.94 & 0.95 & 0.73 & 0.77 & 0.77 & 0.77 & 0.78 & 0.84 & 0.86 & 0.88 & 0.90 & 0.87 \\\\ \niRBM & 0.88 & 0.90 & 0.93 & 0.94 & 0.95 & 0.88 & 0.92 & 0.93 & 0.94 & 0.95 & 0.74 & 0.78 & 0.78 & 0.77 & 0.78 & 0.85 & 0.87 & 0.88 & 0.90 & 0.87 \\\\ \nlav & 0.84 & 0.87 & 0.92 & 0.93 & 0.95 & 0.84 & 0.88 & 0.91 & 0.93 & 0.95 & 0.68 & 0.73 & 0.76 & 0.76 & 0.78 & 0.80 & 0.82 & 0.87 & 0.89 & 0.87 \\\\ \nOzenne et al. & 0.88 & 0.90 & 0.93 & 0.94 & 0.95 & 0.88 & 0.92 & 0.93 & 0.94 & 0.95 & 0.73 & 0.77 & 0.77 & 0.77 & 0.78 & 0.83 & 0.86 & 0.88 & 0.89 & 0.87 \\\\ \nJackknife & 0.83 & 0.88 & 0.92 & 0.93 & 0.95 & 0.84 & 0.89 & 0.91 & 0.93 & 0.95 & 0.69 & 0.73 & 0.83 & 0.88 & 0.94 & 0.80 & 0.83 & 0.90 & 0.92 & 0.94 \\\\ \nBootstrap & 0.76 & 0.85 & 0.91 & 0.92 & 0.94 & 0.77 & 0.86 & 0.90 & 0.93 & 0.94 & 0.63 & 0.71 & 0.82 & 0.87 & 0.94 & 0.75 & 0.81 & 0.89 & 0.92 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{2,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.85 & 0.88 & 0.91 & 0.94 & 0.95 & 0.84 & 0.87 & 0.91 & 0.94 & 0.95 & 0.66 & 0.66 & 0.69 & 0.68 & 0.70 & 0.71 & 0.73 & 0.76 & 0.76 & 0.77 \\\\ \neRBM & 0.90 & 0.91 & 0.92 & 0.94 & 0.95 & 0.89 & 0.90 & 0.92 & 0.95 & 0.95 & 0.71 & 0.71 & 0.71 & 0.69 & 0.70 & 0.76 & 0.78 & 0.78 & 0.77 & 0.77 \\\\ \niRBM & 0.89 & 0.91 & 0.92 & 0.94 & 0.95 & 0.88 & 0.90 & 0.92 & 0.95 & 0.95 & 0.70 & 0.70 & 0.71 & 0.69 & 0.70 & 0.76 & 0.78 & 0.78 & 0.77 & 0.77 \\\\ \nlav & 0.85 & 0.88 & 0.91 & 0.94 & 0.95 & 0.84 & 0.87 & 0.91 & 0.94 & 0.95 & 0.66 & 0.66 & 0.69 & 0.68 & 0.70 & 0.71 & 0.73 & 0.76 & 0.76 & 0.77 \\\\ \nOzenne et al. & 0.89 & 0.91 & 0.92 & 0.94 & 0.95 & 0.89 & 0.90 & 0.92 & 0.95 & 0.95 & 0.70 & 0.70 & 0.70 & 0.69 & 0.70 & 0.76 & 0.77 & 0.77 & 0.77 & 0.77 \\\\ \nJackknife & 0.86 & 0.88 & 0.91 & 0.93 & 0.95 & 0.85 & 0.87 & 0.91 & 0.93 & 0.95 & 0.70 & 0.73 & 0.82 & 0.85 & 0.93 & 0.74 & 0.77 & 0.84 & 0.87 & 0.94 \\\\ \nBootstrap & 0.79 & 0.85 & 0.90 & 0.92 & 0.95 & 0.78 & 0.85 & 0.90 & 0.92 & 0.95 & 0.63 & 0.70 & 0.81 & 0.84 & 0.93 & 0.67 & 0.74 & 0.83 & 0.86 & 0.94 \\\\ \n\\midrule\\addlinespace[2.5pt]\n\\multicolumn{21}{l}{\\(\\mathbf\\Psi_{1,2}\\)} \\\\[2.5pt] \n\\midrule\\addlinespace[2.5pt]\nML & 0.98 & 0.97 & 0.95 & 0.95 & 0.96 & 0.94 & 0.95 & 0.94 & 0.94 & 0.95 & 0.97 & 0.96 & 0.94 & 0.94 & 0.94 & 0.93 & 0.93 & 0.94 & 0.94 & 0.94 \\\\ \neRBM & 0.98 & 0.98 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.95 & 0.94 & 0.95 & 0.98 & 0.98 & 0.95 & 0.94 & 0.94 & 0.95 & 0.95 & 0.94 & 0.94 & 0.94 \\\\ \niRBM & 0.98 & 0.98 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.95 & 0.94 & 0.95 & 0.98 & 0.98 & 0.95 & 0.94 & 0.94 & 0.95 & 0.95 & 0.94 & 0.94 & 0.94 \\\\ \nlav & 0.98 & 0.97 & 0.95 & 0.95 & 0.96 & 0.94 & 0.95 & 0.94 & 0.94 & 0.95 & 0.97 & 0.96 & 0.94 & 0.94 & 0.94 & 0.93 & 0.93 & 0.94 & 0.94 & 0.94 \\\\ \nOzenne et al. & 0.98 & 0.97 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.95 & 0.94 & 0.95 & 0.98 & 0.97 & 0.95 & 0.94 & 0.94 & 0.94 & 0.94 & 0.94 & 0.94 & 0.94 \\\\ \nJackknife & 0.93 & 0.93 & 0.94 & 0.94 & 0.95 & 0.91 & 0.92 & 0.94 & 0.93 & 0.95 & 0.93 & 0.93 & 0.94 & 0.94 & 0.95 & 0.92 & 0.93 & 0.95 & 0.94 & 0.95 \\\\ \nBootstrap & 0.86 & 0.91 & 0.93 & 0.94 & 0.95 & 0.83 & 0.90 & 0.92 & 0.93 & 0.95 & 0.85 & 0.90 & 0.93 & 0.93 & 0.95 & 0.84 & 0.90 & 0.93 & 0.94 & 0.95 \\\\ \n\\bottomrule\n\\end{tabular*}\n\\end{table}\n\n:::\n:::\n\n\n\n\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{caption}\n\\usepackage{longtable}\n\\usepackage{colortbl}\n\\usepackage{array}\n\\usepackage{anyfontsize}\n\\usepackage{multirow}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}