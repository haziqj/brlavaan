### Structural Equation Models

A Structural equation model has two parts, a measurement part and a structural part. For observation $i$, the measurement part of the model is defined as 

$$
y_i = \nu + \Lambda \eta_i + \epsilon_i,
$${#eq-measurement}

where $y_i$ is a $m \times 1$ vector of observed variables, $\nu$ is a $m \times 1$ vector of measurement intercepts, $\gamma$ is a $m \times k$ matrix of factor loadings, $\eta_i$ is a $k \times 1$ vector of latent variables, and $\epsilon_i$ is a $m \times 1$ vector of measurement errors, assumed to be normally distributed with mean zero and covariance matrix $\Theta$.

The structural part of the model is defined as

$$
\eta_i =\alpha + B \eta_i + \zeta_i,
$$ {#eq-structural}

where $\alpha$ is a $k \times 1$ vector of factor means, $B$ is a $k \times k$ matrix of regression coefficients for which $\text{diag}(B) = 0$ and $I-B$ is invertible, and $\zeta_i$ is a $k \times 1$ vector of structural disturbances, assumed to be normally distributed with mean zero and covariance matrix $\Psi$. We also assume that $\text{cov}(\eta_i, \epsilon_i) = \text{cov}(\epsilon_i, \zeta_i) = 0$.

<!-- $$ -->
<!-- \epsilon \sim N(0, \Theta_\epsilon)\\ -->
<!-- \zeta \sim N(0, \Psi)\\ -->
<!-- $$ {#eq-sem} -->

### Estimation methods

#### Maximum likelihood estimation

Parameter estimation in a SEM is typically performed using maximum likelihood estimation, where given some observed data, we estimate parameters as those which make the observations most likely under the assumed model. The specific parameters to estimate depend on the model, since which parameters from @eq-measurement and @eq-structural are present in the model and free depend on the specific SEM. In any case, as described in @dhaene2022resampling, we can combine all free parameters to be estimated in to a single $p \times 1$ vector $\theta$, and define the model mean vector and covariance matrix as

$$
\mu(\theta) = \nu + \Lambda (I - B)^{-1} \alpha \\
\Sigma(\theta) = \Lambda (I - B)^{-1} \Psi (I-B)^{-T} \Lambda^T + \Theta,
$$
where for an invertible matrix $A$, $A^{-T} = (A^{-1})^T$.


Then given independent and identically distributed data $Y = \{y_1, \dots, y_n \}$ sampled from $N(\mu(\theta), \Sigma(\theta))$, parameter estimation is achieved by maximising the multivariate normal log likelihood defined as

$$
l(\theta|Y) = \frac{-nm}{2}\log(2\pi) -\frac{n}{2}\log|\Sigma(\theta)| - \frac{1}{2}\sum_{i=1}^n(y_i - \mu(\theta))^T \Sigma(\theta)^{-1}(y_i - \mu(\theta)).
$${#eq-sem_lik}


Maximising @eq-sem_lik with respect to $\theta$ gives the maximum likelihood estimator of $\theta$, which we denote by $\hat{\theta}$. Equivalently, we can solve the equations $\nabla l(\theta) = 0_p$, where here $\nabla$ denotes gradient with respect to $\theta$, $0_p$ is a $p-$vector of zeroes.

#### Reduced-bias estimation

Within statistical research, it is often the case that an estimator of an unknown parameter $\theta \in \mathbb{R}^p$ is biased, that is the expected value of the estimator is not equal to the parameter. The maximum likelihood estimator, as used for SEMs, is an example of a typically biased estimator which under regularity conditions from @cox1974theoretical, has bias with asymptotic order $O(n^{-1})$. Therefore, $\hat{\theta}$ is asymptotically unbiased but for small samples the bias may be substantial.

As reviewed in, for example, @kosmidis2014bias, the aim of bias reduction methods is to produce a new estimator $\theta^*$ of $\theta$ that approximates the solution of

$$
\hat{\theta} - \theta^* = B(\bar{\theta})
$$ {#eq-bias_eq}

with respect to $\theta^*$. In @eq-bias_eq, $B(\bar{\theta}) = \mathbb{E}(\hat{\theta} - \bar{\theta})$ is the bias function, and $\bar{\theta}$ is the value that $\hat{\theta}$ converges to, or is assumed to converge to in probability, as sample size increases. However, the unbiased estimator $\theta^*$ generally can not be computed exactly since $B(\cdot)$ is typically not available in closed form and $\bar{\theta}$ is not known. Therefore, we may not be able to produce a new estimator that is unbiased, but we can approximate the solution of @eq-bias_eq to produce an estimator with asymptotically smaller bias than the original $\hat{\theta}$.

Bias reduction methods operate explicitly or implicitly, we denote the explicit estimator as $\theta^{ex}$ and the implicit estimator as $\tilde{\theta}$. In explicit methods, we aim to approximate the bias of an estimator, then subtract this bias from the original estimator to produce a reduced-bias estimator. In particular, such methods replace $B(\theta)$ with $B^*$, which is computed from the available data, before computing $\theta^{ex} = \hat{\theta} - B^*$. Implicit methods compute a reduced-biased estimator by replacing $B(\theta)$ with $\hat{B}(\tilde{\theta})$, which is an estimator of the whole bias function at the desired estimator. Then we solve the implicit equation $\hat{\theta} - \tilde{\theta} = \hat{B}(\tilde{\theta})$ for $\tilde{\theta}$. Examples of explicit bias reduction methods include the bootstrap (@efron1994bootstrap; @hall1988bootstrap) and jackknife (@quenouille1956jackknife; @efron1982jackknife), which are both examined in this paper, while a notable example of an implicit method is the bias-reducing adjusted score equations of @firth1993bias, from which reduced-bias M-estimation was developed.

The main bias reduction technique to be examined in this paper is Reduced-bias M-estimation (RBM-estimation; @kosmidis2024empirical), which can operate implicitly or explicitly. This method requires only the SEM likelihood and its first three derivatives, so can be readily implemented, either in closed form, or numerically using packages such as \texttt{numDeriv} (@gilbert2022numderiv). Since the bias is approximated analytically, RBM-estimation is likely to be noticeably faster than simulation based methods such as the bootstrap and jackknife.


When estimation is by maximising a log-likelihood, as is the case for SEMs, implicit RBM-estimation works by adding an appropriate penalty to the log-likelihood, so that rather than maximising $l(\theta)$, we instead maximise

$$
l(\theta) + P(\theta)
$$

for an appropriate penalty term $P(\theta)$. The penalty term is chosen such that the resulting estimator, $\tilde{\theta}$, has asymptotically reduced bias compared to $\hat{\theta}$, while still retaining the same asymptotic properties. This concept started in @firth1993bias for regular maximum likelihood estimators, by adding a bias-reducing adjustment term to the score equations, the derivative of the log likelihood. When estimating the canonical parameter of an exponential family model, @jeffreys1946prior invariant prior can be added as a penalty term to the log-likelihood to achieve the same result.

@kosmidis2024empirical generalised this adjustment to generic M-estimators, which also has the significant advantage of giving rise to a penalised likelihood for any M-estimator estimated by maximising an objective function, rather than only when estimating canonical parameters in an exponential family model.

The penalty term $P(\theta)$ is defined as

$$
P(\theta) = -\frac{1}{2}\text{trace}\{j(\theta)^{-1} e(\theta)\},
$$
where $j(\theta)$ is the negative Hessian matrix $-\nabla \nabla^T l(\theta)$ and $e(\theta)$ is the first order information, a matrix with elements computed as 


$$[e(\theta)]_{st} = \sum_{i=1}^n \left\{ \frac{\partial l_i(\theta)}{\partial \theta_t}  \frac{\partial l_i(\theta)}{\partial \theta_s} \right\},$$

where $l_i(\theta)$ is the $i$-th contribution to the likelihood, such that $l(\theta) = \sum_{i=1}^n l_i(\theta)$.


We define the explicit version of RBM-estimation by obtaining an explicit expression for the bias and subtracting this from the maximum likelihood estimator. The explicit estimator is

$$
\theta^{ex} = \hat{\theta} + j(\hat{\theta})^{-1} A(\hat{\theta}),
$$
where $A(\theta) = \nabla P(\theta)$.


There are advantages and disadvantages to the implicit and explicit approaches. Computing $\theta^{ex}$ is a single step procedure so is likely to be faster to compute then $\Tilde{\theta}$. Indeed, computing $\theta^{ex}$ can be seen as performing one step of the optimisation routine used to compute $\Tilde{\theta}$. However, the explicit approach contains no safeguards on what value the final estimator can take, so may result in an estimate outside the parameter space. The implicit approach allows such constraints to be incorporated into the optimisation procedure, and may be more stable as it does not require $\hat{\theta}$, so is useful in cases where obtaining $\hat{\theta}$ is unstable. The implicit approach also allows extra plug in penalties to be added to achieve extra desired properties such as finiteness of estimators. Such penalties are discussed in @kosmidis2024empirical, and for SEMs specifically, are the subject of future work with the aim of increasing estimation stability.
