Structural Equation Modeling (SEM) is a widely used statistical framework that
enables researchers to specify, estimate, and test complex relationships among
observed and latent variables. Despite its versatility and theoretical appeal,
the practical application of SEM in empirical research is often challenged by
limitations in sample size. Small samples are particularly common in fields
such as clinical psychology, developmental research, and neuroscience, where
data collection is expensive, time-consuming, or logistically constrained
(van de Schoot & Miocević, 2020).

However, small samples do not always arise from poor study design or logistical
constraints; in many cases, they are an inevitable consequence of studying rare
populations. For example, research involving children with burned facial
injuries is inherently limited by the small number of individuals who meet
these specific criteria. Similarly, studies focusing on rare genetic disorders,
elite athlete populations, or unique cultural groups often face strict
limitations in sample size due to the rarity of the target population. In such
contexts, the use of SEM remains desirable for testing complex theoretical
models, but the challenges posed by small samples become unavoidable and must
be addressed through methodological refinement.

When the sample size is small, SEM faces a range of well-documented
difficulties (Deng, Yang & Marcoulides, 2018; Bentler & Yuan,
1999; Nevitt & Hancock, 2004). These include non-convergence of estimation
algorithms, improper solutions such as negative variance estimates (Heywood
cases), inflated standard errors, and low statistical power. Non-convergence,
in particular, can be a very frustrating experience for applied researchers,
often leading them to consider more simple and often suboptimal (non-SEM)
procedures (De Jonckere & Rosseel, 2022).

In response to these challenges, researchers have developed a variety of
techniques to improve estimation performance in small samples. These include
alternative estimators such as Bayesian methods (Muthén & Asparouhov, 2012;
Song & Lee, 2012), penalized likelihood approaches (Huang et al., 2017;
Jacobucci et al., 2016), as well as model simplification strategies (Rosseel &
Loh, 2024). Such developments have significantly improved the feasibility and
stability of SEM in small-sample contexts. However, even when convergence is
achieved and estimates appear admissible, a subtler problem remains: the
presence of finite-sample bias in parameter estimates. 

Finite-sample bias is a well-recognized issue in statistics, particularly in
the context of maximum likelihood estimation, where estimators often exhibit
systematic deviations from the true parameter values in small samples (Cox and
Hinkley, 1974). To address this, a variety of strategies have been proposed.
Analytical bias corrections based on higher-order asymptotic theory---such as
those introduced by Cox and Snell (1968) and extended by Firth (1993)---modify
the score function or likelihood equations to produce estimators with reduced
bias. Building on this foundation, Kosmidis and colleagues (e.g., Kosmidis &
Firth, 2009; Kosmidis, 2020) have developed generalized bias-reduction
techniques using adjusted score functions, which are applicable across a wide
range of models, including generalized linear and mixed models. These methods
are particularly appealing due to their strong theoretical properties and broad
applicability. In parallel, resampling-based techniques such as bootstrap and
jackknife procedures (Efron & Tibshirani, 1994) offer flexible,
model-agnostic tools for bias correction and have gained popularity in applied
research for their ease of implementation. In Bayesian estimation, prior
distributions can also help shrink estimates toward more plausible values in
small samples, though this introduces sensitivity to prior specification 
(Van Erp, Mulder & Oberski, 2018).

Within the SEM literature, initial attempts to mitigate the finite-sample bias
involved the use of restricted maximum likelihood (REML), an estimation
technique originally developed for variance component models (Corbeil & Searle,
1976; Patterson & Thompson, 1971). And although effective, this approach is
only applicable for a very restricted subclass of SEMs, namely those models
that are mathematically equivalent to mixed effect models (Bauer, 2003;
McNeish, 2016, 2018; Cheung, 2013). In a paper focusing on small sample
corrections for Wald tests in SEMs, Ozenne et al. (2020) describe an analytic
method to correct for the finite-sample bias in SEMs based on technology
originating from the generalized estimating equation (GEE) literature
(Kauermann & Carroll, 2001). Importantly, their method only targets the
(observed or latent) (co)variance parameters in the model. All other parameters
(intercepts, regressions, factor loadings) are unaffected by their method.
Finally, a more general strategy is the resampling-based approach proposed by
Dhaene & Rosseel (2022), which adapts jacknife and bootstrap techniques to SEM
for correcting finite-sample bias in all parameter estimates. This method is
broadly applicable and relatively easy to implement, though it can be
computationally intensive and may still exhibit some limitations in very small
samples. Together, these approaches represent important steps toward
addressing finite-sample bias in SEM, though each carries trade-offs between
generality, complexity, and computational cost.

Recently, Kosmidis & Lunardon (2024) introduced a new framework for
reducing bias in M-estimators (including maximum likelihood estimators)
derived from asymptotically unbiased estimating
functions. This framework offers both implicit and explicit methods for bias
correction. The implicit approach involves solving adjusted estimating
equations, while the explicit method subtracts an estimated bias from the
original M-estimates. Notably, these methods require only the first and second
derivatives of the estimating functions, eliminating the need for complex
expectations or resampling techniques. In addition, these methods maintain the
same asymptotic distribution as the original M-estimators, ensuring that
standard inference and model selection procedures remain applicable. 

Motivated by these appealing properties, the goal of this paper is to
investigate how the reduced-bias M (RBM) estimation framework developed by
Kosmidis and Lunardon (2024) can be extended to the context of structural
equation modeling. In contrast to existing bias-reduction techniques for SEM,
the RBM approach is fully general---applicable to any SEM specification---while
simultaneously targeting all model parameters. It also offers a computationally
efficient alternative to resampling-based methods. Moreover, the RBM method is
relatively straightforward to implement in practice, and we provide
accompanying R code in the supplementary materials to facilitate its
application.

The remainder of the paper is structured as follows. We begin by briefly
outlining the SEM framework and introducing the notation used throughout the
paper. We then present a concise overview of several bias-reduction techniques,
including the method proposed by Ozenne and colleagues, the jackknife and
bootstrap approaches, and the recently introduced RBM method. Following this,
we report the results of a comprehensive simulation study designed to evaluate
the performance of the RBM approach in comparison to existing alternatives.
Finally, we conclude with a discussion of the findings and offer practical
recommendations for applied researchers.

