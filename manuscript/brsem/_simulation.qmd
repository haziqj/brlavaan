---
output: html_document
editor_options: 
  chunk_output_type: console
# execute:
#   freeze: auto
#   cache: true
#   echo: false
# 
# bibliography: 
#   - ../refs-HJ.bib
#   - ../refs-OK.bib
---


<!-- ```{r} -->
<!-- #| label: setup -->
<!-- #| include: false -->
<!-- library(tidyverse) -->
<!-- library(brlavaan) -->
<!-- library(gt) -->
<!-- library(glue) -->
<!-- library(semPlot) -->
<!-- library(semptools) -->
<!-- load("results.RData") -->
<!-- ``` -->

To evaluate the effectiveness of our proposed explicit and implicit bias reduction methods, we conducted an extensive simulation study mirroring the work done by @dhaene2022resampling.
We replicated the models and experimental settings exactly, allowing for a direct comparison with existing resampling-based and other methods for bias correction.
This section describes the models, simulation scenarios, and the outcome of interest of our simulations.

### Models

Two distinct structural equation models (SEM) were considered: a) A two-factor SEM with a latent regression; and b) a latent growth curve model (GCM), 
These models represent commonly used models used by practitioners and researchers alike in the social sciences.

The two-factor SEM contains two latent variables, $\eta_{1}$ and $\eta_{2}$, each indicated by three observed variables: 
$y_{1}$, $y_{2}$, $y_{3}$ for $\eta_{1}$ and $y_{4}$, $y_{5}$, $y_{6}$ for $\eta_{2}$. 
Each observed measure has an associated error term $\epsilon_{j}\sim\operatorname{N}(0,\Theta_{jj})$. 
The latent regression path from $\eta_{1}$ to $\eta_{2}$ is labelled $\beta$. 
Factor loadings are represented by $\Lambda_{ij}$, and the latent variances by $\Psi_{11}$ and $\Psi_{22}$.
Altogether, the model has 13 estimable parameters, as specified in @tbl-truth-twofac.
The corresponding path diagram is illustrated in @fig-path-twofac.

::: {#fig-path-twofac}
```{r pathtwofac, engine = "tikz", out.width = "90%"}
\begin{tikzpicture}
\node[inner sep=3pt] {%
\begin{tikzpicture}[%
  >=stealth,
  auto,
  node distance=1.6cm,
  every node/.style={font=\small},
  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},
  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},
  error/.style={circle, draw, minimum size=6mm, inner sep=0mm}
]

% --------------------------------------------------------
% 1) LATENT VARIABLES
% --------------------------------------------------------
\node[latent] (eta1) at (-1.5, -2) {$\eta_{1}$};
\node[latent] (eta2) at ( 3, -2) {$\eta_{2}$};

% --------------------------------------------------------
% 2) OBSERVED VARIABLES + ERROR TERMS
%    Place y1..y3 (for eta1) on the left, y4..y6 (for eta2) on the right.
% --------------------------------------------------------
\node[obs] (y1) at (-3, 0) {$y_{1}$};
\node[obs] (y2) at (-1.5, 0) {$y_{2}$};
\node[obs] (y3) at ( 0, 0) {$y_{3}$};

\node[obs] (y4) at ( 1.5, 0) {$y_{4}$};
\node[obs] (y5) at ( 3, 0) {$y_{5}$};
\node[obs] (y6) at ( 4.5, 0) {$y_{6}$};

% Error terms above each observed variable
\node[error] (e1) at (-3, 1.5) {$\epsilon_{1}$};
\node[error] (e2) at (-1.5, 1.5) {$\epsilon_{2}$};
\node[error] (e3) at ( 0, 1.5) {$\epsilon_{3}$};
\node[error] (e4) at ( 1.5, 1.5) {$\epsilon_{4}$};
\node[error] (e5) at ( 3, 1.5) {$\epsilon_{5}$};
\node[error] (e6) at ( 4.5, 1.5) {$\epsilon_{6}$};

% --------------------------------------------------------
% 3) PATHS FROM ERROR TERMS TO OBSERVED VARIABLES
% --------------------------------------------------------
\draw[->] (e1) -- (y1);
\draw[->] (e2) -- (y2);
\draw[->] (e3) -- (y3);
\draw[->] (e4) -- (y4);
\draw[->] (e5) -- (y5);
\draw[->] (e6) -- (y6);

% --------------------------------------------------------
% 4) FACTOR LOADINGS
%    \eta_1 -> y1, y2, y3
%    \eta_2 -> y4, y5, y6
% --------------------------------------------------------
\draw[->] (eta1) -- node[pos=0.5, right] {$\Lambda_{11}$} (y1);
\draw[->] (eta1) -- node[pos=0.5, right] {$\Lambda_{21}$} (y2);
\draw[->] (eta1) -- node[pos=0.5, right] {$\Lambda_{31}$} (y3);

\draw[->] (eta2) -- node[pos=0.5, right] {$\Lambda_{42}$} (y4);
\draw[->] (eta2) -- node[pos=0.5, right] {$\Lambda_{52}$} (y5);
\draw[->] (eta2) -- node[pos=0.5, right] {$\Lambda_{62}$} (y6);

% --------------------------------------------------------
% 5) REGRESSION BETWEEN LATENT VARIABLES
%    \eta_1 -> \eta_2, labeled beta
% --------------------------------------------------------
\draw[->] (eta1) -- node[midway, above] {$\beta$} (eta2);

% --------------------------------------------------------
% 6) VARIANCES OF LATENT VARIABLES
%    Double-headed arrows for psi_{11} and psi_{22}
% --------------------------------------------------------
\draw[<->] (eta1) to[out=200, in=230, looseness=4] 
  node[left] {$\Psi_{11}$} (eta1);

\draw[<->] (eta2) to[out=-50, in=-20, looseness=4] 
  node[right] {$\Psi_{22}$} (eta2);

\end{tikzpicture}
};
\end{tikzpicture}
```
The path diagram for the two-factor structural equation model. 
:::

The latent GCM on the other hand characterises longitudinally observed continuous variables measured across ten time points $y_{1},\dots,y_{10}$ using two latent factors labelled $i$ for *intercept*, and $s$ for *slope*.
The intercept factor loadings are fixed to unity, while slope factor loadings increment from 0 to 9. 
Latent variances and covariance are represented by parameters $\Psi_{11}$, $\Psi_{22}$, and $\Psi_{12}$. 
Additionally, latent intercepts for both growth factors are freely estimated, representing average initial status and growth trajectory, respectively. 

This model, illustrated in @fig-path-growth, involves a total of 6 parameters (see @tbl-truth-growth).
The reduce number of parameters is due to contraining all error variances $\operatorname{Var}(\epsilon_{j})$ to be equal (to $\Theta$) across all measurement occasions, rather than being subscripted by $j$ as in the regular SEM.
The reason for this is that the latent GCM model can equivalently be viewed as a random intercept and random slope (multilevel) model, where repeated observations over time are nested within individuals. 
Under this hierarchical perspective, it is customary--and theoretically justified--to assume homogeneous measurement error variances across repeated measures, reflecting consistent reliability of observations at each measurement occasion. 
This equivalence also motivates our examination of Restricted Maximum Likelihood (REML) estimation, which is commonly employed in multilevel modelling contexts to reduce bias in the estimation of variance components, particularly when dealing with limited sample sizes.
A description of the REML method is provided in the next subsection.

::: {#fig-path-growth}
```{r pathgrowth, engine = "tikz"}
\usetikzlibrary{shapes.geometric,arrows,calc,positioning}
\begin{tikzpicture}
\node[inner sep=3pt] {%
\begin{tikzpicture}[%
  >=stealth,              % for arrow tips
  auto,                   % automatic label positioning
  node distance=1.6cm,
  every node/.style={font=\small},
  latent/.style={circle, draw, minimum size=8mm, inner sep=0mm},
  obs/.style={rectangle, draw, minimum size=7mm, inner sep=0mm},
  error/.style={circle, draw, minimum size=6mm, inner sep=0mm},
intercept/.style={regular polygon, regular polygon sides=3, draw, inner sep=0pt, minimum size=6mm}
  ]

%-------------------------------------------------------
% 1) LATENT FACTORS (intercept i, slope s)
%-------------------------------------------------------
\node[latent] (i) at (-2,-3) {$i$};
\node[latent] (s) at ( 2,-3) {$s$};
\node[intercept] (int1) at (-2,-4.2) {\footnotesize 1};
\node[intercept] (int2) at (2,-4.2) {\footnotesize 1};

%-------------------------------------------------------
% 2) COVARIANCES AMONG LATENT FACTORS
%    - psi_{11} on i
%    - psi_{22} on s
%    - psi_{21} between i and s
%-------------------------------------------------------
% i <-> s
\draw[<->] (i) to[out=-45, in=225] node[below] {$\Psi_{12}$} (s);

% Variance of i
\draw[<->] (i) to[out=190, in=220, looseness=4] 
  node[left] {$\Psi_{11}$} (i);

% Variance of s
\draw[<->] (s) to[out=-40, in=-10, looseness=4] 
  node[right] {$\Psi_{22}$} (s);

\draw[->] (int1) -- node[right,pos=0.3] {$\alpha_1$} (i);
\draw[->] (int2) -- node[right,pos=0.3] {$\alpha_2$} (s);

%-------------------------------------------------------
% 3) OBSERVED VARIABLES (y1..y10) + ERRORS (eps1..eps10)
%    Arrange them in a horizontal row above i and s.
%-------------------------------------------------------
\foreach \j in {1,...,10} {
  % X-position for y_j (shift them so they are roughly centered)
  \pgfmathsetmacro{\x}{(\j*1.3 - 6.5)}

  % Observed variable y_j
  \node[obs] (y\j) at (\x,0) {$y_{\j}$};

  % Error term epsilon_j above y_j
  \node[error] (e\j) at (\x,1.5) {$\epsilon_{\j}$};

  % Arrow from error to observed
  \draw[->] (e\j) -- (y\j);

  % Intercept factor loadings: all = 1
  \draw[->] (i) -- node[pos=0.45,right] {\footnotesize 1} (y\j);

  % Slope factor loadings: 0..9
  \pgfmathtruncatemacro{\loading}{\j - 1}
  \draw[->] (s) -- node[pos=0.7,right] {\footnotesize\loading} (y\j);
}

\end{tikzpicture}
};
\end{tikzpicture}
```
Path diagram for the growth curve model.
:::

### Competitor methods

In @dhaene2022resampling, the authors compared four bias reduction methods on the two SEM models above namely the Bootstrap, Jackknife, Ozenne et al.'s [-@ozenne2020small] method, and Restricted Maximum Likelihood (REML).
The first two of these methods involve resampling, while @ozenne2020small and REML are analytical methods.

Explain jackknife...

Explain bootstrap...

Explain Ozenne...

Explain REML...

### Simulation scenarios

Assessing bias reduction under these various settings is crucial because estimation methods may behave differently when a departure from ideal condition occurs.
To thoroughly assess robustness and performance of the proposed methods, we considered multiple scenarios varying by three experimental factors: 

- Sample size $n=15,20,50,100,1000$
- Reliability of the observed variables $\text{rel}=0.5,0.8$
- Distribution of $\boldsymbol\eta$ and $\boldsymbol\epsilon$, one of "Normal", "Kurtosis", or "Non-normal".

Reliability in this context refers to the extent to which an observed indicator accurately reflects the underlying latent construct, with higher reliability indicating that a greater proportion of the indicatorâ€™s variance is attributable to the true score, and a lower proportion to measurement error. 
Lower reliability (indicating high measurement error) can exacerbate finite sample bias, particularly in the estimation of variance components, and thus influence the overall performance of bias-correction techniques. 
The chosen values of 0.8 and 0.5 are justified as they represent two distinct and informative measurement scenarios: a reliability of 0.8 simulates a close to ideal condition where the indicators capture 80% of the true variability, thereby reflecting a high-quality measurement scenario, while a reliability of 0.5 simulates a more challenging condition.

::: {#tbl-panel layout-ncol=2}


```{r}
#| label: tbl-truth-twofac
#| html-table-processing: none
#| tbl-cap: 'Two-factor model parameters.'
tibble(
  rel = paste0("Reliability = ", c(0.8, 0.5))
) |>
  mutate(
    truth = list(truth_twofac(0.8), truth_twofac(0.5)),
    param = map(truth, ~ names(.x))
  ) |>
  unnest(c(truth, param)) |> 
  mutate(param = case_when(
    param == "fx=~x2" ~ "$\\Lambda_{2,1}$",
    param == "fx=~x3" ~ "$\\Lambda_{3,1}$",
    param == "fy=~y2" ~ "$\\Lambda_{5,2}$",
    param == "fy=~y3" ~ "$\\Lambda_{6,2}$",
    param == "fy~fx"  ~ "$\\beta$",
    param == "x1~~x1" ~ "$\\Theta_{1,1}$",
    param == "x2~~x2" ~ "$\\Theta_{2,2}$",
    param == "x3~~x3" ~ "$\\Theta_{3,3}$",
    param == "y1~~y1" ~ "$\\Theta_{4,4}$",
    param == "y2~~y2" ~ "$\\Theta_{5,5}$",
    param == "y3~~y3" ~ "$\\Theta_{6,6}$",
    param == "fx~~fx" ~ "$\\Psi_{1,1}$",
    param == "fy~~fy" ~ "$\\Psi_{2,2}$"
  )) |>
  mutate(truth = as.character(truth)) |>
  pivot_wider(names_from = rel, values_from = truth) |>
  gt(rowname_col = "param") |>
  fmt_markdown("param")
```


```{r}
#| label: tbl-truth-growth
#| html-table-processing: none
#| tbl-cap: 'Growth curve model parameters.'
tibble(
  rel = paste0("Reliability = ", c(0.8, 0.5))
) |>
  mutate(
    truth = list(truth_growth(0.8), truth_growth(0.5)),
    param = map(truth, ~ names(.x))
  ) |>
  unnest(c(truth, param)) |>
  distinct(rel, truth, param, .keep_all = TRUE) |>
  mutate(param = case_when(
    param == "v" ~ "$\\theta$",
    param == "i~~i" ~ "$\\Psi_{1,1}$",
    param == "s~~s" ~ "$\\Psi_{2,2}$",
    param == "i~~s" ~ "$\\Psi_{1,2}$",
    param == "i~1" ~ "$\\alpha_1$",
    param == "s~1" ~ "$\\alpha_2$",
  )) |>
  mutate(ord = case_when(
    grepl("Psi", param) ~ 3,
    grepl("alpha", param) ~ 1,
    grepl("theta", param) ~ 2
  )) |>
  arrange(ord) |>
  select(-ord) |>
  mutate(truth = as.character(truth)) |>
  pivot_wider(names_from = rel, values_from = truth) |>
  gt(rowname_col = "param") |>
  fmt_markdown("param")
```

True parameter values used in the simulations for the each reliability setting.
:::

Distribution-free estimation methods exist for SEM, which are initially derived from the Gaussian likelihood, but is generally thought to be quite robust to non-normality.
Investigating the effect of departing from the baseline normality assumption in SEM is also of interest.
This was done by replacing the normal distribution of *both* the latent factors and the measurement errors with one that has higher-order moments, yet still retains the same covariance structure.
In other words, every simulated dataset is constructed to have the same underlying covariance matrix, but the distributional shape (e.g., the bell-shaped curve in the normal case or the altered spread and tail behaviour in the non-normal and kurtosis cases) differs. 
The `{covsim}` [@gronneberg2022covsim] package was utilised for this purpose.
The function allows to generate multivariate data with a target covariance matrix while controlling for the excess kurtosis and skewness of the distribution.
In the baseline Normal distribution, both the skewness and excess kurtosis were set to $0$.
In the "Kurtosis" condition, excess kurtosis was set to $6$ (with skewness held at $0$) to induce heavier tails; and in the "Non-normal" condition, skewness was set to $-2$ with excess kurtosis of $6$, producing asymmetric heavy-tailed distributions.
@fig-compare-dist compares the shapes of the distributions for the three conditions.
Once the data were generated, the observed variables were computed as per equations ... for each model.

```{r}
#| label: fig-compare-dist
#| fig-cap: 'Scatterplots illustrating the three distributional conditions used in the simulation study. Each plot displays 1000 randomly generated observations drawn from a distribution with zero mean vector and covariance matrix $\begin{bmatrix} 1 & 0.3 \\ 0.3 & 1 \end{bmatrix}$. Marginal density plots on the axes highlight differences in symmetry and tail behavior across conditions.'
#| out-width: 100%
#| fig-width: 9
#| fig-height: 3.2

n <- 1000
rho <- 0.3
Sigma <- matrix(c(1, rho, rho, 1), nrow = 2)
dat <- list()
dat$Normal <-
  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(0, 2))[[1]]
dat$Kurtosis <-  # kurtosis
  covsim::rIG(n, Sigma, skewness = rep(0, 2), excesskurtosis = rep(6, 2))[[1]]
dat$`Non-normal` <-  # non-normal
  covsim::rIG(n, Sigma, skewness = rep(-2, 2), excesskurtosis = rep(6, 2))[[1]]

# get max and min for x and y axes
xyminmax <- 
  map(dat, as.data.frame) |>
  bind_rows() |>
  summarise(
    x_min = min(V1),
    x_max = max(V1),
    y_min = min(V2),
    y_max = max(V2)
  ) |> 
  unlist() 

allplots <- imap(dat, \(x, idx) {
  mycols <- rev(RColorBrewer::brewer.pal(3, "Set1"))
  names(mycols) <- names(dat)
  
  p <-
    as.data.frame(x) |>
    ggplot(aes(x = V1, y = V2)) +
    geom_point(alpha = 0.5, size = 1, col = mycols[idx]) +
    geom_density_2d(col = "black") +
    theme_bw() +
    theme(legend.position = "none") +
    scale_x_continuous(limits = c(xyminmax["x_min"], xyminmax["x_max"])) +
    scale_y_continuous(limits = c(xyminmax["y_min"], xyminmax["y_max"])) +
    labs(title = glue::glue("{idx}"), x = NULL, y = NULL)
  ggExtra:::ggMarginal(p, fill = mycols[idx], alpha = 0.5, size = 10) 
})
cowplot::plot_grid(plotlist = allplots, nrow = 1, labels = "auto")
```

### Performance measures

Across these $5\times 2 \times 3 = 30$ distinct scenarios, 1000 replications of the data were generated and fitted using the ML, our two bias reduction methods (eBR and iBR), as well as the competitor methods.
The performance of each method was evaluated using the criteria below.
In what follows, let $r=1,\dots,R$ index the replications, $\hat{\theta}^{(r)}$ be the parameter estimates from the $r$-th replication, and $\theta$ be the true parameter value.

1. Mean bias
  $$
  \operatorname{Bias}(\hat{\theta}) = \frac{1}{R}\sum_{r=1}^{R}(\hat{\theta}^{(r)} - \theta)
  $$

2. Probability of underestimation (median)
  $$
  \operatorname{P}(\hat{\theta}^{(r)} < \theta) = \frac{1}{R}\sum_{r=1}^{R}\mathbb{1}(\hat{\theta}^{(r)} < \theta)
  $$

3. Root Mean Square Error (RMSE)
  $$
  \text{RMSE}(\hat{\theta}) = \sqrt{\frac{1}{R}\sum_{r=1}^{R}(\hat{\theta}^{(r)} - \theta)^2}
  $$

4. Coverage of 95% Wald Confidence Intervals
  $$
  \text{Coverage} = \frac{1}{R}\sum_{r=1}^{R}\mathbb{1}\left(\hat{\theta}^{(r)} - 1.96\sqrt{\widehat{\text{Var}}(\hat{\theta}^{(r)})} \leq \theta \leq \hat{\theta}^{(r)} + 1.96\sqrt{\widehat{\text{Var}}(\hat{\theta}^{(r)})}\right)
  $$

Results summarising these outcomes are discussed in detail in the next section.
