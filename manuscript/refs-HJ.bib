@article{dejonckere2023modelbased,
  title = {A {{Model-Based Shrinkage Target}} to {{Avoid Non-convergence}} in {{Small Sample SEM}}},
  author = {De Jonckere, Julie and Rosseel, Yves},
  year = {2023},
  month = nov,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {30},
  number = {6},
  pages = {941--955},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2023.2171420},
  urldate = {2024-11-04},
  abstract = {Structural equation modeling is prone to a variety of problems when the sample size is small. One solution that attempts to solve the (non-convergence) problem of small sample SEM is found in shrinkage estimation, where a weighted average between the sample variance-covariance matrix (S) and a highly structured shrinkage target (T) is calculated to construct an adjusted sample variance-covariance matrix (Sa), which is then used as input for the analysis. Different target candidates have already been put forward in the literature, but in this paper, we propose using a model-based target matrix specifically tailored for structural equation models. Two simulation studies demonstrate the benefit of using a model-based target over other candidate targets in terms of convergence rate, bias, and MSE, but also emphasize the importance of constructing an optimal shrinkage intensity.},
  keywords = {Non-convergence,shrinkage estimation,small samples},
  file = {/Users/haziqj/Zotero/storage/GZXJ84M5/De Jonckere and Rosseel - 2023 - A Model-Based Shrinkage Target to Avoid Non-convergence in Small Sample SEM.pdf}
}

@article{dhaene2022resampling,
  title = {Resampling {{Based Bias Correction}} for {{Small Sample SEM}}},
  author = {Dhaene, Sara and Rosseel, Yves},
  year = {2022},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {5},
  pages = {755--771},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2022.2057999},
  urldate = {2024-04-17},
  abstract = {Structural Equation Models (SEMs) are typically estimated via Maximum Likelihood. Grounded in large sample theory, estimates are prone to finite sample bias. Although Restricted Maximum Likelihood (REML) can alleviate this bias, its applicability is constrained to SEMs that are mathematically equivalent to mixed effect models. Via Monte Carlo simulations, we explored whether resampling based corrections could serve as viable, more broadly applicable alternatives. Results indicate that Bootstrap and Jackknife corrections effectively attenuate small sample bias, at the expected expense of an increase in variability. Similar conclusions are drawn with respect to a more recently proposed analytic approach by Ozenne et~al., which was included for comparison. For all corrective methods, caution is advised when dealing with non-normal data and/or low reliability.},
  keywords = {Bias correction,Bootstrap,Jackknife,REML,small sample bias},
  file = {/Users/haziqj/Zotero/storage/FWGGT7N8/Dhaene and Rosseel - 2022 - Resampling Based Bias Correction for Small Sample SEM.pdf}
}

@article{firth1993bias,
  title = {Bias Reduction of Maximum Likelihood Estimates},
  author = {Firth, David},
  year = {1993},
  month = mar,
  journal = {Biometrika},
  volume = {80},
  number = {1},
  pages = {27--38},
  issn = {0006-3444},
  doi = {10.1093/biomet/80.1.27},
  urldate = {2023-11-22},
  abstract = {It is shown how, in regular parametric problems, the first-order term is removed from the asymptotic bias of maximum likelihood estimates by a suitable modification of the score function. In exponential families with canonical parameterization the effect is to penalize the likelihood by the Jeffreys invariant prior. In binomial logistic models, Poisson log linear models and certain other generalized linear models, the Jeffreys prior penalty function can be imposed in standard regression software using a scheme of iterative adjustments to the data.},
  file = {/Users/haziqj/Zotero/storage/XJXM24PX/Firth - 1993 - Bias reduction of maximum likelihood estimates.pdf}
}

@article{kosmidis2009bias,
  title = {Bias Reduction in Exponential Family Nonlinear Models},
  author = {Kosmidis, Ioannis and Firth, David},
  year = {2009},
  journal = {Biometrika},
  volume = {96},
  number = {4},
  pages = {793--804},
  publisher = {Oxford University Press},
  urldate = {2024-06-26}
}

@article{kosmidis2024empirical,
  title = {Empirical Bias-Reducing Adjustments to Estimating Functions},
  author = {Kosmidis, Ioannis and Lunardon, Nicola},
  year = {2024},
  month = feb,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {86},
  number = {1},
  pages = {62--89},
  issn = {1369-7412},
  doi = {10.1093/jrsssb/qkad083},
  urldate = {2024-06-25},
  abstract = {We develop a novel, general framework for reduced-bias M-estimation from asymptotically unbiased estimating functions. The framework relies on an empirical approximation of the bias by a function of derivatives of estimating function contributions. Reduced-bias M-estimation operates either implicitly, solving empirically adjusted estimating equations, or explicitly, subtracting the estimated bias from the original M-estimates, and applies to partially or fully specified models with likelihoods or surrogate objectives. Automatic differentiation can abstract away the algebra required to implement reduced-bias M-estimation. As a result, the bias-reduction methods, we introduce have broader applicability, straightforward implementation, and less algebraic or computational effort than other established bias-reduction methods that require resampling or expectations of products of log-likelihood derivatives. If M-estimation is by maximising an objective, then there always exists a bias-reducing penalised objective. That penalised objective relates to information criteria for model selection and can be enhanced with plug-in penalties to deliver reduced-bias M-estimates with extra properties, like finiteness for categorical data models. Inferential procedures and model selection procedures for M-estimators apply unaltered with the reduced-bias M-estimates. We demonstrate and assess the properties of reduced-bias M-estimation in well-used, prominent modelling settings of varying complexity.},
  file = {/Users/haziqj/Zotero/storage/LXTBLDHU/Kosmidis and Lunardon - 2024 - Empirical bias-reducing adjustments to estimating functions.pdf;/Users/haziqj/Zotero/storage/CRZMI84J/7275082.html}
}
