@article{bates2015fitting,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {67},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2025-05-16},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  copyright = {Copyright (c) 2015 Douglas Bates, Martin M{\"a}chler, Ben Bolker, Steve Walker},
  langid = {english},
  keywords = {Cholesky decomposition,linear mixed models,penalized least squares,sparse matrix methods},
  file = {/Users/haziqj/Zotero/storage/DLQNVCHM/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{bauer2003estimating,
  title = {Estimating {{Multilevel Linear Models}} as {{Structural Equation Models}}},
  author = {Bauer, Daniel J.},
  year = {2003},
  month = jun,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {28},
  number = {2},
  pages = {135--167},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/10769986028002135},
  urldate = {2025-05-16},
  abstract = {Multilevel linear models (MLMs) provide a powerful framework for analyzing data collected at nested or non-nested levels, such as students within classrooms. The current article draws on recent analytical and software advances to demonstrate that a broad class of MLMs may be estimated as structural equation models (SEMs). Moreover, within the SEM approach it is possible to include measurement models for predictors or outcomes, and to estimate the mediational pathways among predictors explicitly, tasks which are currently difficult with the conventional approach to multilevel modeling. The equivalency of the SEM approach with conventional methods for estimating MLMs is illustrated using empirical examples, including an example involving both multiple indicator latent factors for the outcomes and a causal chain for the predictors. The limitations of this approach for estimating MLMs are discussed and alternative approaches are considered.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/UP7LDMWM/Bauer - 2003 - Estimating Multilevel Linear Models as Structural Equation Models.pdf}
}

@article{cheung2013implementing,
  title = {Implementing {{Restricted Maximum Likelihood Estimation}} in {{Structural Equation Models}}},
  author = {Cheung, Mike W.-L.},
  year = {2013},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {20},
  number = {1},
  pages = {157--167},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2013.742404},
  urldate = {2025-05-16},
  langid = {english}
}

@article{corbeil1976restricted,
  title = {Restricted Maximum Likelihood ({{REML}}) Estimation of Variance Components in the Mixed Model},
  author = {Corbeil, Robert R. and Searle, Shayle R.},
  year = {1976},
  journal = {Technometrics},
  volume = {18},
  number = {1},
  pages = {31--38},
  publisher = {Taylor \& Francis},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/RJYBRAQ2/Corbeil and Searle - 1976 - Restricted maximum likelihood (REML) estimation of variance components in the mixed model.pdf}
}

@book{davison1997bootstrap,
  title = {Bootstrap Methods and Their Application},
  author = {Davison, Anthony Christopher and Hinkley, David Victor},
  year = {1997},
  number = {1},
  publisher = {Cambridge university press},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/96CN6XZY/Davison and Hinkley - 1997 - Bootstrap methods and their application.pdf}
}

@article{dejonckere2023modelbased,
  title = {A {{Model-Based Shrinkage Target}} to {{Avoid Non-convergence}} in {{Small Sample SEM}}},
  author = {De Jonckere, Julie and Rosseel, Yves},
  year = {2023},
  month = nov,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {30},
  number = {6},
  pages = {941--955},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2023.2171420},
  urldate = {2024-11-04},
  abstract = {Structural equation modeling is prone to a variety of problems when the sample size is small. One solution that attempts to solve the (non-convergence) problem of small sample SEM is found in shrinkage estimation, where a weighted average between the sample variance-covariance matrix (S) and a highly structured shrinkage target (T) is calculated to construct an adjusted sample variance-covariance matrix (Sa), which is then used as input for the analysis. Different target candidates have already been put forward in the literature, but in this paper, we propose using a model-based target matrix specifically tailored for structural equation models. Two simulation studies demonstrate the benefit of using a model-based target over other candidate targets in terms of convergence rate, bias, and MSE, but also emphasize the importance of constructing an optimal shrinkage intensity.},
  keywords = {Non-convergence,shrinkage estimation,small samples},
  file = {/Users/haziqj/Zotero/storage/GZXJ84M5/De Jonckere and Rosseel - 2023 - A Model-Based Shrinkage Target to Avoid Non-convergence in Small Sample SEM.pdf}
}

@article{dhaene2022resampling,
  title = {Resampling {{Based Bias Correction}} for {{Small Sample SEM}}},
  author = {Dhaene, Sara and Rosseel, Yves},
  year = {2022},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {5},
  pages = {755--771},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2022.2057999},
  urldate = {2024-04-17},
  abstract = {Structural Equation Models (SEMs) are typically estimated via Maximum Likelihood. Grounded in large sample theory, estimates are prone to finite sample bias. Although Restricted Maximum Likelihood (REML) can alleviate this bias, its applicability is constrained to SEMs that are mathematically equivalent to mixed effect models. Via Monte Carlo simulations, we explored whether resampling based corrections could serve as viable, more broadly applicable alternatives. Results indicate that Bootstrap and Jackknife corrections effectively attenuate small sample bias, at the expected expense of an increase in variability. Similar conclusions are drawn with respect to a more recently proposed analytic approach by Ozenne et~al., which was included for comparison. For all corrective methods, caution is advised when dealing with non-normal data and/or low reliability.},
  keywords = {Bias correction,Bootstrap,Jackknife,REML,small sample bias},
  file = {/Users/haziqj/Zotero/storage/FWGGT7N8/Dhaene and Rosseel - 2022 - Resampling Based Bias Correction for Small Sample SEM.pdf}
}

@book{efron1982jackknife,
  title = {The {{Jackknife}}, the {{Bootstrap}} and {{Other Resampling Plans}}},
  author = {Efron, Bradley},
  year = {1982},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611970319},
  urldate = {2024-06-26},
  isbn = {978-0-89871-179-0},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/6H2BNXSS/Efron - 1982 - The Jackknife, the Bootstrap and Other Resampling Plans.pdf}
}

@incollection{efron1992bootstrap,
  title = {Bootstrap {{Methods}}: {{Another Look}} at the {{Jackknife}}},
  shorttitle = {Bootstrap {{Methods}}},
  booktitle = {Breakthroughs in {{Statistics}}},
  author = {Efron, Bradley},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1992},
  pages = {569--593},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_41},
  urldate = {2025-05-16},
  isbn = {978-0-387-94039-7 978-1-4612-4380-9},
  file = {/Users/haziqj/Zotero/storage/H5HEZIWK/Efron - 1992 - Bootstrap Methods Another Look at the Jackknife.pdf}
}

@book{efron1994introduction,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  year = {1994},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  address = {New York},
  doi = {10.1201/9780429246593},
  abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
  isbn = {978-0-429-24659-3}
}

@article{firth1993bias,
  title = {Bias Reduction of Maximum Likelihood Estimates},
  author = {Firth, David},
  year = {1993},
  month = mar,
  journal = {Biometrika},
  volume = {80},
  number = {1},
  pages = {27--38},
  issn = {0006-3444},
  doi = {10.1093/biomet/80.1.27},
  urldate = {2023-11-22},
  abstract = {It is shown how, in regular parametric problems, the first-order term is removed from the asymptotic bias of maximum likelihood estimates by a suitable modification of the score function. In exponential families with canonical parameterization the effect is to penalize the likelihood by the Jeffreys invariant prior. In binomial logistic models, Poisson log linear models and certain other generalized linear models, the Jeffreys prior penalty function can be imposed in standard regression software using a scheme of iterative adjustments to the data.},
  file = {/Users/haziqj/Zotero/storage/XJXM24PX/Firth - 1993 - Bias reduction of maximum likelihood estimates.pdf}
}

@book{fitzmaurice2011applied,
  title = {Applied Longitudinal Analysis},
  author = {Fitzmaurice, Garrett M. and Laird, Nan M. and Ware, James H.},
  year = {2011},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley},
  address = {Hoboken},
  abstract = {"Written at a technical level suitable for researchers and graduate students, Applied Longitudinal Analysis provides a description of modern methods for analyzing longitudinal data. Focusing on General Linear and Mixed Effects Models for continuous responses, and extensions of Generalized Linear Models for discrete responses, the authors discuss in detail the relationships among these different models, including their underlying assumptions and relative merits."--Jacket},
  isbn = {978-1-119-51346-9},
  langid = {english},
  lccn = {519.53}
}

@article{fox2006teachers,
  title = {{{TEACHER}}'{{S CORNER}}: {{Structural Equation Modeling With}} the Sem {{Package}} in {{R}}},
  shorttitle = {{{TEACHER}}'{{S CORNER}}},
  author = {Fox, John},
  year = {2006},
  month = jun,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {13},
  number = {3},
  pages = {465--486},
  issn = {1070-5511, 1532-8007},
  doi = {10.1207/s15328007sem1303_7},
  urldate = {2024-05-03},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/LPLY798Q/Fox - 2006 - TEACHER'S CORNER Structural Equation Modeling Wit.pdf}
}

@article{gronneberg2022covsim,
  title = {Covsim: {{An R Package}} for {{Simulating Non-Normal Data}} for {{Structural Equation Models Using Copulas}}},
  shorttitle = {Covsim},
  author = {Gr{\o}nneberg, Steffen and Foldnes, Nj{\aa}l and Marcoulides, Katerina M.},
  year = {2022},
  month = may,
  journal = {Journal of Statistical Software},
  volume = {102},
  pages = {1--45},
  issn = {1548-7660},
  doi = {10.18637/jss.v102.i03},
  urldate = {2025-04-10},
  abstract = {In factor analysis and structural equation modeling non-normal data simulation is traditionally performed by specifying univariate skewness and kurtosis together with the target covariance matrix. However, this leaves little control over the univariate distributions and the multivariate copula of the simulated vector. In this paper we explain how a more flexible simulation method called vine-to-anything (VITA) may be obtained from copula-based techniques, as implemented in a new R package, covsim. VITA is based on the concept of a regular vine, where bivariate copulas are coupled together into a full multivariate copula. We illustrate how to simulate continuous and ordinal data for covariance modeling, and how to use the new package discnorm to test for underlying normality in ordinal data. An introduction to copula and vine simulation is provided in the appendix.},
  copyright = {Copyright (c) 2022 Steffen Gr{\o}nneberg, Nj{\aa}l Foldnes, Katerina M. Marcoulides},
  langid = {english},
  keywords = {covariance model,Non-normal simulation,ordinal covariance models,R,vine copulas},
  file = {/Users/haziqj/Zotero/storage/ZJGF65ER/Grønneberg et al. - 2022 - covsim An R Package for Simulating Non-Normal Data for Structural Equation Models Using Copulas.pdf}
}

@article{hall1988bootstrap,
  title = {On Bootstrap Resampling and Iteration},
  author = {Hall, Peter and Martin, Michael A.},
  year = {1988},
  journal = {Biometrika},
  volume = {75},
  number = {4},
  pages = {661--671},
  publisher = {Oxford University Press},
  urldate = {2024-06-26}
}

@article{harville1977maximum,
  title = {Maximum {{Likelihood Approaches}} to {{Variance Component Estimation}} and to {{Related Problems}}},
  author = {Harville, David A.},
  year = {1977},
  journal = {Journal of the American Statistical Association},
  volume = {72},
  number = {358},
  eprint = {2286796},
  eprinttype = {jstor},
  pages = {320--338},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2286796},
  urldate = {2025-05-16},
  abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
  file = {/Users/haziqj/Zotero/storage/N6IX739V/Harville - 1977 - Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems.pdf}
}

@article{jeffreys1946invariant,
  title = {An Invariant Form for the Prior Probability in Estimation Problems},
  author = {Jeffreys, Harold},
  year = {1946},
  month = sep,
  journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
  volume = {186},
  number = {1007},
  pages = {453--461},
  issn = {0080-4630, 2053-9169},
  doi = {10.1098/rspa.1946.0056},
  urldate = {2025-03-15},
  abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.},
  copyright = {https://royalsociety.org/journals/ethics-policies/data-sharing-mining/},
  langid = {english}
}

@article{kosmidis2009bias,
  title = {Bias Reduction in Exponential Family Nonlinear Models},
  author = {Kosmidis, Ioannis and Firth, David},
  year = {2009},
  journal = {Biometrika},
  volume = {96},
  number = {4},
  pages = {793--804},
  publisher = {Oxford University Press},
  urldate = {2024-06-26}
}

@article{kosmidis2024empirical,
  title = {Empirical Bias-Reducing Adjustments to Estimating Functions},
  author = {Kosmidis, Ioannis and Lunardon, Nicola},
  year = {2024},
  month = feb,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {86},
  number = {1},
  pages = {62--89},
  issn = {1369-7412},
  doi = {10.1093/jrsssb/qkad083},
  urldate = {2024-06-25},
  abstract = {We develop a novel, general framework for reduced-bias M-estimation from asymptotically unbiased estimating functions. The framework relies on an empirical approximation of the bias by a function of derivatives of estimating function contributions. Reduced-bias M-estimation operates either implicitly, solving empirically adjusted estimating equations, or explicitly, subtracting the estimated bias from the original M-estimates, and applies to partially or fully specified models with likelihoods or surrogate objectives. Automatic differentiation can abstract away the algebra required to implement reduced-bias M-estimation. As a result, the bias-reduction methods, we introduce have broader applicability, straightforward implementation, and less algebraic or computational effort than other established bias-reduction methods that require resampling or expectations of products of log-likelihood derivatives. If M-estimation is by maximising an objective, then there always exists a bias-reducing penalised objective. That penalised objective relates to information criteria for model selection and can be enhanced with plug-in penalties to deliver reduced-bias M-estimates with extra properties, like finiteness for categorical data models. Inferential procedures and model selection procedures for M-estimators apply unaltered with the reduced-bias M-estimates. We demonstrate and assess the properties of reduced-bias M-estimation in well-used, prominent modelling settings of varying complexity.},
  file = {/Users/haziqj/Zotero/storage/LXTBLDHU/Kosmidis and Lunardon - 2024 - Empirical bias-reducing adjustments to estimating functions.pdf;/Users/haziqj/Zotero/storage/CRZMI84J/7275082.html}
}

@article{mcneish2016using,
  title = {Using {{Data-Dependent Priors}} to {{Mitigate Small Sample Bias}} in {{Latent Growth Models}}: {{A Discussion}} and {{Illustration Using M}} {\emph{Plus}}},
  shorttitle = {Using {{Data-Dependent Priors}} to {{Mitigate Small Sample Bias}} in {{Latent Growth Models}}},
  author = {McNeish, Daniel M.},
  year = {2016},
  month = feb,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {41},
  number = {1},
  pages = {27--56},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/1076998615621299},
  urldate = {2025-05-16},
  abstract = {Mixed-effects models (MEMs) and latent growth models (LGMs) are often considered interchangeable save the discipline-specific nomenclature. Software implementations of these models, however, are not interchangeable, particularly with small sample sizes. Restricted maximum likelihood estimation that mitigates small sample bias in MEMs has not been widely developed for LGMs, and fully Bayesian methods, while not dependent on asymptotics, can encounter issues because the choice for the factor covariance matrix prior distribution has substantial influence with small samples. This tutorial discusses differences between LGMs and MEMs and demonstrates how data-dependent priors, an established class of methods that blend frequentist and Bayesian paradigms, can be implemented within M plus 7.1 to abate the small sample bias that is prevalent with LGM software while keeping additional programming to the bare minimum.},
  langid = {english}
}

@article{mcneish2018brief,
  title = {Brief {{Research Report}}: {{Growth Models With Small Samples}} and {{Missing Data}}},
  shorttitle = {Brief {{Research Report}}},
  author = {McNeish, Daniel},
  year = {2018},
  month = oct,
  journal = {The Journal of Experimental Education},
  volume = {86},
  number = {4},
  pages = {690--701},
  issn = {0022-0973, 1940-0683},
  doi = {10.1080/00220973.2017.1369384},
  urldate = {2025-05-16},
  langid = {english}
}

@article{mcneish2018differentiating,
  title = {Differentiating between Mixed-Effects and Latent-Curve Approaches to Growth Modeling},
  author = {McNeish, Daniel and Matta, Tyler},
  year = {2018},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {50},
  number = {4},
  pages = {1398--1414},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0976-5},
  urldate = {2025-05-16},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/3J9KJTUC/McNeish and Matta - 2018 - Differentiating between mixed-effects and latent-curve approaches to growth modeling.pdf}
}

@article{ozenne2020small,
  title = {Small {{Sample Corrections}} for {{Wald Tests}} in {{Latent Variable Models}}},
  author = {Ozenne, Brice and Fisher, Patrick M. and {Budtz-Jorgensen}, Esben},
  year = {2020},
  month = aug,
  journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume = {69},
  number = {4},
  pages = {841--861},
  issn = {0035-9254},
  doi = {10.1111/rssc.12414},
  urldate = {2024-04-17},
  abstract = {Latent variable models are commonly used in psychology and increasingly used for analysing brain imaging data. Such studies typically involve a small number of participants (n\&lt;100), where standard asymptotic results often fail to control the type 1 error appropriately. The paper presents two corrections improving the control of the type 1 error of Wald tests in latent variable models estimated by using maximum likelihood. First, we derive a correction for the bias of the maximum likelihood estimator of the variance parameters. This enables us to estimate corrected standard errors for model parameters and corrected Wald statistics. Second, we use a Student t-distribution instead of a Gaussian distribution to account for the variability of the variance estimator. The degrees of freedom of the Student t-distributions are estimated by using a Satterthwaite approximation. A simulation study based on data from two published brain imaging studies demonstrates that combining these two corrections provides superior control of the type 1 error rate compared with the uncorrected Wald test, despite being conservative for some parameters. The methods proposed are implemented in the R package lavaSearch2, which is available from https://cran.r-project.org/web/packages/lavaSearch2.},
  file = {/Users/haziqj/Zotero/storage/PRLWHM7X/Ozenne et al. - 2020 - Small Sample Corrections for Wald Tests in Latent Variable Models.pdf;/Users/haziqj/Zotero/storage/P7FWUJWA/7058441.html}
}

@article{patterson1971recovery,
  title = {Recovery of Inter-Block Information When Block Sizes Are Unequal},
  author = {Patterson, H. Desmond and Thompson, Robin},
  year = {1971},
  journal = {Biometrika},
  volume = {58},
  number = {3},
  pages = {545--554},
  publisher = {Oxford University Press},
  urldate = {2025-05-16}
}

@article{quenouille1949approximate,
  title = {Approximate {{Tests}} of {{Correlation}} in {{Time-Series}}},
  author = {Quenouille, M. H.},
  year = {1949},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {11},
  number = {1},
  eprint = {2983696},
  eprinttype = {jstor},
  pages = {68--84},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9246},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/Y9689UE5/Quenouille - 1949 - Approximate Tests of Correlation in Time-Series.pdf}
}

@article{quenouille1956notes,
  title = {Notes on Bias in Estimation},
  author = {Quenouille, Maurice H.},
  year = {1956},
  journal = {Biometrika},
  volume = {43},
  number = {3/4},
  eprint = {2332914},
  eprinttype = {jstor},
  pages = {353--360},
  publisher = {JSTOR},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/3DNDZMP3/Quenouille - 1956 - Notes on bias in estimation.pdf}
}

@article{sanchez2005structural,
  title = {Structural {{Equation Models}}: {{A Review With Applications}} to {{Environmental Epidemiology}}},
  shorttitle = {Structural {{Equation Models}}},
  author = {S{\'a}nchez, Brisa N and {Budtz-J{\o}rgensen}, Esben and Ryan, Louise M and Hu, Howard},
  year = {2005},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {472},
  pages = {1443--1455},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000001005},
  urldate = {2024-04-23},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/SW9Y9DKG/Sánchez et al. - 2005 - Structural Equation Models A Review With Applicat.pdf}
}

@article{savalei2022computational,
  title = {Computational {{Options}} for {{Standard Errors}} and {{Test Statistics}} with {{Incomplete Normal}} and {{Nonnormal Data}} in {{SEM}}},
  author = {Savalei, Victoria and Rosseel, Yves},
  year = {2022},
  month = mar,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {2},
  pages = {163--181},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2021.1877548},
  urldate = {2024-04-25},
  abstract = {This article provides an overview of different computational options for inference following normal theory maximum likelihood (ML) estimation in structural equation modeling (SEM) with incomplete normal and nonnormal data. Complete data are covered as a special case. These computational options include whether the information matrix is observed or expected, whether the observed information matrix is estimated numerically or using an analytic asymptotic approximation, and whether the information matrix and the outer product matrix of the score vector are evaluated at the saturated or at the structured estimates. A variety of different standard errors and robust test statistics become possible by varying these options. We review the asymptotic properties of these computational variations, and we show how to obtain them using lavaan in R. We hope that this article will encourage methodologists to study the impact of the available computational options on the performance of standard errors and test statistics in SEM.},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/2S72L7P2/Savalei and Rosseel - 2022 - Computational Options for Standard Errors and Test Statistics with Incomplete Normal and Nonnormal D.pdf}
}

@book{skrondal2004generalized,
  title = {Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models},
  shorttitle = {Generalized Latent Variable Modeling},
  author = {Skrondal, Anders and {Rabe-Hesketh}, S.},
  year = {2004},
  series = {Chapman \& {{Hall}}/{{CRC}} Interdisciplinary Statistics Series},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton},
  isbn = {978-1-58488-000-4},
  lccn = {QA278.6 .S57 2004},
  keywords = {Latent structure analysis,Latent variables}
}

@article{sterzinger2023maximum,
  title = {Maximum Softly-Penalized Likelihood for Mixed Effects Logistic Regression},
  author = {Sterzinger, Philipp and Kosmidis, Ioannis},
  year = {2023},
  journal = {Statistics and Computing},
  volume = {33},
  number = {2},
  issn = {0960-3174},
  doi = {10.1007/s11222-023-10217-3},
  urldate = {2024-12-10},
  abstract = {Maximum likelihood estimation in logistic regression with mixed effects is known to often result in estimates on the boundary of the parameter space. Such estimates, which include infinite values for fixed effects and singular or infinite variance components, can cause havoc to numerical estimation procedures and inference. We introduce an appropriately scaled additive penalty to the log-likelihood function, or an approximation thereof, which penalizes the fixed effects by the Jeffreys' invariant prior for the model with no random effects and the variance components by a composition of negative Huber loss functions. The resulting maximum penalized likelihood estimates are shown to lie in the interior of the parameter space. Appropriate scaling of the penalty guarantees that the penalization is soft enough to preserve the optimal asymptotic properties expected by the maximum likelihood estimator, namely consistency, asymptotic normality, and Cram{\'e}r-Rao efficiency. Our choice of penalties and scaling factor preserves equivariance of the fixed effects estimates under linear transformation of the model parameters, such as contrasts. Maximum softly-penalized likelihood is compared to competing approaches on two real-data examples, and through comprehensive simulation studies that illustrate its superior finite sample performance.},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/5U6JQ6BC/Sterzinger and Kosmidis - 2023 - Maximum softly-penalized likelihood for mixed effects logistic regression.pdf}
}

@article{tukey1958bias,
  title = {Bias and Confidence in Not Quite Large Samples},
  author = {Tukey, John},
  year = {1958},
  journal = {Ann. Math. Statist.},
  volume = {29},
  pages = {614},
  urldate = {2025-05-16}
}

@book{vandeschoot2020small,
  title = {Small {{Sample Size Solutions}}: {{A Guide}} for {{Applied Researchers}} and {{Practitioners}}},
  shorttitle = {Small Sample Size Solutions},
  author = {{van de Schoot}, Rens and Mio{\v c}evi{\'c}, Milica},
  year = {2020},
  publisher = {Routledge},
  address = {Abingdon, Oxon; New York, NY},
  isbn = {978-0-429-27387-2},
  langid = {english},
  lccn = {Q180.55.M4},
  keywords = {Data sets,Methodology,Research},
  file = {/Users/haziqj/Zotero/storage/HKX8ZJD2/2020 - Small sample size solutions a guide for applied researchers and practitioners.pdf}
}
