@article{dejonckere2023modelbased,
  title = {A {{Model-Based Shrinkage Target}} to {{Avoid Non-convergence}} in {{Small Sample SEM}}},
  author = {De Jonckere, Julie and Rosseel, Yves},
  year = {2023},
  month = nov,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {30},
  number = {6},
  pages = {941--955},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2023.2171420},
  urldate = {2024-11-04},
  abstract = {Structural equation modeling is prone to a variety of problems when the sample size is small. One solution that attempts to solve the (non-convergence) problem of small sample SEM is found in shrinkage estimation, where a weighted average between the sample variance-covariance matrix (S) and a highly structured shrinkage target (T) is calculated to construct an adjusted sample variance-covariance matrix (Sa), which is then used as input for the analysis. Different target candidates have already been put forward in the literature, but in this paper, we propose using a model-based target matrix specifically tailored for structural equation models. Two simulation studies demonstrate the benefit of using a model-based target over other candidate targets in terms of convergence rate, bias, and MSE, but also emphasize the importance of constructing an optimal shrinkage intensity.},
  keywords = {Non-convergence,shrinkage estimation,small samples},
  file = {/Users/haziqj/Zotero/storage/GZXJ84M5/De Jonckere and Rosseel - 2023 - A Model-Based Shrinkage Target to Avoid Non-convergence in Small Sample SEM.pdf}
}

@article{dhaene2022resampling,
  title = {Resampling {{Based Bias Correction}} for {{Small Sample SEM}}},
  author = {Dhaene, Sara and Rosseel, Yves},
  year = {2022},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {5},
  pages = {755--771},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2022.2057999},
  urldate = {2024-04-17},
  abstract = {Structural Equation Models (SEMs) are typically estimated via Maximum Likelihood. Grounded in large sample theory, estimates are prone to finite sample bias. Although Restricted Maximum Likelihood (REML) can alleviate this bias, its applicability is constrained to SEMs that are mathematically equivalent to mixed effect models. Via Monte Carlo simulations, we explored whether resampling based corrections could serve as viable, more broadly applicable alternatives. Results indicate that Bootstrap and Jackknife corrections effectively attenuate small sample bias, at the expected expense of an increase in variability. Similar conclusions are drawn with respect to a more recently proposed analytic approach by Ozenne et~al., which was included for comparison. For all corrective methods, caution is advised when dealing with non-normal data and/or low reliability.},
  keywords = {Bias correction,Bootstrap,Jackknife,REML,small sample bias},
  file = {/Users/haziqj/Zotero/storage/FWGGT7N8/Dhaene and Rosseel - 2022 - Resampling Based Bias Correction for Small Sample SEM.pdf}
}

@article{firth1993bias,
  title = {Bias Reduction of Maximum Likelihood Estimates},
  author = {Firth, David},
  year = {1993},
  month = mar,
  journal = {Biometrika},
  volume = {80},
  number = {1},
  pages = {27--38},
  issn = {0006-3444},
  doi = {10.1093/biomet/80.1.27},
  urldate = {2023-11-22},
  abstract = {It is shown how, in regular parametric problems, the first-order term is removed from the asymptotic bias of maximum likelihood estimates by a suitable modification of the score function. In exponential families with canonical parameterization the effect is to penalize the likelihood by the Jeffreys invariant prior. In binomial logistic models, Poisson log linear models and certain other generalized linear models, the Jeffreys prior penalty function can be imposed in standard regression software using a scheme of iterative adjustments to the data.},
  file = {/Users/haziqj/Zotero/storage/XJXM24PX/Firth - 1993 - Bias reduction of maximum likelihood estimates.pdf}
}

@article{fox2006teachers,
  title = {{{TEACHER}}'{{S CORNER}}: {{Structural Equation Modeling With}} the Sem {{Package}} in {{R}}},
  shorttitle = {{{TEACHER}}'{{S CORNER}}},
  author = {Fox, John},
  year = {2006},
  month = jun,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {13},
  number = {3},
  pages = {465--486},
  issn = {1070-5511, 1532-8007},
  doi = {10.1207/s15328007sem1303_7},
  urldate = {2024-05-03},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/LPLY798Q/Fox - 2006 - TEACHER'S CORNER Structural Equation Modeling Wit.pdf}
}

@article{kosmidis2009bias,
  title = {Bias Reduction in Exponential Family Nonlinear Models},
  author = {Kosmidis, Ioannis and Firth, David},
  year = {2009},
  journal = {Biometrika},
  volume = {96},
  number = {4},
  pages = {793--804},
  publisher = {Oxford University Press},
  urldate = {2024-06-26}
}

@article{kosmidis2024empirical,
  title = {Empirical Bias-Reducing Adjustments to Estimating Functions},
  author = {Kosmidis, Ioannis and Lunardon, Nicola},
  year = {2024},
  month = feb,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {86},
  number = {1},
  pages = {62--89},
  issn = {1369-7412},
  doi = {10.1093/jrsssb/qkad083},
  urldate = {2024-06-25},
  abstract = {We develop a novel, general framework for reduced-bias M-estimation from asymptotically unbiased estimating functions. The framework relies on an empirical approximation of the bias by a function of derivatives of estimating function contributions. Reduced-bias M-estimation operates either implicitly, solving empirically adjusted estimating equations, or explicitly, subtracting the estimated bias from the original M-estimates, and applies to partially or fully specified models with likelihoods or surrogate objectives. Automatic differentiation can abstract away the algebra required to implement reduced-bias M-estimation. As a result, the bias-reduction methods, we introduce have broader applicability, straightforward implementation, and less algebraic or computational effort than other established bias-reduction methods that require resampling or expectations of products of log-likelihood derivatives. If M-estimation is by maximising an objective, then there always exists a bias-reducing penalised objective. That penalised objective relates to information criteria for model selection and can be enhanced with plug-in penalties to deliver reduced-bias M-estimates with extra properties, like finiteness for categorical data models. Inferential procedures and model selection procedures for M-estimators apply unaltered with the reduced-bias M-estimates. We demonstrate and assess the properties of reduced-bias M-estimation in well-used, prominent modelling settings of varying complexity.},
  file = {/Users/haziqj/Zotero/storage/LXTBLDHU/Kosmidis and Lunardon - 2024 - Empirical bias-reducing adjustments to estimating functions.pdf;/Users/haziqj/Zotero/storage/CRZMI84J/7275082.html}
}

@article{ozenne2020small,
  title = {Small {{Sample Corrections}} for {{Wald Tests}} in {{Latent Variable Models}}},
  author = {Ozenne, Brice and Fisher, Patrick M. and {Budtz-J{$\oslash$}rgensen}, Esben},
  year = {2020},
  month = aug,
  journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume = {69},
  number = {4},
  pages = {841--861},
  issn = {0035-9254},
  doi = {10.1111/rssc.12414},
  urldate = {2024-04-17},
  abstract = {Latent variable models are commonly used in psychology and increasingly used for analysing brain imaging data. Such studies typically involve a small number of participants (n\&lt;100), where standard asymptotic results often fail to control the type 1 error appropriately. The paper presents two corrections improving the control of the type 1 error of Wald tests in latent variable models estimated by using maximum likelihood. First, we derive a correction for the bias of the maximum likelihood estimator of the variance parameters. This enables us to estimate corrected standard errors for model parameters and corrected Wald statistics. Second, we use a Student t-distribution instead of a Gaussian distribution to account for the variability of the variance estimator. The degrees of freedom of the Student t-distributions are estimated by using a Satterthwaite approximation. A simulation study based on data from two published brain imaging studies demonstrates that combining these two corrections provides superior control of the type 1 error rate compared with the uncorrected Wald test, despite being conservative for some parameters. The methods proposed are implemented in the R package lavaSearch2, which is available from https://cran.r-project.org/web/packages/lavaSearch2.},
  file = {/Users/haziqj/Zotero/storage/PRLWHM7X/Ozenne et al. - 2020 - Small Sample Corrections for Wald Tests in Latent Variable Models.pdf;/Users/haziqj/Zotero/storage/P7FWUJWA/7058441.html}
}

@article{sanchez2005structural,
  title = {Structural {{Equation Models}}: {{A Review With Applications}} to {{Environmental Epidemiology}}},
  shorttitle = {Structural {{Equation Models}}},
  author = {S{\'a}nchez, Brisa N and {Budtz-J{\o}rgensen}, Esben and Ryan, Louise M and Hu, Howard},
  year = {2005},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {472},
  pages = {1443--1455},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000001005},
  urldate = {2024-04-23},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/SW9Y9DKG/SÃ¡nchez et al. - 2005 - Structural Equation Models A Review With Applicat.pdf}
}

@article{savalei2022computational,
  title = {Computational {{Options}} for {{Standard Errors}} and {{Test Statistics}} with {{Incomplete Normal}} and {{Nonnormal Data}} in {{SEM}}},
  author = {Savalei, Victoria and Rosseel, Yves},
  year = {2022},
  month = mar,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {2},
  pages = {163--181},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2021.1877548},
  urldate = {2024-04-25},
  abstract = {This article provides an overview of different computational options for inference following normal theory maximum likelihood (ML) estimation in structural equation modeling (SEM) with incomplete normal and nonnormal data. Complete data are covered as a special case. These computational options include whether the information matrix is observed or expected, whether the observed information matrix is estimated numerically or using an analytic asymptotic approximation, and whether the information matrix and the outer product matrix of the score vector are evaluated at the saturated or at the structured estimates. A variety of different standard errors and robust test statistics become possible by varying these options. We review the asymptotic properties of these computational variations, and we show how to obtain them using lavaan in R. We hope that this article will encourage methodologists to study the impact of the available computational options on the performance of standard errors and test statistics in SEM.},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/2S72L7P2/Savalei and Rosseel - 2022 - Computational Options for Standard Errors and Test Statistics with Incomplete Normal and Nonnormal D.pdf}
}

@article{sterzinger2023maximum,
  title = {Maximum Softly-Penalized Likelihood for Mixed Effects Logistic Regression},
  author = {Sterzinger, Philipp and Kosmidis, Ioannis},
  year = {2023},
  journal = {Statistics and Computing},
  volume = {33},
  number = {2},
  issn = {0960-3174},
  doi = {10.1007/s11222-023-10217-3},
  urldate = {2024-12-10},
  abstract = {Maximum likelihood estimation in logistic regression with mixed effects is known to often result in estimates on the boundary of the parameter space. Such estimates, which include infinite values for fixed effects and singular or infinite variance components, can cause havoc to numerical estimation procedures and inference. We introduce an appropriately scaled additive penalty to the log-likelihood function, or an approximation thereof, which penalizes the fixed effects by the Jeffreys' invariant prior for the model with no random effects and the variance components by a composition of negative Huber loss functions. The resulting maximum penalized likelihood estimates are shown to lie in the interior of the parameter space. Appropriate scaling of the penalty guarantees that the penalization is soft enough to preserve the optimal asymptotic properties expected by the maximum likelihood estimator, namely consistency, asymptotic normality, and Cram{\'e}r-Rao efficiency. Our choice of penalties and scaling factor preserves equivariance of the fixed effects estimates under linear transformation of the model parameters, such as contrasts. Maximum softly-penalized likelihood is compared to competing approaches on two real-data examples, and through comprehensive simulation studies that illustrate its superior finite sample performance.},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/5U6JQ6BC/Sterzinger and Kosmidis - 2023 - Maximum softly-penalized likelihood for mixed effects logistic regression.pdf}
}

@book{vandeschoot2020small,
  title = {Small {{Sample Size Solutions}}: {{A Guide}} for {{Applied Researchers}} and {{Practitioners}}},
  shorttitle = {Small Sample Size Solutions},
  author = {{van de Schoot}, Rens and Mio{\v c}evi{\'c}, Milica},
  year = {2020},
  publisher = {Routledge},
  address = {Abingdon, Oxon; New York, NY},
  isbn = {978-0-429-27387-2},
  langid = {english},
  lccn = {Q180.55.M4},
  keywords = {Data sets,Methodology,Research},
  file = {/Users/haziqj/Zotero/storage/HKX8ZJD2/2020 - Small sample size solutions a guide for applied researchers and practitioners.pdf}
}
