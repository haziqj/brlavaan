@article{bates2015fitting,
  title = {Fitting {{Linear Mixed-Effects Models Using}} Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  month = oct,
  journal = {Journal of Statistical Software},
  volume = {67},
  pages = {1--48},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2025-05-16},
  abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
  copyright = {Copyright (c) 2015 Douglas Bates, Martin M{\"a}chler, Ben Bolker, Steve Walker},
  langid = {english},
  keywords = {Cholesky decomposition,linear mixed models,penalized least squares,sparse matrix methods},
  file = {/Users/haziqj/Zotero/storage/DLQNVCHM/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@article{bauer2003estimating,
  title = {Estimating {{Multilevel Linear Models}} as {{Structural Equation Models}}},
  author = {Bauer, Daniel J.},
  year = {2003},
  month = jun,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {28},
  number = {2},
  pages = {135--167},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/10769986028002135},
  urldate = {2025-05-16},
  abstract = {Multilevel linear models (MLMs) provide a powerful framework for analyzing data collected at nested or non-nested levels, such as students within classrooms. The current article draws on recent analytical and software advances to demonstrate that a broad class of MLMs may be estimated as structural equation models (SEMs). Moreover, within the SEM approach it is possible to include measurement models for predictors or outcomes, and to estimate the mediational pathways among predictors explicitly, tasks which are currently difficult with the conventional approach to multilevel modeling. The equivalency of the SEM approach with conventional methods for estimating MLMs is illustrated using empirical examples, including an example involving both multiple indicator latent factors for the outcomes and a causal chain for the predictors. The limitations of this approach for estimating MLMs are discussed and alternative approaches are considered.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/UP7LDMWM/Bauer - 2003 - Estimating Multilevel Linear Models as Structural Equation Models.pdf}
}

@article{bentler1999structural,
  title = {Structural {{Equation Modeling}} with {{Small Samples}}: {{Test Statistics}}},
  shorttitle = {Structural {{Equation Modeling}} with {{Small Samples}}},
  author = {Bentler, Peter M. and Yuan, Ke-Hai},
  year = {1999},
  month = apr,
  journal = {Multivariate Behavioral Research},
  volume = {34},
  number = {2},
  pages = {181--197},
  issn = {0027-3171, 1532-7906},
  doi = {10.1207/S15327906Mb340203},
  urldate = {2025-05-16},
  langid = {english}
}

@article{cheung2013implementing,
  title = {Implementing {{Restricted Maximum Likelihood Estimation}} in {{Structural Equation Models}}},
  author = {Cheung, Mike W.-L.},
  year = {2013},
  month = jan,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {20},
  number = {1},
  pages = {157--167},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2013.742404},
  urldate = {2025-05-16},
  langid = {english}
}

@article{corbeil1976restricted,
  title = {Restricted Maximum Likelihood ({{REML}}) Estimation of Variance Components in the Mixed Model},
  author = {Corbeil, Robert R. and Searle, Shayle R.},
  year = {1976},
  journal = {Technometrics},
  volume = {18},
  number = {1},
  pages = {31--38},
  publisher = {Taylor \& Francis},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/RJYBRAQ2/Corbeil and Searle - 1976 - Restricted maximum likelihood (REML) estimation of variance components in the mixed model.pdf}
}

@article{cox1968general,
  title = {A {{General Definition}} of {{Residuals}}},
  author = {Cox, D. R. and Snell, E. J.},
  year = {1968},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {30},
  number = {2},
  eprint = {2984505},
  eprinttype = {jstor},
  pages = {248--275},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9246},
  urldate = {2025-05-16},
  abstract = {Residuals are usually defined in connection with linear models. Here a more general definition is given and some asymptotic properties found. Some illustrative examples are discussed, including a regression problem involving exponentially distributed errors and some problems concerning Poisson and binomially distributed observations.},
  file = {/Users/haziqj/Zotero/storage/QA57UZ8C/Cox and Snell - 1968 - A General Definition of Residuals.pdf}
}

@book{cox1979theoretical,
  title = {Theoretical {{Statistics}}},
  author = {Cox, D. R. and Hinkley, D. V.},
  year = {1979},
  month = sep,
  publisher = {{Chapman and Hall/CRC}},
  address = {New York},
  doi = {10.1201/b14832},
  abstract = {A text that stresses the general concepts of the theory of statistics Theoretical Statistics provides a systematic statement of the theory of statistics, emphasizing general concepts rather than mathematical rigor. Chapters 1 through 3 provide an overview of statistics and discuss some of the basic philosophical ideas and problems behind statistica},
  isbn = {978-0-429-17021-8}
}

@book{davison1997bootstrap,
  title = {Bootstrap Methods and Their Application},
  author = {Davison, Anthony Christopher and Hinkley, David Victor},
  year = {1997},
  number = {1},
  publisher = {Cambridge university press},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/96CN6XZY/Davison and Hinkley - 1997 - Bootstrap methods and their application.pdf}
}

@article{dejonckere2022using,
  title = {Using {{Bounded Estimation}} to {{Avoid Nonconvergence}} in {{Small Sample Structural Equation Modeling}}},
  author = {De Jonckere, Julie and Rosseel, Yves},
  year = {2022},
  month = may,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {3},
  pages = {412--427},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2021.1982716},
  urldate = {2024-08-05},
  abstract = {The most frustrating outcome of an SEM analysis is nonconvergence. Nonconvergence typically happens when the sample size is small (N{$<$}100) or very small (N{$<$}50). To minimize the frequency of nonconvergence, this paper proposes a solution called bounded estimation. The idea is to use data-driven lower and upper bounds for a subset of the model parameters during estimation. In this paper, we provide a rationale to compute these bounds, and we study the effect of different approaches to employ these bounds on the frequency of nonconvergence. A simulation study shows that bounded estimation dramatically decreases the frequency of nonconvergence in both correctly and misspecified models, without any (negative) effects on the quality of the point estimates for the unbounded parameters.},
  keywords = {estimation,nonconvergence,Small samples},
  file = {/Users/haziqj/Zotero/storage/UZUKJDM4/De Jonckere and Rosseel - 2022 - Using Bounded Estimation to Avoid Nonconvergence in Small Sample Structural Equation Modeling.pdf}
}

@article{dejonckere2023modelbased,
  title = {A {{Model-Based Shrinkage Target}} to {{Avoid Non-convergence}} in {{Small Sample SEM}}},
  author = {De Jonckere, Julie and Rosseel, Yves},
  year = {2023},
  month = nov,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {30},
  number = {6},
  pages = {941--955},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2023.2171420},
  urldate = {2024-11-04},
  abstract = {Structural equation modeling is prone to a variety of problems when the sample size is small. One solution that attempts to solve the (non-convergence) problem of small sample SEM is found in shrinkage estimation, where a weighted average between the sample variance-covariance matrix (S) and a highly structured shrinkage target (T) is calculated to construct an adjusted sample variance-covariance matrix (Sa), which is then used as input for the analysis. Different target candidates have already been put forward in the literature, but in this paper, we propose using a model-based target matrix specifically tailored for structural equation models. Two simulation studies demonstrate the benefit of using a model-based target over other candidate targets in terms of convergence rate, bias, and MSE, but also emphasize the importance of constructing an optimal shrinkage intensity.},
  keywords = {Non-convergence,shrinkage estimation,small samples},
  file = {/Users/haziqj/Zotero/storage/GZXJ84M5/De Jonckere and Rosseel - 2023 - A Model-Based Shrinkage Target to Avoid Non-convergence in Small Sample SEM.pdf}
}

@article{deng2018structural,
  title = {Structural Equation Modeling with Many Variables: {{A}} Systematic Review of Issues and Developments},
  shorttitle = {Structural Equation Modeling with Many Variables},
  author = {Deng, Lifang and Yang, Miao and Marcoulides, Katerina M.},
  year = {2018},
  journal = {Frontiers in psychology},
  volume = {9},
  pages = {580},
  publisher = {Frontiers Media SA},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/8GN5Y8EL/Deng et al. - 2018 - Structural equation modeling with many variables A systematic review of issues and developments.pdf}
}

@article{dhaene2022resampling,
  title = {Resampling {{Based Bias Correction}} for {{Small Sample SEM}}},
  author = {Dhaene, Sara and Rosseel, Yves},
  year = {2022},
  month = sep,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {5},
  pages = {755--771},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2022.2057999},
  urldate = {2024-04-17},
  abstract = {Structural Equation Models (SEMs) are typically estimated via Maximum Likelihood. Grounded in large sample theory, estimates are prone to finite sample bias. Although Restricted Maximum Likelihood (REML) can alleviate this bias, its applicability is constrained to SEMs that are mathematically equivalent to mixed effect models. Via Monte Carlo simulations, we explored whether resampling based corrections could serve as viable, more broadly applicable alternatives. Results indicate that Bootstrap and Jackknife corrections effectively attenuate small sample bias, at the expected expense of an increase in variability. Similar conclusions are drawn with respect to a more recently proposed analytic approach by Ozenne et~al., which was included for comparison. For all corrective methods, caution is advised when dealing with non-normal data and/or low reliability.},
  keywords = {Bias correction,Bootstrap,Jackknife,REML,small sample bias},
  file = {/Users/haziqj/Zotero/storage/FWGGT7N8/Dhaene and Rosseel - 2022 - Resampling Based Bias Correction for Small Sample SEM.pdf}
}

@book{efron1982jackknife,
  title = {The {{Jackknife}}, the {{Bootstrap}} and {{Other Resampling Plans}}},
  author = {Efron, Bradley},
  year = {1982},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611970319},
  urldate = {2024-06-26},
  isbn = {978-0-89871-179-0},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/6H2BNXSS/Efron - 1982 - The Jackknife, the Bootstrap and Other Resampling Plans.pdf}
}

@incollection{efron1992bootstrap,
  title = {Bootstrap {{Methods}}: {{Another Look}} at the {{Jackknife}}},
  shorttitle = {Bootstrap {{Methods}}},
  booktitle = {Breakthroughs in {{Statistics}}},
  author = {Efron, Bradley},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1992},
  pages = {569--593},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_41},
  urldate = {2025-05-16},
  isbn = {978-0-387-94039-7 978-1-4612-4380-9},
  file = {/Users/haziqj/Zotero/storage/H5HEZIWK/Efron - 1992 - Bootstrap Methods Another Look at the Jackknife.pdf}
}

@book{efron1994introduction,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  year = {1994},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  address = {New York},
  doi = {10.1201/9780429246593},
  abstract = {An Introduction to the Bootstrap arms scientists and engineers as well as statisticians with the computational techniques they need to analyze and understand complicated data sets. The bootstrap is a computer-based method of statistical inference that answers statistical questions without formulas and gives a direct appreciation of variance, bias, coverage, and other probabilistic phenomena. This book presents an overview of the bootstrap and related methods for assessing statistical accuracy, concentrating on the ideas rather than their mathematical justification. Not just for beginners, the presentation starts off slowly, but builds in both scope and depth to ideas that are quite sophisticated.},
  isbn = {978-0-429-24659-3}
}

@article{firth1993bias,
  title = {Bias Reduction of Maximum Likelihood Estimates},
  author = {Firth, David},
  year = {1993},
  month = mar,
  journal = {Biometrika},
  volume = {80},
  number = {1},
  pages = {27--38},
  issn = {0006-3444},
  doi = {10.1093/biomet/80.1.27},
  urldate = {2023-11-22},
  abstract = {It is shown how, in regular parametric problems, the first-order term is removed from the asymptotic bias of maximum likelihood estimates by a suitable modification of the score function. In exponential families with canonical parameterization the effect is to penalize the likelihood by the Jeffreys invariant prior. In binomial logistic models, Poisson log linear models and certain other generalized linear models, the Jeffreys prior penalty function can be imposed in standard regression software using a scheme of iterative adjustments to the data.},
  file = {/Users/haziqj/Zotero/storage/XJXM24PX/Firth - 1993 - Bias reduction of maximum likelihood estimates.pdf}
}

@book{fitzmaurice2011applied,
  title = {Applied Longitudinal Analysis},
  author = {Fitzmaurice, Garrett M. and Laird, Nan M. and Ware, James H.},
  year = {2011},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley},
  address = {Hoboken},
  abstract = {"Written at a technical level suitable for researchers and graduate students, Applied Longitudinal Analysis provides a description of modern methods for analyzing longitudinal data. Focusing on General Linear and Mixed Effects Models for continuous responses, and extensions of Generalized Linear Models for discrete responses, the authors discuss in detail the relationships among these different models, including their underlying assumptions and relative merits."--Jacket},
  isbn = {978-1-119-51346-9},
  langid = {english},
  lccn = {519.53}
}

@article{fox2006teachers,
  title = {{{TEACHER}}'{{S CORNER}}: {{Structural Equation Modeling With}} the Sem {{Package}} in {{R}}},
  shorttitle = {{{TEACHER}}'{{S CORNER}}},
  author = {Fox, John},
  year = {2006},
  month = jun,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {13},
  number = {3},
  pages = {465--486},
  issn = {1070-5511, 1532-8007},
  doi = {10.1207/s15328007sem1303_7},
  urldate = {2024-05-03},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/LPLY798Q/Fox - 2006 - TEACHER'S CORNER Structural Equation Modeling Wit.pdf}
}

@article{gronneberg2022covsim,
  title = {Covsim: {{An R Package}} for {{Simulating Non-Normal Data}} for {{Structural Equation Models Using Copulas}}},
  shorttitle = {Covsim},
  author = {Gr{\o}nneberg, Steffen and Foldnes, Nj{\aa}l and Marcoulides, Katerina M.},
  year = {2022},
  month = may,
  journal = {Journal of Statistical Software},
  volume = {102},
  pages = {1--45},
  issn = {1548-7660},
  doi = {10.18637/jss.v102.i03},
  urldate = {2025-04-10},
  abstract = {In factor analysis and structural equation modeling non-normal data simulation is traditionally performed by specifying univariate skewness and kurtosis together with the target covariance matrix. However, this leaves little control over the univariate distributions and the multivariate copula of the simulated vector. In this paper we explain how a more flexible simulation method called vine-to-anything (VITA) may be obtained from copula-based techniques, as implemented in a new R package, covsim. VITA is based on the concept of a regular vine, where bivariate copulas are coupled together into a full multivariate copula. We illustrate how to simulate continuous and ordinal data for covariance modeling, and how to use the new package discnorm to test for underlying normality in ordinal data. An introduction to copula and vine simulation is provided in the appendix.},
  copyright = {Copyright (c) 2022 Steffen Gr{\o}nneberg, Nj{\aa}l Foldnes, Katerina M. Marcoulides},
  langid = {english},
  keywords = {covariance model,Non-normal simulation,ordinal covariance models,R,vine copulas},
  file = {/Users/haziqj/Zotero/storage/ZJGF65ER/Grønneberg et al. - 2022 - covsim An R Package for Simulating Non-Normal Data for Structural Equation Models Using Copulas.pdf}
}

@article{hall1988bootstrap,
  title = {On Bootstrap Resampling and Iteration},
  author = {Hall, Peter and Martin, Michael A.},
  year = {1988},
  journal = {Biometrika},
  volume = {75},
  number = {4},
  pages = {661--671},
  publisher = {Oxford University Press},
  urldate = {2024-06-26}
}

@article{harville1977maximum,
  title = {Maximum {{Likelihood Approaches}} to {{Variance Component Estimation}} and to {{Related Problems}}},
  author = {Harville, David A.},
  year = {1977},
  journal = {Journal of the American Statistical Association},
  volume = {72},
  number = {358},
  eprint = {2286796},
  eprinttype = {jstor},
  pages = {320--338},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2286796},
  urldate = {2025-05-16},
  abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
  file = {/Users/haziqj/Zotero/storage/N6IX739V/Harville - 1977 - Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems.pdf}
}

@article{huang2017penalized,
  title = {A {{Penalized Likelihood Method}} for {{Structural Equation Modeling}}},
  author = {Huang, Po-Hsien and Chen, Hung and Weng, Li-Jen},
  year = {2017},
  month = jun,
  journal = {Psychometrika},
  volume = {82},
  number = {2},
  pages = {329--354},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-017-9566-9},
  urldate = {2025-05-16},
  abstract = {A penalized likelihood (PL) method for structural equation modeling (SEM) was proposed as a methodology for exploring the underlying relations among both observed and latent variables. Compared to the usual likelihood method, PL includes a penalty term to control the complexity of the hypothesized model. When the penalty level is appropriately chosen, the PL can yield an SEM model that balances the model goodness-of-fit and model complexity. In addition, the PL results in a sparse estimate that enhances the interpretability of the final model. The proposed method is especially useful when limited substantive knowledge is available for model specifications. The PL method can be also understood as a methodology that links the traditional SEM to the exploratory SEM (Asparouhov \& Muth{\'e}n in Struct Equ Model Multidiscipl J 16:397--438, 2009). An expectation-conditional maximization algorithm was developed to maximize the PL criterion. The asymptotic properties of the proposed PL were also derived. The performance of PL was evaluated through a numerical experiment, and two real data illustrations were presented to demonstrate its utility in psychological research.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english}
}

@article{jacobucci2016regularized,
  title = {Regularized {{Structural Equation Modeling}}},
  author = {Jacobucci, Ross and Grimm, Kevin J. and McArdle, John J.},
  year = {2016},
  month = jul,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {23},
  number = {4},
  pages = {555--566},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2016.1154793},
  urldate = {2025-05-16},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/2R88VPEB/Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf}
}

@article{jeffreys1946invariant,
  title = {An Invariant Form for the Prior Probability in Estimation Problems},
  author = {Jeffreys, Harold},
  year = {1946},
  month = sep,
  journal = {Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences},
  volume = {186},
  number = {1007},
  pages = {453--461},
  issn = {0080-4630, 2053-9169},
  doi = {10.1098/rspa.1946.0056},
  urldate = {2025-03-15},
  abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems.},
  copyright = {https://royalsociety.org/journals/ethics-policies/data-sharing-mining/},
  langid = {english}
}

@article{kauermann2001note,
  title = {A {{Note}} on the {{Efficiency}} of {{Sandwich Covariance Matrix Estimation}}},
  author = {Kauermann, G{\"o}ran and Carroll, Raymond J},
  year = {2001},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {456},
  pages = {1387--1396},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214501753382309},
  urldate = {2025-05-16},
  langid = {english}
}

@article{kosmidis2009bias,
  title = {Bias Reduction in Exponential Family Nonlinear Models},
  author = {Kosmidis, Ioannis and Firth, David},
  year = {2009},
  journal = {Biometrika},
  volume = {96},
  number = {4},
  pages = {793--804},
  publisher = {Oxford University Press},
  urldate = {2024-06-26}
}

@article{kosmidis2020mean,
  title = {Mean and Median Bias Reduction in Generalized Linear Models},
  author = {Kosmidis, Ioannis and Kenne Pagui, Euloge Clovis and Sartori, Nicola},
  year = {2020},
  month = feb,
  journal = {Statistics and Computing},
  volume = {30},
  number = {1},
  pages = {43--59},
  issn = {1573-1375},
  doi = {10.1007/s11222-019-09860-6},
  urldate = {2025-05-16},
  abstract = {This paper presents an integrated framework for estimation and inference from generalized linear models using adjusted score equations that result in mean and median bias reduction. The framework unifies theoretical and methodological aspects of past research on mean bias reduction and accommodates, in a natural way, new advances on median bias reduction. General expressions for the adjusted score functions are derived in terms of quantities that are readily available in standard software for fitting generalized linear models. The resulting estimating equations are solved using a unifying quasi-Fisher scoring algorithm that is shown to be equivalent to iteratively reweighted least squares with appropriately adjusted working variates. Formal links between the iterations for mean and median bias reduction are established. Core model invariance properties are used to develop a novel mixed adjustment strategy when the estimation of a dispersion parameter is necessary. It is also shown how median bias reduction in multinomial logistic regression can be done using the equivalent Poisson log-linear model. The estimates coming out from mean and median bias reduction are found to overcome practical issues related to infinite estimates that can occur with positive probability in generalized linear models with multinomial or discrete responses, and can result in valid inferences even in the presence of a high-dimensional nuisance parameter.},
  langid = {english},
  keywords = {Adjusted score equations,Applied Statistics,Biostatistics,Data separation,Dispersion,Iterative reweighted least squares,Linear Models and Regression,Multinomial regression,Parameterization invariance,Parametric Inference,Quantitative Psychology,Statistical Learning},
  file = {/Users/haziqj/Zotero/storage/G2YAGE2R/Kosmidis et al. - 2020 - Mean and median bias reduction in generalized linear models.pdf}
}

@article{kosmidis2024empirical,
  title = {Empirical Bias-Reducing Adjustments to Estimating Functions},
  author = {Kosmidis, Ioannis and Lunardon, Nicola},
  year = {2024},
  month = feb,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {86},
  number = {1},
  pages = {62--89},
  issn = {1369-7412},
  doi = {10.1093/jrsssb/qkad083},
  urldate = {2024-06-25},
  abstract = {We develop a novel, general framework for reduced-bias M-estimation from asymptotically unbiased estimating functions. The framework relies on an empirical approximation of the bias by a function of derivatives of estimating function contributions. Reduced-bias M-estimation operates either implicitly, solving empirically adjusted estimating equations, or explicitly, subtracting the estimated bias from the original M-estimates, and applies to partially or fully specified models with likelihoods or surrogate objectives. Automatic differentiation can abstract away the algebra required to implement reduced-bias M-estimation. As a result, the bias-reduction methods, we introduce have broader applicability, straightforward implementation, and less algebraic or computational effort than other established bias-reduction methods that require resampling or expectations of products of log-likelihood derivatives. If M-estimation is by maximising an objective, then there always exists a bias-reducing penalised objective. That penalised objective relates to information criteria for model selection and can be enhanced with plug-in penalties to deliver reduced-bias M-estimates with extra properties, like finiteness for categorical data models. Inferential procedures and model selection procedures for M-estimators apply unaltered with the reduced-bias M-estimates. We demonstrate and assess the properties of reduced-bias M-estimation in well-used, prominent modelling settings of varying complexity.},
  file = {/Users/haziqj/Zotero/storage/LXTBLDHU/Kosmidis and Lunardon - 2024 - Empirical bias-reducing adjustments to estimating functions.pdf;/Users/haziqj/Zotero/storage/CRZMI84J/7275082.html}
}

@book{lee2012basic,
  title = {Basic and Advanced Structural Equation Models with Applications in the Medical and Behavioural Sciences},
  author = {Lee, Sik-Yum and Song, Xin-Yuan},
  year = {2012},
  publisher = {Wiley},
  address = {Hoboken},
  abstract = {"This book introduces the Bayesian approach to SEMs, including the selection of prior distributions and data augmentation, and offers an overview of the subject's recent advances"--},
  isbn = {978-1-118-35887-0 978-1-118-35880-1 978-1-118-35943-3},
  lccn = {QA278.3},
  keywords = {Bayesian statistical decision theory,MATHEMATICS / Probability & Statistics / General,Structural equation modeling},
  file = {/Users/haziqj/Zotero/storage/HPJIIQHY/Lee and Song - 2012 - Basic and advanced structural equation models with applications in the medical and behavioural scien.pdf}
}

@article{mcneish2016using,
  title = {Using {{Data-Dependent Priors}} to {{Mitigate Small Sample Bias}} in {{Latent Growth Models}}: {{A Discussion}} and {{Illustration Using M}} {\emph{Plus}}},
  shorttitle = {Using {{Data-Dependent Priors}} to {{Mitigate Small Sample Bias}} in {{Latent Growth Models}}},
  author = {McNeish, Daniel},
  year = {2016},
  month = feb,
  journal = {Journal of Educational and Behavioral Statistics},
  volume = {41},
  number = {1},
  pages = {27--56},
  issn = {1076-9986, 1935-1054},
  doi = {10.3102/1076998615621299},
  urldate = {2025-05-16},
  abstract = {Mixed-effects models (MEMs) and latent growth models (LGMs) are often considered interchangeable save the discipline-specific nomenclature. Software implementations of these models, however, are not interchangeable, particularly with small sample sizes. Restricted maximum likelihood estimation that mitigates small sample bias in MEMs has not been widely developed for LGMs, and fully Bayesian methods, while not dependent on asymptotics, can encounter issues because the choice for the factor covariance matrix prior distribution has substantial influence with small samples. This tutorial discusses differences between LGMs and MEMs and demonstrates how data-dependent priors, an established class of methods that blend frequentist and Bayesian paradigms, can be implemented within M plus 7.1 to abate the small sample bias that is prevalent with LGM software while keeping additional programming to the bare minimum.},
  langid = {english}
}

@article{mcneish2018brief,
  title = {Brief {{Research Report}}: {{Growth Models With Small Samples}} and {{Missing Data}}},
  shorttitle = {Brief {{Research Report}}},
  author = {McNeish, Daniel},
  year = {2018},
  month = oct,
  journal = {The Journal of Experimental Education},
  volume = {86},
  number = {4},
  pages = {690--701},
  issn = {0022-0973, 1940-0683},
  doi = {10.1080/00220973.2017.1369384},
  urldate = {2025-05-16},
  langid = {english}
}

@article{mcneish2018differentiating,
  title = {Differentiating between Mixed-Effects and Latent-Curve Approaches to Growth Modeling},
  author = {McNeish, Daniel and Matta, Tyler},
  year = {2018},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {50},
  number = {4},
  pages = {1398--1414},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0976-5},
  urldate = {2025-05-16},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/3J9KJTUC/McNeish and Matta - 2018 - Differentiating between mixed-effects and latent-curve approaches to growth modeling.pdf}
}

@article{muthen2012bayesian,
  title = {Bayesian Structural Equation Modeling: {{A}} More Flexible Representation of Substantive Theory},
  shorttitle = {Bayesian Structural Equation Modeling},
  author = {Muth{\'e}n, Bengt and Asparouhov, Tihomir},
  year = {2012},
  journal = {Psychological Methods},
  volume = {17},
  number = {3},
  pages = {313--335},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1463},
  doi = {10.1037/a0026802},
  abstract = {This article proposes a new approach to factor analysis and structural equation modeling using Bayesian analysis. The new approach replaces parameter specifications of exact zeros with approximate zeros based on informative, small-variance priors. It is argued that this produces an analysis that better reflects substantive theories. The proposed Bayesian approach is particularly beneficial in applications where parameters are added to a conventional model such that a nonidentified model is obtained if maximum-likelihood estimation is applied. This approach is useful for measurement aspects of latent variable modeling, such as with confirmatory factor analysis, and the measurement part of structural equation modeling. Two application areas are studied, cross-loadings and residual correlations in confirmatory factor analysis. An example using a full structural equation model is also presented, showing an efficient way to find model misspecification. The approach encompasses 3 elements: model testing using posterior predictive checking, model estimation, and model modification. Monte Carlo simulations and real data are analyzed using Mplus. The real-data analyses use data from Holzinger and Swineford's (1939) classic mental abilities study, Big Five personality factor data from a British survey, and science achievement data from the National Educational Longitudinal Study of 1988. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Confirmatory Factor Analysis,Factor Analysis,Markov Chains,Statistical Probability,Structural Equation Modeling},
  file = {/Users/haziqj/Zotero/storage/2FPB9S6W/2012-24038-001.html}
}

@article{nevitt2004evaluating,
  title = {Evaluating {{Small Sample Approaches}} for {{Model Test Statistics}} in {{Structural Equation Modeling}}},
  author = {Nevitt, Jonathan and Hancock, Gregory R.},
  year = {2004},
  month = jul,
  journal = {Multivariate Behavioral Research},
  volume = {39},
  number = {3},
  pages = {439--478},
  issn = {0027-3171, 1532-7906},
  doi = {10.1207/S15327906MBR3903_3},
  urldate = {2025-05-16},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/GQSFCXFQ/Nevitt and Hancock - 2004 - Evaluating Small Sample Approaches for Model Test Statistics in Structural Equation Modeling.pdf}
}

@article{ozenne2020small,
  title = {Small {{Sample Corrections}} for {{Wald Tests}} in {{Latent Variable Models}}},
  author = {Ozenne, Brice and Fisher, Patrick M. and {Budtz-Jorgensen}, Esben},
  year = {2020},
  month = aug,
  journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume = {69},
  number = {4},
  pages = {841--861},
  issn = {0035-9254},
  doi = {10.1111/rssc.12414},
  urldate = {2024-04-17},
  abstract = {Latent variable models are commonly used in psychology and increasingly used for analysing brain imaging data. Such studies typically involve a small number of participants (n\&lt;100), where standard asymptotic results often fail to control the type 1 error appropriately. The paper presents two corrections improving the control of the type 1 error of Wald tests in latent variable models estimated by using maximum likelihood. First, we derive a correction for the bias of the maximum likelihood estimator of the variance parameters. This enables us to estimate corrected standard errors for model parameters and corrected Wald statistics. Second, we use a Student t-distribution instead of a Gaussian distribution to account for the variability of the variance estimator. The degrees of freedom of the Student t-distributions are estimated by using a Satterthwaite approximation. A simulation study based on data from two published brain imaging studies demonstrates that combining these two corrections provides superior control of the type 1 error rate compared with the uncorrected Wald test, despite being conservative for some parameters. The methods proposed are implemented in the R package lavaSearch2, which is available from https://cran.r-project.org/web/packages/lavaSearch2.},
  file = {/Users/haziqj/Zotero/storage/PRLWHM7X/Ozenne et al. - 2020 - Small Sample Corrections for Wald Tests in Latent Variable Models.pdf;/Users/haziqj/Zotero/storage/P7FWUJWA/7058441.html}
}

@article{patterson1971recovery,
  title = {Recovery of Inter-Block Information When Block Sizes Are Unequal},
  author = {Patterson, H. Desmond and Thompson, Robin},
  year = {1971},
  journal = {Biometrika},
  volume = {58},
  number = {3},
  pages = {545--554},
  publisher = {Oxford University Press},
  urldate = {2025-05-16}
}

@article{quenouille1949approximate,
  title = {Approximate {{Tests}} of {{Correlation}} in {{Time-Series}}},
  author = {Quenouille, M. H.},
  year = {1949},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {11},
  number = {1},
  eprint = {2983696},
  eprinttype = {jstor},
  pages = {68--84},
  publisher = {[Royal Statistical Society, Oxford University Press]},
  issn = {0035-9246},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/Y9689UE5/Quenouille - 1949 - Approximate Tests of Correlation in Time-Series.pdf}
}

@article{quenouille1956notes,
  title = {Notes on Bias in Estimation},
  author = {Quenouille, Maurice H.},
  year = {1956},
  journal = {Biometrika},
  volume = {43},
  number = {3/4},
  eprint = {2332914},
  eprinttype = {jstor},
  pages = {353--360},
  publisher = {JSTOR},
  urldate = {2025-05-16},
  file = {/Users/haziqj/Zotero/storage/3DNDZMP3/Quenouille - 1956 - Notes on bias in estimation.pdf}
}

@article{rosseel2024structural,
  title = {A Structural after Measurement Approach to Structural Equation Modeling.},
  author = {Rosseel, Yves and Loh, Wen Wei},
  year = {2024},
  month = jun,
  journal = {Psychological Methods},
  volume = {29},
  number = {3},
  pages = {561--588},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000503},
  urldate = {2025-05-16},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/IL9Q5UAE/Rosseel and Loh - 2024 - A structural after measurement approach to structural equation modeling..pdf}
}

@article{sanchez2005structural,
  title = {Structural {{Equation Models}}: {{A Review With Applications}} to {{Environmental Epidemiology}}},
  shorttitle = {Structural {{Equation Models}}},
  author = {S{\'a}nchez, Brisa N and {Budtz-J{\o}rgensen}, Esben and Ryan, Louise M and Hu, Howard},
  year = {2005},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {472},
  pages = {1443--1455},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214505000001005},
  urldate = {2024-04-23},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/SW9Y9DKG/Sánchez et al. - 2005 - Structural Equation Models A Review With Applicat.pdf}
}

@article{savalei2022computational,
  title = {Computational {{Options}} for {{Standard Errors}} and {{Test Statistics}} with {{Incomplete Normal}} and {{Nonnormal Data}} in {{SEM}}},
  author = {Savalei, Victoria and Rosseel, Yves},
  year = {2022},
  month = mar,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {29},
  number = {2},
  pages = {163--181},
  issn = {1070-5511, 1532-8007},
  doi = {10.1080/10705511.2021.1877548},
  urldate = {2024-04-25},
  abstract = {This article provides an overview of different computational options for inference following normal theory maximum likelihood (ML) estimation in structural equation modeling (SEM) with incomplete normal and nonnormal data. Complete data are covered as a special case. These computational options include whether the information matrix is observed or expected, whether the observed information matrix is estimated numerically or using an analytic asymptotic approximation, and whether the information matrix and the outer product matrix of the score vector are evaluated at the saturated or at the structured estimates. A variety of different standard errors and robust test statistics become possible by varying these options. We review the asymptotic properties of these computational variations, and we show how to obtain them using lavaan in R. We hope that this article will encourage methodologists to study the impact of the available computational options on the performance of standard errors and test statistics in SEM.},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/2S72L7P2/Savalei and Rosseel - 2022 - Computational Options for Standard Errors and Test Statistics with Incomplete Normal and Nonnormal D.pdf}
}

@book{skrondal2004generalized,
  title = {Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models},
  shorttitle = {Generalized Latent Variable Modeling},
  author = {Skrondal, Anders and {Rabe-Hesketh}, S.},
  year = {2004},
  series = {Chapman \& {{Hall}}/{{CRC}} Interdisciplinary Statistics Series},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton},
  isbn = {978-1-58488-000-4},
  lccn = {QA278.6 .S57 2004},
  keywords = {Latent structure analysis,Latent variables}
}

@article{sterzinger2023maximum,
  title = {Maximum Softly-Penalized Likelihood for Mixed Effects Logistic Regression},
  author = {Sterzinger, Philipp and Kosmidis, Ioannis},
  year = {2023},
  journal = {Statistics and Computing},
  volume = {33},
  number = {2},
  issn = {0960-3174},
  doi = {10.1007/s11222-023-10217-3},
  urldate = {2024-12-10},
  abstract = {Maximum likelihood estimation in logistic regression with mixed effects is known to often result in estimates on the boundary of the parameter space. Such estimates, which include infinite values for fixed effects and singular or infinite variance components, can cause havoc to numerical estimation procedures and inference. We introduce an appropriately scaled additive penalty to the log-likelihood function, or an approximation thereof, which penalizes the fixed effects by the Jeffreys' invariant prior for the model with no random effects and the variance components by a composition of negative Huber loss functions. The resulting maximum penalized likelihood estimates are shown to lie in the interior of the parameter space. Appropriate scaling of the penalty guarantees that the penalization is soft enough to preserve the optimal asymptotic properties expected by the maximum likelihood estimator, namely consistency, asymptotic normality, and Cram{\'e}r-Rao efficiency. Our choice of penalties and scaling factor preserves equivariance of the fixed effects estimates under linear transformation of the model parameters, such as contrasts. Maximum softly-penalized likelihood is compared to competing approaches on two real-data examples, and through comprehensive simulation studies that illustrate its superior finite sample performance.},
  langid = {english},
  file = {/Users/haziqj/Zotero/storage/5U6JQ6BC/Sterzinger and Kosmidis - 2023 - Maximum softly-penalized likelihood for mixed effects logistic regression.pdf}
}

@article{tukey1958bias,
  title = {Bias and Confidence in Not Quite Large Samples},
  author = {Tukey, John},
  year = {1958},
  journal = {Ann. Math. Statist.},
  volume = {29},
  pages = {614},
  urldate = {2025-05-16}
}

@book{vandervaart1998asymptotic,
  title = {Asymptotic {{Statistics}}},
  author = {{van der Vaart}, A. W.},
  year = {1998},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511802256},
  urldate = {2024-06-26},
  abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
  isbn = {978-0-521-78450-4},
  file = {/Users/haziqj/Zotero/storage/FM52JE7S/A3C7DAD3F7E66A1FA60E9C8FE132EE1D.html}
}

@book{vandeschoot2020small,
  title = {Small {{Sample Size Solutions}}: {{A Guide}} for {{Applied Researchers}} and {{Practitioners}}},
  shorttitle = {Small Sample Size Solutions},
  author = {{van de Schoot}, Rens and Mio{\v c}evi{\'c}, Milica},
  year = {2020},
  publisher = {Routledge},
  address = {Abingdon, Oxon; New York, NY},
  isbn = {978-0-429-27387-2},
  langid = {english},
  lccn = {Q180.55.M4},
  keywords = {Data sets,Methodology,Research},
  file = {/Users/haziqj/Zotero/storage/HKX8ZJD2/2020 - Small sample size solutions a guide for applied researchers and practitioners.pdf}
}

@article{vanerp2018prior,
  title = {Prior Sensitivity Analysis in Default {{Bayesian}} Structural Equation Modeling.},
  author = {Van Erp, Sara and Mulder, Joris and Oberski, Daniel L.},
  year = {2018},
  journal = {Psychological Methods},
  volume = {23},
  number = {2},
  pages = {363},
  publisher = {American Psychological Association},
  urldate = {2025-05-16}
}

@article{kosmidis2021finite,
    author = {Kosmidis, Ioannis and Firth, David},
    title = {Jeffreys-prior penalty, finiteness and shrinkage in binomial-response generalized linear models},
    journal = {Biometrika},
    volume = {108},
    number = {1},
    pages = {71--82},
    year = {2021},
    doi = {10.1093/biomet/asaa052}
}